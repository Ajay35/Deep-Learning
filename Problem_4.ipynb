{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mu1 = [-3,4]\n",
    "mu2 = [4,-3]\n",
    "\n",
    "sig1 = [[16, 0],[0, 9]]\n",
    "sig2 = [[16, 0],[0, 9]]\n",
    "\n",
    "train_set0 = np.append(np.random.multivariate_normal(mu1,sig1,200),np.ones((200,1)),axis=1)\n",
    "train_set1 = np.append(np.random.multivariate_normal(mu2,sig2,200),-np.ones((200,1)),axis=1)\n",
    "test_set0 = np.append(np.random.multivariate_normal(mu1,sig1,100),np.ones((100,1)),axis=1)\n",
    "test_set1 = np.append(np.random.multivariate_normal(mu2,sig2,100),-np.ones((100,1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train set concatenated\n",
    "\n",
    "X_train=np.zeros((400,2))\n",
    "y_train=np.zeros((400,1))\n",
    "X_test=np.zeros((200,2))\n",
    "y_test=np.zeros((200,1))\n",
    "ind = 0\n",
    "for i in range(0,200):\n",
    "    X_train[ind]=np.array([train_set0[i][0],train_set0[i][1]])\n",
    "    y_train[ind]=train_set0[i][2]\n",
    "    ind+=1\n",
    "for i in range(0,200):\n",
    "    X_train[ind]=np.array([train_set1[i][0],train_set1[i][1]])\n",
    "    y_train[ind]=train_set1[i][2]\n",
    "    ind+=1\n",
    "\n",
    "ind1=0\n",
    "for i in range(0,100):\n",
    "    X_test[ind1]=np.array([test_set0[i][0],test_set0[i][1]])\n",
    "    y_test[ind1]=test_set0[i][2]\n",
    "    ind1+=1\n",
    "for i in range(0,100):\n",
    "    X_test[ind1]=np.array([test_set1[i][0],test_set1[i][1]])\n",
    "    y_test[ind1]=test_set1[i][2]\n",
    "    ind1+=1\n",
    "\n",
    "X_train = np.concatenate((X_train, np.ones((400,1))),axis = 1)\n",
    "X_test = np.concatenate((X_test, np.ones((200,1))),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.T\n",
    "X_test=X_test.T\n",
    "y_train=y_train.T\n",
    "y_test=y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def sigmoid(x):\n",
    "    a  = 1/(1+np.exp(-x))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(w, Data):\n",
    "    pred = []\n",
    "    z = np.dot(w,Data)\n",
    "    a = sigmoid(z)\n",
    "    for i in range(0,len(a[0])):\n",
    "        if (a[0][i] > 0.5): \n",
    "            pred.append(1)\n",
    "        elif (a[0][i] <= 0.5):\n",
    "            pred.append(-1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1471.198759576943 Training Accuracy 17.0\n",
      "Epoch 2 Loss 1458.8845671680845 Training Accuracy 17.25\n",
      "Epoch 3 Loss 1446.6088272537352 Training Accuracy 17.5\n",
      "Epoch 4 Loss 1434.3723031287786 Training Accuracy 17.5\n",
      "Epoch 5 Loss 1422.1757683755773 Training Accuracy 17.5\n",
      "Epoch 6 Loss 1410.0200067242367 Training Accuracy 17.5\n",
      "Epoch 7 Loss 1397.905811895813 Training Accuracy 17.5\n",
      "Epoch 8 Loss 1385.8339874276685 Training Accuracy 17.5\n",
      "Epoch 9 Loss 1373.8053464801421 Training Accuracy 17.5\n",
      "Epoch 10 Loss 1361.8207116236715 Training Accuracy 17.75\n",
      "Epoch 11 Loss 1349.880914605465 Training Accuracy 18.0\n",
      "Epoch 12 Loss 1337.986796094791 Training Accuracy 18.0\n",
      "Epoch 13 Loss 1326.1392054059183 Training Accuracy 18.0\n",
      "Epoch 14 Loss 1314.339000197699 Training Accuracy 18.25\n",
      "Epoch 15 Loss 1302.5870461487712 Training Accuracy 18.5\n",
      "Epoch 16 Loss 1290.8842166073148 Training Accuracy 19.0\n",
      "Epoch 17 Loss 1279.2313922142769 Training Accuracy 19.75\n",
      "Epoch 18 Loss 1267.6294604989585 Training Accuracy 20.0\n",
      "Epoch 19 Loss 1256.0793154458415 Training Accuracy 20.25\n",
      "Epoch 20 Loss 1244.5818570315241 Training Accuracy 20.25\n",
      "Epoch 21 Loss 1233.1379907306318 Training Accuracy 20.5\n",
      "Epoch 22 Loss 1221.7486269895735 Training Accuracy 20.75\n",
      "Epoch 23 Loss 1210.4146806670417 Training Accuracy 21.25\n",
      "Epoch 24 Loss 1199.1370704401706 Training Accuracy 21.25\n",
      "Epoch 25 Loss 1187.916718175315 Training Accuracy 21.5\n",
      "Epoch 26 Loss 1176.7545482624716 Training Accuracy 21.5\n",
      "Epoch 27 Loss 1165.6514869124273 Training Accuracy 21.75\n",
      "Epoch 28 Loss 1154.608461415817 Training Accuracy 22.25\n",
      "Epoch 29 Loss 1143.6263993633743 Training Accuracy 22.25\n",
      "Epoch 30 Loss 1132.7062278267833 Training Accuracy 22.75\n",
      "Epoch 31 Loss 1121.8488724996878 Training Accuracy 23.0\n",
      "Epoch 32 Loss 1111.055256798579 Training Accuracy 23.25\n",
      "Epoch 33 Loss 1100.3263009234636 Training Accuracy 23.25\n",
      "Epoch 34 Loss 1089.6629208784243 Training Accuracy 23.5\n",
      "Epoch 35 Loss 1079.0660274524062 Training Accuracy 23.75\n",
      "Epoch 36 Loss 1068.536525160811 Training Accuracy 24.0\n",
      "Epoch 37 Loss 1058.0753111487347 Training Accuracy 24.5\n",
      "Epoch 38 Loss 1047.6832740569707 Training Accuracy 25.0\n",
      "Epoch 39 Loss 1037.3612928521866 Training Accuracy 25.5\n",
      "Epoch 40 Loss 1027.1102356229842 Training Accuracy 26.0\n",
      "Epoch 41 Loss 1016.9309583438769 Training Accuracy 26.25\n",
      "Epoch 42 Loss 1006.8243036095265 Training Accuracy 26.25\n",
      "Epoch 43 Loss 996.7910993419129 Training Accuracy 26.5\n",
      "Epoch 44 Loss 986.8321574734388 Training Accuracy 27.0\n",
      "Epoch 45 Loss 976.9482726092929 Training Accuracy 27.250000000000004\n",
      "Epoch 46 Loss 967.1402206727233 Training Accuracy 27.500000000000004\n",
      "Epoch 47 Loss 957.4087575371846 Training Accuracy 27.750000000000004\n",
      "Epoch 48 Loss 947.7546176496373 Training Accuracy 28.000000000000004\n",
      "Epoch 49 Loss 938.1785126495791 Training Accuracy 28.249999999999996\n",
      "Epoch 50 Loss 928.6811299886742 Training Accuracy 28.749999999999996\n",
      "Epoch 51 Loss 919.2631315561304 Training Accuracy 28.749999999999996\n",
      "Epoch 52 Loss 909.9251523152366 Training Accuracy 29.5\n",
      "Epoch 53 Loss 900.6677989567306 Training Accuracy 30.0\n",
      "Epoch 54 Loss 891.4916485748944 Training Accuracy 30.5\n",
      "Epoch 55 Loss 882.3972473725078 Training Accuracy 30.5\n",
      "Epoch 56 Loss 873.3851094009794 Training Accuracy 31.25\n",
      "Epoch 57 Loss 864.4557153421629 Training Accuracy 31.25\n",
      "Epoch 58 Loss 855.6095113385103 Training Accuracy 31.5\n",
      "Epoch 59 Loss 846.846907878348 Training Accuracy 32.0\n",
      "Epoch 60 Loss 838.1682787431324 Training Accuracy 33.0\n",
      "Epoch 61 Loss 829.5739600235959 Training Accuracy 33.25\n",
      "Epoch 62 Loss 821.064249211677 Training Accuracy 34.0\n",
      "Epoch 63 Loss 812.6394043750612 Training Accuracy 34.0\n",
      "Epoch 64 Loss 804.2996434210186 Training Accuracy 35.0\n",
      "Epoch 65 Loss 796.045143456012 Training Accuracy 35.5\n",
      "Epoch 66 Loss 787.876040247244 Training Accuracy 36.0\n",
      "Epoch 67 Loss 779.7924277919204 Training Accuracy 36.0\n",
      "Epoch 68 Loss 771.794357999525 Training Accuracy 36.75\n",
      "Epoch 69 Loss 763.8818404918047 Training Accuracy 37.0\n",
      "Epoch 70 Loss 756.0548425244883 Training Accuracy 37.75\n",
      "Epoch 71 Loss 748.3132890339828 Training Accuracy 38.25\n",
      "Epoch 72 Loss 740.6570628114305 Training Accuracy 38.25\n",
      "Epoch 73 Loss 733.0860048055731 Training Accuracy 38.25\n",
      "Epoch 74 Loss 725.5999145548766 Training Accuracy 38.75\n",
      "Epoch 75 Loss 718.1985507483287 Training Accuracy 38.75\n",
      "Epoch 76 Loss 710.8816319132686 Training Accuracy 39.0\n",
      "Epoch 77 Loss 703.6488372275419 Training Accuracy 39.25\n",
      "Epoch 78 Loss 696.4998074522393 Training Accuracy 39.75\n",
      "Epoch 79 Loss 689.4341459802839 Training Accuracy 39.75\n",
      "Epoch 80 Loss 682.4514199951939 Training Accuracy 40.25\n",
      "Epoch 81 Loss 675.5511617335076 Training Accuracy 40.25\n",
      "Epoch 82 Loss 668.7328698435972 Training Accuracy 40.75\n",
      "Epoch 83 Loss 661.9960108329603 Training Accuracy 41.5\n",
      "Epoch 84 Loss 655.3400205955442 Training Accuracy 42.5\n",
      "Epoch 85 Loss 648.7643060102575 Training Accuracy 43.5\n",
      "Epoch 86 Loss 642.2682466015289 Training Accuracy 43.5\n",
      "Epoch 87 Loss 635.8511962525924 Training Accuracy 44.25\n",
      "Epoch 88 Loss 629.5124849621172 Training Accuracy 44.5\n",
      "Epoch 89 Loss 623.2514206348081 Training Accuracy 44.5\n",
      "Epoch 90 Loss 617.0672908967194 Training Accuracy 44.5\n",
      "Epoch 91 Loss 610.9593649261853 Training Accuracy 44.5\n",
      "Epoch 92 Loss 604.9268952915049 Training Accuracy 44.5\n",
      "Epoch 93 Loss 598.9691197867786 Training Accuracy 44.5\n",
      "Epoch 94 Loss 593.0852632576009 Training Accuracy 45.0\n",
      "Epoch 95 Loss 587.2745394086263 Training Accuracy 45.0\n",
      "Epoch 96 Loss 581.5361525853674 Training Accuracy 45.5\n",
      "Epoch 97 Loss 575.8692995229256 Training Accuracy 46.0\n",
      "Epoch 98 Loss 570.2731710547131 Training Accuracy 46.5\n",
      "Epoch 99 Loss 564.7469537745892 Training Accuracy 46.5\n",
      "Epoch 100 Loss 559.2898316462105 Training Accuracy 46.75\n",
      "Epoch 101 Loss 553.900987553782 Training Accuracy 47.0\n",
      "Epoch 102 Loss 548.5796047888106 Training Accuracy 47.5\n",
      "Epoch 103 Loss 543.3248684678872 Training Accuracy 47.75\n",
      "Epoch 104 Loss 538.1359668769836 Training Accuracy 48.25\n",
      "Epoch 105 Loss 533.0120927382314 Training Accuracy 48.5\n",
      "Epoch 106 Loss 527.952444395664 Training Accuracy 48.75\n",
      "Epoch 107 Loss 522.9562269169389 Training Accuracy 48.75\n",
      "Epoch 108 Loss 518.0226531086278 Training Accuracy 48.75\n",
      "Epoch 109 Loss 513.1509444432386 Training Accuracy 48.75\n",
      "Epoch 110 Loss 508.34033189675 Training Accuracy 48.75\n",
      "Epoch 111 Loss 503.5900566960359 Training Accuracy 48.75\n",
      "Epoch 112 Loss 498.89937097618144 Training Accuracy 48.75\n",
      "Epoch 113 Loss 494.26753834829344 Training Accuracy 49.0\n",
      "Epoch 114 Loss 489.69383437900285 Training Accuracy 49.0\n",
      "Epoch 115 Loss 485.17754698342577 Training Accuracy 49.0\n",
      "Epoch 116 Loss 480.7179767338861 Training Accuracy 49.0\n",
      "Epoch 117 Loss 476.3144370872085 Training Accuracy 49.25\n",
      "Epoch 118 Loss 471.9662545338404 Training Accuracy 49.5\n",
      "Epoch 119 Loss 467.6727686724776 Training Accuracy 49.5\n",
      "Epoch 120 Loss 463.43333221421886 Training Accuracy 49.5\n",
      "Epoch 121 Loss 459.2473109205797 Training Accuracy 49.5\n",
      "Epoch 122 Loss 455.114083479938 Training Accuracy 49.5\n",
      "Epoch 123 Loss 451.0330413271792 Training Accuracy 49.5\n",
      "Epoch 124 Loss 447.00358841143606 Training Accuracy 49.5\n",
      "Epoch 125 Loss 443.0251409169064 Training Accuracy 49.5\n",
      "Epoch 126 Loss 439.09712694176034 Training Accuracy 49.5\n",
      "Epoch 127 Loss 435.21898614013224 Training Accuracy 49.5\n",
      "Epoch 128 Loss 431.39016933213645 Training Accuracy 49.5\n",
      "Epoch 129 Loss 427.61013808674335 Training Accuracy 49.75\n",
      "Epoch 130 Loss 423.8783642822228 Training Accuracy 49.75\n",
      "Epoch 131 Loss 420.1943296486962 Training Accuracy 49.75\n",
      "Epoch 132 Loss 416.5575252971484 Training Accuracy 49.75\n",
      "Epoch 133 Loss 412.96745123903804 Training Accuracy 49.75\n",
      "Epoch 134 Loss 409.42361590041634 Training Accuracy 50.24999999999999\n",
      "Epoch 135 Loss 405.9255356342172 Training Accuracy 50.24999999999999\n",
      "Epoch 136 Loss 402.4727342341303 Training Accuracy 50.24999999999999\n",
      "Epoch 137 Loss 399.0647424532025 Training Accuracy 50.24999999999999\n",
      "Epoch 138 Loss 395.7010975300497 Training Accuracy 50.24999999999999\n",
      "Epoch 139 Loss 392.3813427252942 Training Accuracy 50.24999999999999\n",
      "Epoch 140 Loss 389.105026870572 Training Accuracy 50.24999999999999\n",
      "Epoch 141 Loss 385.87170393219566 Training Accuracy 50.24999999999999\n",
      "Epoch 142 Loss 382.68093259129927 Training Accuracy 50.24999999999999\n",
      "Epoch 143 Loss 379.53227584203796 Training Accuracy 50.24999999999999\n",
      "Epoch 144 Loss 376.42530060917653 Training Accuracy 50.24999999999999\n",
      "Epoch 145 Loss 373.35957738616526 Training Accuracy 50.74999999999999\n",
      "Epoch 146 Loss 370.33467989457876 Training Accuracy 50.74999999999999\n",
      "Epoch 147 Loss 367.35018476558537 Training Accuracy 51.0\n",
      "Epoch 148 Loss 364.40567124391293 Training Accuracy 51.24999999999999\n",
      "Epoch 149 Loss 361.50072091459236 Training Accuracy 51.24999999999999\n",
      "Epoch 150 Loss 358.63491745258625 Training Accuracy 51.24999999999999\n",
      "Epoch 151 Loss 355.80784639525234 Training Accuracy 51.24999999999999\n",
      "Epoch 152 Loss 353.019094937443 Training Accuracy 51.24999999999999\n",
      "Epoch 153 Loss 350.2682517489093 Training Accuracy 51.24999999999999\n",
      "Epoch 154 Loss 347.55490681356014 Training Accuracy 51.5\n",
      "Epoch 155 Loss 344.87865129001864 Training Accuracy 51.5\n",
      "Epoch 156 Loss 342.2390773928258 Training Accuracy 51.74999999999999\n",
      "Epoch 157 Loss 339.63577829355637 Training Accuracy 52.0\n",
      "Epoch 158 Loss 337.0683480410473 Training Accuracy 52.0\n",
      "Epoch 159 Loss 334.5363814998768 Training Accuracy 52.0\n",
      "Epoch 160 Loss 332.03947430618695 Training Accuracy 52.25\n",
      "Epoch 161 Loss 329.5772228399022 Training Accuracy 52.5\n",
      "Epoch 162 Loss 327.1492242123747 Training Accuracy 52.75\n",
      "Epoch 163 Loss 324.75507626846104 Training Accuracy 52.75\n",
      "Epoch 164 Loss 322.3943776020301 Training Accuracy 52.75\n",
      "Epoch 165 Loss 320.06672758389595 Training Accuracy 53.0\n",
      "Epoch 166 Loss 317.7717264011754 Training Accuracy 53.0\n",
      "Epoch 167 Loss 315.5089751070753 Training Accuracy 53.25\n",
      "Epoch 168 Loss 313.27807568013833 Training Accuracy 53.75\n",
      "Epoch 169 Loss 311.07863109198723 Training Accuracy 54.25\n",
      "Epoch 170 Loss 308.9102453826388 Training Accuracy 54.25\n",
      "Epoch 171 Loss 306.772523742482 Training Accuracy 54.25\n",
      "Epoch 172 Loss 304.66507260005227 Training Accuracy 54.25\n",
      "Epoch 173 Loss 302.58749971475925 Training Accuracy 54.50000000000001\n",
      "Epoch 174 Loss 300.5394142737689 Training Accuracy 55.25\n",
      "Epoch 175 Loss 298.52042699227434 Training Accuracy 55.75\n",
      "Epoch 176 Loss 296.5301502164287 Training Accuracy 55.75\n",
      "Epoch 177 Loss 294.56819802825385 Training Accuracy 56.00000000000001\n",
      "Epoch 178 Loss 292.63418635187855 Training Accuracy 56.25\n",
      "Epoch 179 Loss 290.7277330604975 Training Accuracy 56.49999999999999\n",
      "Epoch 180 Loss 288.84845808348723 Training Accuracy 56.75\n",
      "Epoch 181 Loss 286.9959835131501 Training Accuracy 56.75\n",
      "Epoch 182 Loss 285.1699337105983 Training Accuracy 56.75\n",
      "Epoch 183 Loss 283.3699354103292 Training Accuracy 57.25\n",
      "Epoch 184 Loss 281.5956178230804 Training Accuracy 57.25\n",
      "Epoch 185 Loss 279.8466127365864 Training Accuracy 57.25\n",
      "Epoch 186 Loss 278.12255461390083 Training Accuracy 57.75\n",
      "Epoch 187 Loss 276.42308068897415 Training Accuracy 57.75\n",
      "Epoch 188 Loss 274.74783105921705 Training Accuracy 58.25\n",
      "Epoch 189 Loss 273.0964487748063 Training Accuracy 58.25\n",
      "Epoch 190 Loss 271.4685799245215 Training Accuracy 58.25\n",
      "Epoch 191 Loss 269.8638737179333 Training Accuracy 59.0\n",
      "Epoch 192 Loss 268.2819825637831 Training Accuracy 59.0\n",
      "Epoch 193 Loss 266.72256214442996 Training Accuracy 59.0\n",
      "Epoch 194 Loss 265.1852714862574 Training Accuracy 59.25\n",
      "Epoch 195 Loss 263.66977302595905 Training Accuracy 59.25\n",
      "Epoch 196 Loss 262.17573267264356 Training Accuracy 59.5\n",
      "Epoch 197 Loss 260.70281986572 Training Accuracy 59.5\n",
      "Epoch 198 Loss 259.2507076285409 Training Accuracy 60.0\n",
      "Epoch 199 Loss 257.8190726178021 Training Accuracy 60.25\n",
      "Epoch 200 Loss 256.40759516871134 Training Accuracy 60.5\n",
      "Epoch 201 Loss 255.01595933595343 Training Accuracy 60.75000000000001\n",
      "Epoch 202 Loss 253.6438529304944 Training Accuracy 60.75000000000001\n",
      "Epoch 203 Loss 252.29096755227795 Training Accuracy 60.75000000000001\n",
      "Epoch 204 Loss 250.95699861888062 Training Accuracy 61.0\n",
      "Epoch 205 Loss 249.64164539020018 Training Accuracy 61.25000000000001\n",
      "Epoch 206 Loss 248.3446109892634 Training Accuracy 61.5\n",
      "Epoch 207 Loss 247.06560241924439 Training Accuracy 61.75000000000001\n",
      "Epoch 208 Loss 245.80433057679497 Training Accuracy 62.0\n",
      "Epoch 209 Loss 244.56051026179188 Training Accuracy 62.74999999999999\n",
      "Epoch 210 Loss 243.33386018361324 Training Accuracy 63.24999999999999\n",
      "Epoch 211 Loss 242.12410296405918 Training Accuracy 63.5\n",
      "Epoch 212 Loss 240.9309651370364 Training Accuracy 64.0\n",
      "Epoch 213 Loss 239.75417714512912 Training Accuracy 64.25\n",
      "Epoch 214 Loss 238.59347333318064 Training Accuracy 64.5\n",
      "Epoch 215 Loss 237.4485919390117 Training Accuracy 64.75\n",
      "Epoch 216 Loss 236.31927508140274 Training Accuracy 64.75\n",
      "Epoch 217 Loss 235.20526874546727 Training Accuracy 65.0\n",
      "Epoch 218 Loss 234.10632276554418 Training Accuracy 65.75\n",
      "Epoch 219 Loss 233.02219080573505 Training Accuracy 66.0\n",
      "Epoch 220 Loss 231.95263033821286 Training Accuracy 66.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221 Loss 230.89740261942612 Training Accuracy 66.75\n",
      "Epoch 222 Loss 229.85627266432138 Training Accuracy 66.75\n",
      "Epoch 223 Loss 228.8290092187043 Training Accuracy 67.0\n",
      "Epoch 224 Loss 227.81538472985804 Training Accuracy 67.0\n",
      "Epoch 225 Loss 226.81517531553473 Training Accuracy 67.5\n",
      "Epoch 226 Loss 225.82816073143258 Training Accuracy 67.75\n",
      "Epoch 227 Loss 224.8541243372689 Training Accuracy 68.25\n",
      "Epoch 228 Loss 223.8928530615566 Training Accuracy 68.75\n",
      "Epoch 229 Loss 222.9441373651866 Training Accuracy 69.25\n",
      "Epoch 230 Loss 222.00777120391777 Training Accuracy 69.25\n",
      "Epoch 231 Loss 221.08355198987084 Training Accuracy 69.25\n",
      "Epoch 232 Loss 220.1712805521197 Training Accuracy 69.75\n",
      "Epoch 233 Loss 219.27076109647055 Training Accuracy 69.75\n",
      "Epoch 234 Loss 218.3818011645147 Training Accuracy 69.75\n",
      "Epoch 235 Loss 217.50421159203893 Training Accuracy 70.25\n",
      "Epoch 236 Loss 216.6378064668716 Training Accuracy 70.5\n",
      "Epoch 237 Loss 215.78240308624171 Training Accuracy 70.5\n",
      "Epoch 238 Loss 214.93782191372213 Training Accuracy 71.0\n",
      "Epoch 239 Loss 214.1038865358268 Training Accuracy 71.25\n",
      "Epoch 240 Loss 213.2804236183275 Training Accuracy 71.25\n",
      "Epoch 241 Loss 212.4672628623516 Training Accuracy 71.5\n",
      "Epoch 242 Loss 211.66423696032095 Training Accuracy 71.5\n",
      "Epoch 243 Loss 210.87118155178717 Training Accuracy 71.5\n",
      "Epoch 244 Loss 210.0879351792159 Training Accuracy 71.5\n",
      "Epoch 245 Loss 209.31433924377023 Training Accuracy 71.75\n",
      "Epoch 246 Loss 208.55023796114006 Training Accuracy 71.75\n",
      "Epoch 247 Loss 207.79547831746066 Training Accuracy 71.75\n",
      "Epoch 248 Loss 207.0499100253629 Training Accuracy 71.75\n",
      "Epoch 249 Loss 206.31338548019204 Training Accuracy 71.75\n",
      "Epoch 250 Loss 205.58575971643313 Training Accuracy 71.75\n",
      "Epoch 251 Loss 204.86689036437454 Training Accuracy 71.75\n",
      "Epoch 252 Loss 204.15663760704237 Training Accuracy 72.0\n",
      "Epoch 253 Loss 203.45486413743342 Training Accuracy 72.0\n",
      "Epoch 254 Loss 202.76143511607455 Training Accuracy 72.25\n",
      "Epoch 255 Loss 202.0762181289322 Training Accuracy 72.25\n",
      "Epoch 256 Loss 201.39908314569513 Training Accuracy 72.5\n",
      "Epoch 257 Loss 200.7299024784512 Training Accuracy 72.5\n",
      "Epoch 258 Loss 200.0685507407767 Training Accuracy 72.5\n",
      "Epoch 259 Loss 199.41490480725585 Training Accuracy 73.0\n",
      "Epoch 260 Loss 198.7688437734462 Training Accuracy 73.25\n",
      "Epoch 261 Loss 198.13024891630306 Training Accuracy 73.25\n",
      "Epoch 262 Loss 197.4990036550766 Training Accuracy 73.25\n",
      "Epoch 263 Loss 196.87499351269182 Training Accuracy 73.25\n",
      "Epoch 264 Loss 196.2581060776221 Training Accuracy 73.25\n",
      "Epoch 265 Loss 195.64823096626395 Training Accuracy 73.25\n",
      "Epoch 266 Loss 195.0452597858211 Training Accuracy 73.5\n",
      "Epoch 267 Loss 194.44908609770368 Training Accuracy 73.5\n",
      "Epoch 268 Loss 193.8596053814481 Training Accuracy 74.0\n",
      "Epoch 269 Loss 193.27671499916187 Training Accuracy 74.0\n",
      "Epoch 270 Loss 192.70031416049594 Training Accuracy 74.5\n",
      "Epoch 271 Loss 192.13030388814812 Training Accuracy 74.75\n",
      "Epoch 272 Loss 191.56658698389865 Training Accuracy 75.5\n",
      "Epoch 273 Loss 191.00906799517853 Training Accuracy 75.5\n",
      "Epoch 274 Loss 190.45765318217119 Training Accuracy 75.75\n",
      "Epoch 275 Loss 189.91225048544737 Training Accuracy 75.75\n",
      "Epoch 276 Loss 189.372769494131 Training Accuracy 76.0\n",
      "Epoch 277 Loss 188.8391214145956 Training Accuracy 76.0\n",
      "Epoch 278 Loss 188.31121903968847 Training Accuracy 76.0\n",
      "Epoch 279 Loss 187.78897671847974 Training Accuracy 76.0\n",
      "Epoch 280 Loss 187.2723103265338 Training Accuracy 76.0\n",
      "Epoch 281 Loss 186.76113723669937 Training Accuracy 76.25\n",
      "Epoch 282 Loss 186.25537629041358 Training Accuracy 76.5\n",
      "Epoch 283 Loss 185.75494776951717 Training Accuracy 76.5\n",
      "Epoch 284 Loss 185.259773368575 Training Accuracy 76.5\n",
      "Epoch 285 Loss 184.76977616769753 Training Accuracy 76.5\n",
      "Epoch 286 Loss 184.28488060585835 Training Accuracy 76.5\n",
      "Epoch 287 Loss 183.80501245470188 Training Accuracy 76.5\n",
      "Epoch 288 Loss 183.3300987928362 Training Accuracy 76.75\n",
      "Epoch 289 Loss 182.86006798060467 Training Accuracy 77.0\n",
      "Epoch 290 Loss 182.39484963533127 Training Accuracy 77.25\n",
      "Epoch 291 Loss 181.93437460703228 Training Accuracy 77.25\n",
      "Epoch 292 Loss 181.47857495458993 Training Accuracy 77.5\n",
      "Epoch 293 Loss 181.02738392237956 Training Accuracy 77.5\n",
      "Epoch 294 Loss 180.58073591734572 Training Accuracy 77.75\n",
      "Epoch 295 Loss 180.13856648651972 Training Accuracy 77.75\n",
      "Epoch 296 Loss 179.7008122949723 Training Accuracy 77.75\n",
      "Epoch 297 Loss 179.26741110419502 Training Accuracy 77.75\n",
      "Epoch 298 Loss 178.8383017509032 Training Accuracy 77.75\n",
      "Epoch 299 Loss 178.4134241262546 Training Accuracy 77.75\n",
      "Epoch 300 Loss 177.99271915547592 Training Accuracy 77.75\n",
      "Epoch 301 Loss 177.5761287778921 Training Accuracy 77.75\n",
      "Epoch 302 Loss 177.16359592734983 Training Accuracy 78.0\n",
      "Epoch 303 Loss 176.75506451303 Training Accuracy 78.0\n",
      "Epoch 304 Loss 176.35047940064183 Training Accuracy 78.0\n",
      "Epoch 305 Loss 175.94978639399216 Training Accuracy 78.0\n",
      "Epoch 306 Loss 175.55293221692347 Training Accuracy 78.0\n",
      "Epoch 307 Loss 175.15986449561328 Training Accuracy 78.25\n",
      "Epoch 308 Loss 174.77053174122972 Training Accuracy 78.5\n",
      "Epoch 309 Loss 174.38488333293577 Training Accuracy 78.5\n",
      "Epoch 310 Loss 174.00286950123572 Training Accuracy 78.75\n",
      "Epoch 311 Loss 173.6244413116586 Training Accuracy 78.75\n",
      "Epoch 312 Loss 173.2495506487706 Training Accuracy 79.0\n",
      "Epoch 313 Loss 172.8781502005119 Training Accuracy 79.0\n",
      "Epoch 314 Loss 172.51019344285083 Training Accuracy 79.0\n",
      "Epoch 315 Loss 172.14563462474888 Training Accuracy 79.0\n",
      "Epoch 316 Loss 171.78442875343202 Training Accuracy 79.0\n",
      "Epoch 317 Loss 171.42653157996074 Training Accuracy 79.0\n",
      "Epoch 318 Loss 171.07189958509417 Training Accuracy 79.5\n",
      "Epoch 319 Loss 170.72048996544177 Training Accuracy 79.5\n",
      "Epoch 320 Loss 170.37226061989716 Training Accuracy 79.5\n",
      "Epoch 321 Loss 170.02717013634864 Training Accuracy 79.5\n",
      "Epoch 322 Loss 169.68517777866055 Training Accuracy 79.5\n",
      "Epoch 323 Loss 169.34624347392008 Training Accuracy 79.5\n",
      "Epoch 324 Loss 169.01032779994475 Training Accuracy 80.0\n",
      "Epoch 325 Loss 168.67739197304388 Training Accuracy 80.0\n",
      "Epoch 326 Loss 168.3473978360309 Training Accuracy 80.0\n",
      "Epoch 327 Loss 168.0203078464795 Training Accuracy 80.0\n",
      "Epoch 328 Loss 167.69608506521962 Training Accuracy 80.0\n",
      "Epoch 329 Loss 167.3746931450682 Training Accuracy 80.0\n",
      "Epoch 330 Loss 167.05609631978967 Training Accuracy 80.0\n",
      "Epoch 331 Loss 166.7402593932816 Training Accuracy 80.0\n",
      "Epoch 332 Loss 166.4271477289809 Training Accuracy 80.0\n",
      "Epoch 333 Loss 166.11672723948547 Training Accuracy 80.0\n",
      "Epoch 334 Loss 165.8089643763876 Training Accuracy 80.0\n",
      "Epoch 335 Loss 165.50382612031416 Training Accuracy 80.0\n",
      "Epoch 336 Loss 165.2012799711693 Training Accuracy 80.0\n",
      "Epoch 337 Loss 164.90129393857555 Training Accuracy 80.0\n",
      "Epoch 338 Loss 164.6038365325092 Training Accuracy 80.25\n",
      "Epoch 339 Loss 164.30887675412566 Training Accuracy 80.25\n",
      "Epoch 340 Loss 164.01638408677076 Training Accuracy 80.25\n",
      "Epoch 341 Loss 163.72632848717467 Training Accuracy 80.25\n",
      "Epoch 342 Loss 163.43868037682367 Training Accuracy 80.5\n",
      "Epoch 343 Loss 163.1534106335066 Training Accuracy 80.5\n",
      "Epoch 344 Loss 162.87049058303216 Training Accuracy 81.0\n",
      "Epoch 345 Loss 162.58989199111346 Training Accuracy 81.0\n",
      "Epoch 346 Loss 162.31158705541625 Training Accuracy 81.0\n",
      "Epoch 347 Loss 162.03554839776706 Training Accuracy 81.0\n",
      "Epoch 348 Loss 161.76174905651874 Training Accuracy 81.0\n",
      "Epoch 349 Loss 161.49016247906854 Training Accuracy 81.0\n",
      "Epoch 350 Loss 161.2207625145273 Training Accuracy 81.25\n",
      "Epoch 351 Loss 160.9535234065351 Training Accuracy 81.25\n",
      "Epoch 352 Loss 160.68841978622072 Training Accuracy 81.25\n",
      "Epoch 353 Loss 160.4254266653022 Training Accuracy 81.75\n",
      "Epoch 354 Loss 160.16451942932494 Training Accuracy 81.75\n",
      "Epoch 355 Loss 159.90567383103485 Training Accuracy 81.75\n",
      "Epoch 356 Loss 159.64886598388324 Training Accuracy 81.75\n",
      "Epoch 357 Loss 159.3940723556611 Training Accuracy 81.75\n",
      "Epoch 358 Loss 159.14126976225972 Training Accuracy 81.75\n",
      "Epoch 359 Loss 158.8904353615549 Training Accuracy 81.75\n",
      "Epoch 360 Loss 158.6415466474126 Training Accuracy 81.75\n",
      "Epoch 361 Loss 158.3945814438126 Training Accuracy 81.75\n",
      "Epoch 362 Loss 158.14951789908858 Training Accuracy 81.75\n",
      "Epoch 363 Loss 157.90633448028157 Training Accuracy 81.75\n",
      "Epoch 364 Loss 157.66500996760428 Training Accuracy 81.75\n",
      "Epoch 365 Loss 157.42552344901463 Training Accuracy 81.75\n",
      "Epoch 366 Loss 157.18785431489525 Training Accuracy 81.75\n",
      "Epoch 367 Loss 156.95198225283744 Training Accuracy 81.75\n",
      "Epoch 368 Loss 156.71788724252707 Training Accuracy 82.0\n",
      "Epoch 369 Loss 156.48554955072996 Training Accuracy 82.0\n",
      "Epoch 370 Loss 156.25494972637554 Training Accuracy 82.0\n",
      "Epoch 371 Loss 156.02606859573538 Training Accuracy 82.0\n",
      "Epoch 372 Loss 155.798887257696 Training Accuracy 82.0\n",
      "Epoch 373 Loss 155.57338707912282 Training Accuracy 82.0\n",
      "Epoch 374 Loss 155.349549690314 Training Accuracy 82.0\n",
      "Epoch 375 Loss 155.12735698054195 Training Accuracy 82.0\n",
      "Epoch 376 Loss 154.9067910936809 Training Accuracy 82.0\n",
      "Epoch 377 Loss 154.68783442391847 Training Accuracy 82.0\n",
      "Epoch 378 Loss 154.47046961154956 Training Accuracy 82.0\n",
      "Epoch 379 Loss 154.25467953885112 Training Accuracy 82.0\n",
      "Epoch 380 Loss 154.04044732603538 Training Accuracy 82.0\n",
      "Epoch 381 Loss 153.82775632728084 Training Accuracy 82.0\n",
      "Epoch 382 Loss 153.61659012683862 Training Accuracy 82.0\n",
      "Epoch 383 Loss 153.40693253521277 Training Accuracy 82.0\n",
      "Epoch 384 Loss 153.19876758541358 Training Accuracy 82.0\n",
      "Epoch 385 Loss 152.9920795292813 Training Accuracy 82.0\n",
      "Epoch 386 Loss 152.78685283387983 Training Accuracy 82.0\n",
      "Epoch 387 Loss 152.58307217795806 Training Accuracy 82.0\n",
      "Epoch 388 Loss 152.38072244847845 Training Accuracy 82.0\n",
      "Epoch 389 Loss 152.1797887372103 Training Accuracy 82.0\n",
      "Epoch 390 Loss 151.98025633738726 Training Accuracy 81.75\n",
      "Epoch 391 Loss 151.78211074042736 Training Accuracy 82.0\n",
      "Epoch 392 Loss 151.58533763271424 Training Accuracy 82.0\n",
      "Epoch 393 Loss 151.38992289243862 Training Accuracy 82.0\n",
      "Epoch 394 Loss 151.19585258649843 Training Accuracy 82.25\n",
      "Epoch 395 Loss 151.00311296745633 Training Accuracy 82.25\n",
      "Epoch 396 Loss 150.81169047055417 Training Accuracy 82.25\n",
      "Epoch 397 Loss 150.62157171078212 Training Accuracy 82.25\n",
      "Epoch 398 Loss 150.43274348000196 Training Accuracy 82.25\n",
      "Epoch 399 Loss 150.24519274412395 Training Accuracy 82.25\n",
      "Epoch 400 Loss 150.05890664033495 Training Accuracy 82.25\n",
      "Epoch 401 Loss 149.87387247437746 Training Accuracy 82.25\n",
      "Epoch 402 Loss 149.69007771787906 Training Accuracy 82.5\n",
      "Epoch 403 Loss 149.50751000572987 Training Accuracy 82.5\n",
      "Epoch 404 Loss 149.3261571335084 Training Accuracy 82.5\n",
      "Epoch 405 Loss 149.14600705495394 Training Accuracy 82.75\n",
      "Epoch 406 Loss 148.967047879485 Training Accuracy 82.75\n",
      "Epoch 407 Loss 148.7892678697624 Training Accuracy 82.75\n",
      "Epoch 408 Loss 148.61265543929704 Training Accuracy 82.75\n",
      "Epoch 409 Loss 148.43719915009999 Training Accuracy 82.75\n",
      "Epoch 410 Loss 148.26288771037576 Training Accuracy 82.75\n",
      "Epoch 411 Loss 148.08970997225626 Training Accuracy 82.75\n",
      "Epoch 412 Loss 147.9176549295759 Training Accuracy 82.75\n",
      "Epoch 413 Loss 147.7467117156862 Training Accuracy 82.75\n",
      "Epoch 414 Loss 147.57686960130945 Training Accuracy 82.75\n",
      "Epoch 415 Loss 147.40811799243085 Training Accuracy 82.75\n",
      "Epoch 416 Loss 147.24044642822767 Training Accuracy 83.0\n",
      "Epoch 417 Loss 147.07384457903547 Training Accuracy 83.0\n",
      "Epoch 418 Loss 146.9083022443503 Training Accuracy 83.0\n",
      "Epoch 419 Loss 146.74380935086597 Training Accuracy 83.0\n",
      "Epoch 420 Loss 146.58035595054608 Training Accuracy 83.0\n",
      "Epoch 421 Loss 146.41793221872987 Training Accuracy 83.0\n",
      "Epoch 422 Loss 146.25652845227145 Training Accuracy 83.0\n",
      "Epoch 423 Loss 146.09613506771123 Training Accuracy 83.0\n",
      "Epoch 424 Loss 145.93674259947977 Training Accuracy 83.0\n",
      "Epoch 425 Loss 145.7783416981326 Training Accuracy 83.0\n",
      "Epoch 426 Loss 145.6209231286158 Training Accuracy 83.0\n",
      "Epoch 427 Loss 145.4644777685619 Training Accuracy 83.0\n",
      "Epoch 428 Loss 145.30899660661493 Training Accuracy 83.0\n",
      "Epoch 429 Loss 145.15447074078466 Training Accuracy 83.0\n",
      "Epoch 430 Loss 145.00089137682923 Training Accuracy 83.0\n",
      "Epoch 431 Loss 144.84824982666527 Training Accuracy 83.0\n",
      "Epoch 432 Loss 144.69653750680587 Training Accuracy 83.25\n",
      "Epoch 433 Loss 144.5457459368248 Training Accuracy 83.25\n",
      "Epoch 434 Loss 144.39586673784729 Training Accuracy 83.25\n",
      "Epoch 435 Loss 144.2468916310662 Training Accuracy 83.5\n",
      "Epoch 436 Loss 144.098812436284 Training Accuracy 83.5\n",
      "Epoch 437 Loss 143.95162107047884 Training Accuracy 83.5\n",
      "Epoch 438 Loss 143.8053095463955 Training Accuracy 83.5\n",
      "Epoch 439 Loss 143.65986997115976 Training Accuracy 83.5\n",
      "Epoch 440 Loss 143.5152945449163 Training Accuracy 83.5\n",
      "Epoch 441 Loss 143.37157555948943 Training Accuracy 83.5\n",
      "Epoch 442 Loss 143.22870539706662 Training Accuracy 83.75\n",
      "Epoch 443 Loss 143.08667652890358 Training Accuracy 83.75\n",
      "Epoch 444 Loss 142.94548151405138 Training Accuracy 83.75\n",
      "Epoch 445 Loss 142.80511299810465 Training Accuracy 83.75\n",
      "Epoch 446 Loss 142.66556371197078 Training Accuracy 83.75\n",
      "Epoch 447 Loss 142.52682647065913 Training Accuracy 83.75\n",
      "Epoch 448 Loss 142.38889417209103 Training Accuracy 84.0\n",
      "Epoch 449 Loss 142.25175979592854 Training Accuracy 84.0\n",
      "Epoch 450 Loss 142.11541640242353 Training Accuracy 84.0\n",
      "Epoch 451 Loss 141.97985713128477 Training Accuracy 84.0\n",
      "Epoch 452 Loss 141.84507520056437 Training Accuracy 84.0\n",
      "Epoch 453 Loss 141.71106390556199 Training Accuracy 84.0\n",
      "Epoch 454 Loss 141.57781661774723 Training Accuracy 84.0\n",
      "Epoch 455 Loss 141.44532678369956 Training Accuracy 84.0\n",
      "Epoch 456 Loss 141.3135879240653 Training Accuracy 84.0\n",
      "Epoch 457 Loss 141.18259363253202 Training Accuracy 84.0\n",
      "Epoch 458 Loss 141.05233757481892 Training Accuracy 84.0\n",
      "Epoch 459 Loss 140.9228134876841 Training Accuracy 84.0\n",
      "Epoch 460 Loss 140.7940151779477 Training Accuracy 84.25\n",
      "Epoch 461 Loss 140.66593652153054 Training Accuracy 84.25\n",
      "Epoch 462 Loss 140.53857146250837 Training Accuracy 84.25\n",
      "Epoch 463 Loss 140.4119140121815 Training Accuracy 84.25\n",
      "Epoch 464 Loss 140.28595824815903 Training Accuracy 84.25\n",
      "Epoch 465 Loss 140.16069831345763 Training Accuracy 84.25\n",
      "Epoch 466 Loss 140.03612841561494 Training Accuracy 84.25\n",
      "Epoch 467 Loss 139.91224282581706 Training Accuracy 84.25\n",
      "Epoch 468 Loss 139.7890358780396 Training Accuracy 84.25\n",
      "Epoch 469 Loss 139.66650196820254 Training Accuracy 84.25\n",
      "Epoch 470 Loss 139.5446355533384 Training Accuracy 84.5\n",
      "Epoch 471 Loss 139.42343115077358 Training Accuracy 84.5\n",
      "Epoch 472 Loss 139.3028833373222 Training Accuracy 84.5\n",
      "Epoch 473 Loss 139.18298674849333 Training Accuracy 84.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 474 Loss 139.06373607770976 Training Accuracy 84.5\n",
      "Epoch 475 Loss 138.9451260755396 Training Accuracy 84.5\n",
      "Epoch 476 Loss 138.82715154893964 Training Accuracy 84.5\n",
      "Epoch 477 Loss 138.7098073605104 Training Accuracy 84.5\n",
      "Epoch 478 Loss 138.59308842776292 Training Accuracy 84.5\n",
      "Epoch 479 Loss 138.47698972239667 Training Accuracy 84.5\n",
      "Epoch 480 Loss 138.36150626958886 Training Accuracy 84.5\n",
      "Epoch 481 Loss 138.24663314729457 Training Accuracy 84.5\n",
      "Epoch 482 Loss 138.13236548555764 Training Accuracy 84.5\n",
      "Epoch 483 Loss 138.01869846583222 Training Accuracy 84.5\n",
      "Epoch 484 Loss 137.90562732031458 Training Accuracy 84.5\n",
      "Epoch 485 Loss 137.79314733128524 Training Accuracy 84.5\n",
      "Epoch 486 Loss 137.6812538304612 Training Accuracy 84.5\n",
      "Epoch 487 Loss 137.56994219835778 Training Accuracy 84.5\n",
      "Epoch 488 Loss 137.4592078636604 Training Accuracy 84.5\n",
      "Epoch 489 Loss 137.34904630260576 Training Accuracy 84.5\n",
      "Epoch 490 Loss 137.23945303837235 Training Accuracy 84.5\n",
      "Epoch 491 Loss 137.13042364048025 Training Accuracy 84.5\n",
      "Epoch 492 Loss 137.0219537241999 Training Accuracy 84.5\n",
      "Epoch 493 Loss 136.91403894996978 Training Accuracy 84.5\n",
      "Epoch 494 Loss 136.80667502282276 Training Accuracy 84.75\n",
      "Epoch 495 Loss 136.69985769182114 Training Accuracy 84.75\n",
      "Epoch 496 Loss 136.59358274949994 Training Accuracy 84.75\n",
      "Epoch 497 Loss 136.4878460313188 Training Accuracy 84.75\n",
      "Epoch 498 Loss 136.38264341512166 Training Accuracy 84.75\n",
      "Epoch 499 Loss 136.2779708206046 Training Accuracy 84.75\n",
      "Epoch 500 Loss 136.1738242087916 Training Accuracy 84.75\n",
      "Epoch 501 Loss 136.07019958151815 Training Accuracy 84.75\n",
      "Epoch 502 Loss 135.9670929809222 Training Accuracy 85.0\n",
      "Epoch 503 Loss 135.86450048894272 Training Accuracy 85.0\n",
      "Epoch 504 Loss 135.76241822682556 Training Accuracy 85.0\n",
      "Epoch 505 Loss 135.6608423546368 Training Accuracy 85.25\n",
      "Epoch 506 Loss 135.55976907078292 Training Accuracy 85.25\n",
      "Epoch 507 Loss 135.45919461153795 Training Accuracy 85.25\n",
      "Epoch 508 Loss 135.35911525057776 Training Accuracy 85.25\n",
      "Epoch 509 Loss 135.25952729852094 Training Accuracy 85.25\n",
      "Epoch 510 Loss 135.1604271024763 Training Accuracy 85.25\n",
      "Epoch 511 Loss 135.06181104559698 Training Accuracy 85.25\n",
      "Epoch 512 Loss 134.96367554664099 Training Accuracy 85.25\n",
      "Epoch 513 Loss 134.86601705953802 Training Accuracy 85.25\n",
      "Epoch 514 Loss 134.76883207296262 Training Accuracy 85.25\n",
      "Epoch 515 Loss 134.67211710991333 Training Accuracy 85.25\n",
      "Epoch 516 Loss 134.57586872729772 Training Accuracy 85.25\n",
      "Epoch 517 Loss 134.48008351552386 Training Accuracy 85.25\n",
      "Epoch 518 Loss 134.38475809809705 Training Accuracy 85.25\n",
      "Epoch 519 Loss 134.2898891312225 Training Accuracy 85.25\n",
      "Epoch 520 Loss 134.1954733034139 Training Accuracy 85.25\n",
      "Epoch 521 Loss 134.10150733510685 Training Accuracy 85.25\n",
      "Epoch 522 Loss 134.0079879782786 Training Accuracy 85.25\n",
      "Epoch 523 Loss 133.91491201607255 Training Accuracy 85.25\n",
      "Epoch 524 Loss 133.8222762624281 Training Accuracy 85.25\n",
      "Epoch 525 Loss 133.73007756171612 Training Accuracy 85.25\n",
      "Epoch 526 Loss 133.63831278837898 Training Accuracy 85.25\n",
      "Epoch 527 Loss 133.54697884657597 Training Accuracy 85.25\n",
      "Epoch 528 Loss 133.4560726698333 Training Accuracy 85.25\n",
      "Epoch 529 Loss 133.36559122069968 Training Accuracy 85.25\n",
      "Epoch 530 Loss 133.27553149040577 Training Accuracy 85.25\n",
      "Epoch 531 Loss 133.18589049852903 Training Accuracy 85.25\n",
      "Epoch 532 Loss 133.0966652926631 Training Accuracy 85.25\n",
      "Epoch 533 Loss 133.00785294809128 Training Accuracy 85.25\n",
      "Epoch 534 Loss 132.91945056746516 Training Accuracy 85.5\n",
      "Epoch 535 Loss 132.83145528048723 Training Accuracy 85.5\n",
      "Epoch 536 Loss 132.74386424359807 Training Accuracy 85.5\n",
      "Epoch 537 Loss 132.65667463966764 Training Accuracy 85.5\n",
      "Epoch 538 Loss 132.5698836776909 Training Accuracy 85.5\n",
      "Epoch 539 Loss 132.48348859248765 Training Accuracy 85.5\n",
      "Epoch 540 Loss 132.39748664440626 Training Accuracy 85.5\n",
      "Epoch 541 Loss 132.3118751190316 Training Accuracy 85.5\n",
      "Epoch 542 Loss 132.22665132689687 Training Accuracy 85.5\n",
      "Epoch 543 Loss 132.14181260319927 Training Accuracy 85.5\n",
      "Epoch 544 Loss 132.0573563075196 Training Accuracy 85.5\n",
      "Epoch 545 Loss 131.9732798235456 Training Accuracy 85.5\n",
      "Epoch 546 Loss 131.889580558799 Training Accuracy 85.5\n",
      "Epoch 547 Loss 131.80625594436617 Training Accuracy 85.5\n",
      "Epoch 548 Loss 131.72330343463256 Training Accuracy 85.5\n",
      "Epoch 549 Loss 131.64072050702052 Training Accuracy 85.5\n",
      "Epoch 550 Loss 131.5585046617307 Training Accuracy 85.5\n",
      "Epoch 551 Loss 131.4766534214868 Training Accuracy 85.5\n",
      "Epoch 552 Loss 131.39516433128375 Training Accuracy 85.5\n",
      "Epoch 553 Loss 131.31403495813942 Training Accuracy 85.5\n",
      "Epoch 554 Loss 131.23326289084935 Training Accuracy 85.5\n",
      "Epoch 555 Loss 131.1528457397447 Training Accuracy 85.5\n",
      "Epoch 556 Loss 131.0727811364537 Training Accuracy 85.5\n",
      "Epoch 557 Loss 130.99306673366584 Training Accuracy 85.5\n",
      "Epoch 558 Loss 130.9137002048996 Training Accuracy 85.5\n",
      "Epoch 559 Loss 130.83467924427282 Training Accuracy 85.75\n",
      "Epoch 560 Loss 130.75600156627615 Training Accuracy 85.75\n",
      "Epoch 561 Loss 130.6776649055497 Training Accuracy 85.75\n",
      "Epoch 562 Loss 130.5996670166624 Training Accuracy 85.75\n",
      "Epoch 563 Loss 130.52200567389423 Training Accuracy 85.75\n",
      "Epoch 564 Loss 130.44467867102128 Training Accuracy 85.75\n",
      "Epoch 565 Loss 130.36768382110358 Training Accuracy 85.75\n",
      "Epoch 566 Loss 130.291018956276 Training Accuracy 85.75\n",
      "Epoch 567 Loss 130.214681927541 Training Accuracy 85.75\n",
      "Epoch 568 Loss 130.1386706045652 Training Accuracy 85.75\n",
      "Epoch 569 Loss 130.06298287547756 Training Accuracy 85.75\n",
      "Epoch 570 Loss 129.98761664667063 Training Accuracy 85.75\n",
      "Epoch 571 Loss 129.91256984260443 Training Accuracy 85.75\n",
      "Epoch 572 Loss 129.83784040561238 Training Accuracy 85.75\n",
      "Epoch 573 Loss 129.7634262957103 Training Accuracy 85.75\n",
      "Epoch 574 Loss 129.68932549040727 Training Accuracy 85.75\n",
      "Epoch 575 Loss 129.61553598451923 Training Accuracy 85.75\n",
      "Epoch 576 Loss 129.54205578998494 Training Accuracy 85.75\n",
      "Epoch 577 Loss 129.46888293568412 Training Accuracy 85.75\n",
      "Epoch 578 Loss 129.39601546725777 Training Accuracy 85.75\n",
      "Epoch 579 Loss 129.3234514469313 Training Accuracy 85.75\n",
      "Epoch 580 Loss 129.25118895333918 Training Accuracy 85.75\n",
      "Epoch 581 Loss 129.1792260813523 Training Accuracy 85.75\n",
      "Epoch 582 Loss 129.10756094190728 Training Accuracy 86.0\n",
      "Epoch 583 Loss 129.0361916618379 Training Accuracy 86.0\n",
      "Epoch 584 Loss 128.96511638370873 Training Accuracy 86.0\n",
      "Epoch 585 Loss 128.8943332656507 Training Accuracy 86.0\n",
      "Epoch 586 Loss 128.8238404811989 Training Accuracy 86.0\n",
      "Epoch 587 Loss 128.75363621913223 Training Accuracy 86.0\n",
      "Epoch 588 Loss 128.6837186833151 Training Accuracy 86.0\n",
      "Epoch 589 Loss 128.61408609254107 Training Accuracy 86.0\n",
      "Epoch 590 Loss 128.5447366803783 Training Accuracy 86.0\n",
      "Epoch 591 Loss 128.47566869501728 Training Accuracy 86.0\n",
      "Epoch 592 Loss 128.40688039911996 Training Accuracy 86.0\n",
      "Epoch 593 Loss 128.338370069671 Training Accuracy 86.0\n",
      "Epoch 594 Loss 128.27013599783078 Training Accuracy 86.0\n",
      "Epoch 595 Loss 128.20217648879023 Training Accuracy 86.0\n",
      "Epoch 596 Loss 128.13448986162746 Training Accuracy 86.0\n",
      "Epoch 597 Loss 128.06707444916586 Training Accuracy 86.0\n",
      "Epoch 598 Loss 127.99992859783438 Training Accuracy 86.25\n",
      "Epoch 599 Loss 127.93305066752922 Training Accuracy 86.25\n",
      "Epoch 600 Loss 127.86643903147709 Training Accuracy 86.25\n",
      "Epoch 601 Loss 127.80009207610038 Training Accuracy 86.5\n",
      "Epoch 602 Loss 127.73400820088396 Training Accuracy 86.5\n",
      "Epoch 603 Loss 127.66818581824323 Training Accuracy 86.5\n",
      "Epoch 604 Loss 127.60262335339411 Training Accuracy 86.5\n",
      "Epoch 605 Loss 127.53731924422452 Training Accuracy 86.5\n",
      "Epoch 606 Loss 127.47227194116722 Training Accuracy 86.5\n",
      "Epoch 607 Loss 127.40747990707439 Training Accuracy 86.5\n",
      "Epoch 608 Loss 127.3429416170936 Training Accuracy 86.5\n",
      "Epoch 609 Loss 127.27865555854525 Training Accuracy 86.5\n",
      "Epoch 610 Loss 127.2146202308014 Training Accuracy 86.5\n",
      "Epoch 611 Loss 127.1508341451662 Training Accuracy 86.5\n",
      "Epoch 612 Loss 127.0872958247577 Training Accuracy 86.5\n",
      "Epoch 613 Loss 127.02400380439087 Training Accuracy 86.5\n",
      "Epoch 614 Loss 126.96095663046218 Training Accuracy 86.5\n",
      "Epoch 615 Loss 126.89815286083558 Training Accuracy 86.5\n",
      "Epoch 616 Loss 126.83559106472954 Training Accuracy 86.5\n",
      "Epoch 617 Loss 126.77326982260583 Training Accuracy 86.5\n",
      "Epoch 618 Loss 126.71118772605918 Training Accuracy 86.5\n",
      "Epoch 619 Loss 126.6493433777085 Training Accuracy 86.5\n",
      "Epoch 620 Loss 126.58773539108932 Training Accuracy 86.5\n",
      "Epoch 621 Loss 126.52636239054739 Training Accuracy 86.5\n",
      "Epoch 622 Loss 126.4652230111336 Training Accuracy 86.5\n",
      "Epoch 623 Loss 126.40431589850009 Training Accuracy 86.5\n",
      "Epoch 624 Loss 126.34363970879754 Training Accuracy 86.5\n",
      "Epoch 625 Loss 126.28319310857374 Training Accuracy 86.5\n",
      "Epoch 626 Loss 126.22297477467319 Training Accuracy 86.5\n",
      "Epoch 627 Loss 126.16298339413795 Training Accuracy 86.5\n",
      "Epoch 628 Loss 126.10321766410961 Training Accuracy 86.25\n",
      "Epoch 629 Loss 126.04367629173237 Training Accuracy 86.25\n",
      "Epoch 630 Loss 125.98435799405725 Training Accuracy 86.25\n",
      "Epoch 631 Loss 125.92526149794733 Training Accuracy 86.5\n",
      "Epoch 632 Loss 125.86638553998422 Training Accuracy 86.5\n",
      "Epoch 633 Loss 125.80772886637541 Training Accuracy 86.5\n",
      "Epoch 634 Loss 125.74929023286273 Training Accuracy 86.5\n",
      "Epoch 635 Loss 125.691068404632 Training Accuracy 86.5\n",
      "Epoch 636 Loss 125.6330621562235 Training Accuracy 86.5\n",
      "Epoch 637 Loss 125.57527027144354 Training Accuracy 86.5\n",
      "Epoch 638 Loss 125.51769154327698 Training Accuracy 86.5\n",
      "Epoch 639 Loss 125.46032477380086 Training Accuracy 86.5\n",
      "Epoch 640 Loss 125.40316877409884 Training Accuracy 86.5\n",
      "Epoch 641 Loss 125.3462223641767 Training Accuracy 86.5\n",
      "Epoch 642 Loss 125.2894843728788 Training Accuracy 86.5\n",
      "Epoch 643 Loss 125.23295363780531 Training Accuracy 86.5\n",
      "Epoch 644 Loss 125.1766290052307 Training Accuracy 86.5\n",
      "Epoch 645 Loss 125.12050933002267 Training Accuracy 86.5\n",
      "Epoch 646 Loss 125.06459347556245 Training Accuracy 86.5\n",
      "Epoch 647 Loss 125.00888031366566 Training Accuracy 86.5\n",
      "Epoch 648 Loss 124.95336872450419 Training Accuracy 86.5\n",
      "Epoch 649 Loss 124.89805759652891 Training Accuracy 86.5\n",
      "Epoch 650 Loss 124.84294582639328 Training Accuracy 86.5\n",
      "Epoch 651 Loss 124.7880323188776 Training Accuracy 86.75\n",
      "Epoch 652 Loss 124.73331598681449 Training Accuracy 86.75\n",
      "Epoch 653 Loss 124.67879575101482 Training Accuracy 86.75\n",
      "Epoch 654 Loss 124.6244705401945 Training Accuracy 86.75\n",
      "Epoch 655 Loss 124.57033929090231 Training Accuracy 86.75\n",
      "Epoch 656 Loss 124.51640094744835 Training Accuracy 86.75\n",
      "Epoch 657 Loss 124.46265446183313 Training Accuracy 86.75\n",
      "Epoch 658 Loss 124.40909879367771 Training Accuracy 86.75\n",
      "Epoch 659 Loss 124.35573291015454 Training Accuracy 86.75\n",
      "Epoch 660 Loss 124.30255578591881 Training Accuracy 86.75\n",
      "Epoch 661 Loss 124.24956640304079 Training Accuracy 86.75\n",
      "Epoch 662 Loss 124.19676375093894 Training Accuracy 86.75\n",
      "Epoch 663 Loss 124.14414682631343 Training Accuracy 86.75\n",
      "Epoch 664 Loss 124.09171463308078 Training Accuracy 86.75\n",
      "Epoch 665 Loss 124.03946618230883 Training Accuracy 86.75\n",
      "Epoch 666 Loss 123.98740049215267 Training Accuracy 86.75\n",
      "Epoch 667 Loss 123.93551658779124 Training Accuracy 86.75\n",
      "Epoch 668 Loss 123.88381350136432 Training Accuracy 86.75\n",
      "Epoch 669 Loss 123.83229027191065 Training Accuracy 86.75\n",
      "Epoch 670 Loss 123.78094594530626 Training Accuracy 86.75\n",
      "Epoch 671 Loss 123.72977957420382 Training Accuracy 86.75\n",
      "Epoch 672 Loss 123.67879021797242 Training Accuracy 86.75\n",
      "Epoch 673 Loss 123.62797694263803 Training Accuracy 86.75\n",
      "Epoch 674 Loss 123.57733882082462 Training Accuracy 86.75\n",
      "Epoch 675 Loss 123.52687493169591 Training Accuracy 86.75\n",
      "Epoch 676 Loss 123.4765843608977 Training Accuracy 86.75\n",
      "Epoch 677 Loss 123.42646620050085 Training Accuracy 86.75\n",
      "Epoch 678 Loss 123.3765195489448 Training Accuracy 86.75\n",
      "Epoch 679 Loss 123.32674351098167 Training Accuracy 86.75\n",
      "Epoch 680 Loss 123.27713719762119 Training Accuracy 86.75\n",
      "Epoch 681 Loss 123.22769972607576 Training Accuracy 86.75\n",
      "Epoch 682 Loss 123.17843021970648 Training Accuracy 86.75\n",
      "Epoch 683 Loss 123.12932780796957 Training Accuracy 86.75\n",
      "Epoch 684 Loss 123.08039162636342 Training Accuracy 86.75\n",
      "Epoch 685 Loss 123.031620816376 Training Accuracy 86.75\n",
      "Epoch 686 Loss 122.9830145254331 Training Accuracy 86.75\n",
      "Epoch 687 Loss 122.93457190684694 Training Accuracy 86.75\n",
      "Epoch 688 Loss 122.88629211976533 Training Accuracy 86.75\n",
      "Epoch 689 Loss 122.83817432912136 Training Accuracy 86.75\n",
      "Epoch 690 Loss 122.79021770558354 Training Accuracy 86.75\n",
      "Epoch 691 Loss 122.74242142550668 Training Accuracy 86.75\n",
      "Epoch 692 Loss 122.69478467088304 Training Accuracy 86.75\n",
      "Epoch 693 Loss 122.64730662929401 Training Accuracy 86.75\n",
      "Epoch 694 Loss 122.59998649386245 Training Accuracy 86.75\n",
      "Epoch 695 Loss 122.55282346320527 Training Accuracy 86.75\n",
      "Epoch 696 Loss 122.50581674138677 Training Accuracy 86.75\n",
      "Epoch 697 Loss 122.4589655378722 Training Accuracy 86.75\n",
      "Epoch 698 Loss 122.41226906748199 Training Accuracy 86.75\n",
      "Epoch 699 Loss 122.36572655034634 Training Accuracy 86.75\n",
      "Epoch 700 Loss 122.3193372118602 Training Accuracy 86.75\n",
      "Epoch 701 Loss 122.27310028263895 Training Accuracy 86.75\n",
      "Epoch 702 Loss 122.22701499847429 Training Accuracy 86.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 703 Loss 122.18108060029061 Training Accuracy 86.75\n",
      "Epoch 704 Loss 122.13529633410201 Training Accuracy 86.75\n",
      "Epoch 705 Loss 122.08966145096944 Training Accuracy 86.75\n",
      "Epoch 706 Loss 122.04417520695847 Training Accuracy 86.75\n",
      "Epoch 707 Loss 121.99883686309748 Training Accuracy 86.75\n",
      "Epoch 708 Loss 121.95364568533618 Training Accuracy 86.75\n",
      "Epoch 709 Loss 121.90860094450463 Training Accuracy 86.75\n",
      "Epoch 710 Loss 121.86370191627249 Training Accuracy 86.75\n",
      "Epoch 711 Loss 121.8189478811091 Training Accuracy 86.75\n",
      "Epoch 712 Loss 121.77433812424331 Training Accuracy 86.75\n",
      "Epoch 713 Loss 121.72987193562446 Training Accuracy 86.75\n",
      "Epoch 714 Loss 121.68554860988293 Training Accuracy 86.75\n",
      "Epoch 715 Loss 121.64136744629192 Training Accuracy 86.75\n",
      "Epoch 716 Loss 121.5973277487289 Training Accuracy 86.75\n",
      "Epoch 717 Loss 121.5534288256379 Training Accuracy 86.75\n",
      "Epoch 718 Loss 121.50966998999192 Training Accuracy 86.75\n",
      "Epoch 719 Loss 121.46605055925583 Training Accuracy 86.75\n",
      "Epoch 720 Loss 121.42256985534964 Training Accuracy 86.75\n",
      "Epoch 721 Loss 121.37922720461192 Training Accuracy 86.75\n",
      "Epoch 722 Loss 121.33602193776407 Training Accuracy 86.75\n",
      "Epoch 723 Loss 121.29295338987424 Training Accuracy 86.75\n",
      "Epoch 724 Loss 121.25002090032234 Training Accuracy 86.75\n",
      "Epoch 725 Loss 121.20722381276468 Training Accuracy 86.75\n",
      "Epoch 726 Loss 121.16456147509959 Training Accuracy 86.75\n",
      "Epoch 727 Loss 121.12203323943298 Training Accuracy 86.75\n",
      "Epoch 728 Loss 121.0796384620443 Training Accuracy 86.75\n",
      "Epoch 729 Loss 121.03737650335295 Training Accuracy 86.75\n",
      "Epoch 730 Loss 120.99524672788483 Training Accuracy 86.75\n",
      "Epoch 731 Loss 120.9532485042395 Training Accuracy 86.75\n",
      "Epoch 732 Loss 120.91138120505727 Training Accuracy 86.75\n",
      "Epoch 733 Loss 120.86964420698695 Training Accuracy 86.75\n",
      "Epoch 734 Loss 120.82803689065369 Training Accuracy 86.75\n",
      "Epoch 735 Loss 120.78655864062733 Training Accuracy 86.75\n",
      "Epoch 736 Loss 120.74520884539072 Training Accuracy 86.75\n",
      "Epoch 737 Loss 120.70398689730882 Training Accuracy 86.75\n",
      "Epoch 738 Loss 120.66289219259762 Training Accuracy 86.75\n",
      "Epoch 739 Loss 120.62192413129367 Training Accuracy 86.75\n",
      "Epoch 740 Loss 120.58108211722373 Training Accuracy 87.0\n",
      "Epoch 741 Loss 120.54036555797485 Training Accuracy 87.0\n",
      "Epoch 742 Loss 120.49977386486462 Training Accuracy 87.25\n",
      "Epoch 743 Loss 120.45930645291175 Training Accuracy 87.25\n",
      "Epoch 744 Loss 120.41896274080685 Training Accuracy 87.25\n",
      "Epoch 745 Loss 120.3787421508837 Training Accuracy 87.25\n",
      "Epoch 746 Loss 120.3386441090905 Training Accuracy 87.25\n",
      "Epoch 747 Loss 120.29866804496164 Training Accuracy 87.25\n",
      "Epoch 748 Loss 120.25881339158954 Training Accuracy 87.25\n",
      "Epoch 749 Loss 120.21907958559692 Training Accuracy 87.25\n",
      "Epoch 750 Loss 120.17946606710922 Training Accuracy 87.25\n",
      "Epoch 751 Loss 120.13997227972729 Training Accuracy 87.25\n",
      "Epoch 752 Loss 120.10059767050039 Training Accuracy 87.25\n",
      "Epoch 753 Loss 120.06134168989949 Training Accuracy 87.25\n",
      "Epoch 754 Loss 120.02220379179053 Training Accuracy 87.25\n",
      "Epoch 755 Loss 119.98318343340831 Training Accuracy 87.25\n",
      "Epoch 756 Loss 119.94428007533044 Training Accuracy 87.25\n",
      "Epoch 757 Loss 119.90549318145136 Training Accuracy 87.25\n",
      "Epoch 758 Loss 119.86682221895708 Training Accuracy 87.25\n",
      "Epoch 759 Loss 119.82826665829963 Training Accuracy 87.25\n",
      "Epoch 760 Loss 119.78982597317193 Training Accuracy 87.25\n",
      "Epoch 761 Loss 119.75149964048322 Training Accuracy 87.25\n",
      "Epoch 762 Loss 119.71328714033407 Training Accuracy 87.25\n",
      "Epoch 763 Loss 119.67518795599227 Training Accuracy 87.25\n",
      "Epoch 764 Loss 119.63720157386838 Training Accuracy 87.25\n",
      "Epoch 765 Loss 119.59932748349206 Training Accuracy 87.25\n",
      "Epoch 766 Loss 119.56156517748808 Training Accuracy 87.25\n",
      "Epoch 767 Loss 119.52391415155296 Training Accuracy 87.25\n",
      "Epoch 768 Loss 119.48637390443153 Training Accuracy 87.25\n",
      "Epoch 769 Loss 119.44894393789397 Training Accuracy 87.25\n",
      "Epoch 770 Loss 119.41162375671281 Training Accuracy 87.25\n",
      "Epoch 771 Loss 119.37441286864028 Training Accuracy 87.25\n",
      "Epoch 772 Loss 119.33731078438593 Training Accuracy 87.25\n",
      "Epoch 773 Loss 119.30031701759414 Training Accuracy 87.25\n",
      "Epoch 774 Loss 119.26343108482227 Training Accuracy 87.25\n",
      "Epoch 775 Loss 119.22665250551874 Training Accuracy 87.25\n",
      "Epoch 776 Loss 119.1899808020012 Training Accuracy 87.5\n",
      "Epoch 777 Loss 119.15341549943528 Training Accuracy 87.5\n",
      "Epoch 778 Loss 119.11695612581315 Training Accuracy 87.5\n",
      "Epoch 779 Loss 119.08060221193246 Training Accuracy 87.5\n",
      "Epoch 780 Loss 119.0443532913755 Training Accuracy 87.5\n",
      "Epoch 781 Loss 119.0082089004883 Training Accuracy 87.5\n",
      "Epoch 782 Loss 118.97216857836031 Training Accuracy 87.5\n",
      "Epoch 783 Loss 118.93623186680395 Training Accuracy 87.5\n",
      "Epoch 784 Loss 118.90039831033445 Training Accuracy 87.5\n",
      "Epoch 785 Loss 118.8646674561498 Training Accuracy 87.5\n",
      "Epoch 786 Loss 118.82903885411102 Training Accuracy 87.5\n",
      "Epoch 787 Loss 118.79351205672252 Training Accuracy 87.5\n",
      "Epoch 788 Loss 118.75808661911256 Training Accuracy 87.5\n",
      "Epoch 789 Loss 118.722762099014 Training Accuracy 87.5\n",
      "Epoch 790 Loss 118.68753805674523 Training Accuracy 87.5\n",
      "Epoch 791 Loss 118.65241405519112 Training Accuracy 87.5\n",
      "Epoch 792 Loss 118.61738965978425 Training Accuracy 87.5\n",
      "Epoch 793 Loss 118.58246443848643 Training Accuracy 87.5\n",
      "Epoch 794 Loss 118.54763796177004 Training Accuracy 87.5\n",
      "Epoch 795 Loss 118.51290980259982 Training Accuracy 87.5\n",
      "Epoch 796 Loss 118.47827953641476 Training Accuracy 87.5\n",
      "Epoch 797 Loss 118.44374674111009 Training Accuracy 87.5\n",
      "Epoch 798 Loss 118.4093109970194 Training Accuracy 87.5\n",
      "Epoch 799 Loss 118.37497188689716 Training Accuracy 87.5\n",
      "Epoch 800 Loss 118.34072899590099 Training Accuracy 87.5\n",
      "Epoch 801 Loss 118.3065819115744 Training Accuracy 87.5\n",
      "Epoch 802 Loss 118.27253022382959 Training Accuracy 87.5\n",
      "Epoch 803 Loss 118.23857352493035 Training Accuracy 87.5\n",
      "Epoch 804 Loss 118.2047114094752 Training Accuracy 87.5\n",
      "Epoch 805 Loss 118.1709434743805 Training Accuracy 87.5\n",
      "Epoch 806 Loss 118.13726931886401 Training Accuracy 87.5\n",
      "Epoch 807 Loss 118.10368854442831 Training Accuracy 87.5\n",
      "Epoch 808 Loss 118.07020075484432 Training Accuracy 87.5\n",
      "Epoch 809 Loss 118.03680555613542 Training Accuracy 87.5\n",
      "Epoch 810 Loss 118.00350255656113 Training Accuracy 87.5\n",
      "Epoch 811 Loss 117.97029136660129 Training Accuracy 87.5\n",
      "Epoch 812 Loss 117.93717159894021 Training Accuracy 87.5\n",
      "Epoch 813 Loss 117.90414286845115 Training Accuracy 87.5\n",
      "Epoch 814 Loss 117.87120479218063 Training Accuracy 87.5\n",
      "Epoch 815 Loss 117.83835698933318 Training Accuracy 87.5\n",
      "Epoch 816 Loss 117.80559908125605 Training Accuracy 87.5\n",
      "Epoch 817 Loss 117.772930691424 Training Accuracy 87.5\n",
      "Epoch 818 Loss 117.74035144542444 Training Accuracy 87.5\n",
      "Epoch 819 Loss 117.70786097094242 Training Accuracy 87.5\n",
      "Epoch 820 Loss 117.67545889774608 Training Accuracy 87.5\n",
      "Epoch 821 Loss 117.64314485767177 Training Accuracy 87.5\n",
      "Epoch 822 Loss 117.61091848460974 Training Accuracy 87.5\n",
      "Epoch 823 Loss 117.57877941448974 Training Accuracy 87.75\n",
      "Epoch 824 Loss 117.54672728526673 Training Accuracy 87.75\n",
      "Epoch 825 Loss 117.5147617369067 Training Accuracy 87.75\n",
      "Epoch 826 Loss 117.4828824113728 Training Accuracy 87.75\n",
      "Epoch 827 Loss 117.45108895261126 Training Accuracy 87.75\n",
      "Epoch 828 Loss 117.41938100653773 Training Accuracy 87.75\n",
      "Epoch 829 Loss 117.38775822102359 Training Accuracy 88.25\n",
      "Epoch 830 Loss 117.35622024588235 Training Accuracy 88.25\n",
      "Epoch 831 Loss 117.3247667328563 Training Accuracy 88.25\n",
      "Epoch 832 Loss 117.29339733560309 Training Accuracy 88.25\n",
      "Epoch 833 Loss 117.26211170968251 Training Accuracy 88.25\n",
      "Epoch 834 Loss 117.23090951254349 Training Accuracy 88.25\n",
      "Epoch 835 Loss 117.19979040351095 Training Accuracy 88.25\n",
      "Epoch 836 Loss 117.16875404377303 Training Accuracy 88.25\n",
      "Epoch 837 Loss 117.13780009636825 Training Accuracy 88.25\n",
      "Epoch 838 Loss 117.10692822617281 Training Accuracy 88.25\n",
      "Epoch 839 Loss 117.07613809988808 Training Accuracy 88.25\n",
      "Epoch 840 Loss 117.04542938602802 Training Accuracy 88.25\n",
      "Epoch 841 Loss 117.01480175490688 Training Accuracy 88.25\n",
      "Epoch 842 Loss 116.98425487862701 Training Accuracy 88.25\n",
      "Epoch 843 Loss 116.95378843106649 Training Accuracy 88.25\n",
      "Epoch 844 Loss 116.92340208786722 Training Accuracy 88.25\n",
      "Epoch 845 Loss 116.89309552642293 Training Accuracy 88.25\n",
      "Epoch 846 Loss 116.86286842586722 Training Accuracy 88.25\n",
      "Epoch 847 Loss 116.83272046706188 Training Accuracy 88.25\n",
      "Epoch 848 Loss 116.80265133258514 Training Accuracy 88.25\n",
      "Epoch 849 Loss 116.77266070672016 Training Accuracy 88.25\n",
      "Epoch 850 Loss 116.74274827544345 Training Accuracy 88.25\n",
      "Epoch 851 Loss 116.71291372641346 Training Accuracy 88.25\n",
      "Epoch 852 Loss 116.68315674895946 Training Accuracy 88.25\n",
      "Epoch 853 Loss 116.65347703407006 Training Accuracy 88.25\n",
      "Epoch 854 Loss 116.62387427438232 Training Accuracy 88.25\n",
      "Epoch 855 Loss 116.59434816417047 Training Accuracy 88.5\n",
      "Epoch 856 Loss 116.56489839933523 Training Accuracy 88.5\n",
      "Epoch 857 Loss 116.53552467739274 Training Accuracy 88.5\n",
      "Epoch 858 Loss 116.50622669746394 Training Accuracy 88.5\n",
      "Epoch 859 Loss 116.4770041602637 Training Accuracy 88.5\n",
      "Epoch 860 Loss 116.44785676809047 Training Accuracy 88.5\n",
      "Epoch 861 Loss 116.41878422481548 Training Accuracy 88.5\n",
      "Epoch 862 Loss 116.38978623587259 Training Accuracy 88.5\n",
      "Epoch 863 Loss 116.36086250824772 Training Accuracy 88.5\n",
      "Epoch 864 Loss 116.33201275046869 Training Accuracy 88.5\n",
      "Epoch 865 Loss 116.30323667259505 Training Accuracy 88.5\n",
      "Epoch 866 Loss 116.27453398620791 Training Accuracy 88.5\n",
      "Epoch 867 Loss 116.24590440439995 Training Accuracy 88.5\n",
      "Epoch 868 Loss 116.21734764176549 Training Accuracy 88.5\n",
      "Epoch 869 Loss 116.18886341439057 Training Accuracy 88.5\n",
      "Epoch 870 Loss 116.16045143984327 Training Accuracy 88.5\n",
      "Epoch 871 Loss 116.13211143716383 Training Accuracy 88.5\n",
      "Epoch 872 Loss 116.10384312685524 Training Accuracy 88.5\n",
      "Epoch 873 Loss 116.07564623087347 Training Accuracy 88.5\n",
      "Epoch 874 Loss 116.04752047261812 Training Accuracy 88.5\n",
      "Epoch 875 Loss 116.0194655769229 Training Accuracy 88.5\n",
      "Epoch 876 Loss 115.99148127004645 Training Accuracy 88.5\n",
      "Epoch 877 Loss 115.96356727966294 Training Accuracy 88.5\n",
      "Epoch 878 Loss 115.93572333485292 Training Accuracy 88.5\n",
      "Epoch 879 Loss 115.90794916609426 Training Accuracy 88.5\n",
      "Epoch 880 Loss 115.88024450525297 Training Accuracy 88.5\n",
      "Epoch 881 Loss 115.85260908557441 Training Accuracy 88.5\n",
      "Epoch 882 Loss 115.8250426416742 Training Accuracy 88.5\n",
      "Epoch 883 Loss 115.79754490952949 Training Accuracy 88.5\n",
      "Epoch 884 Loss 115.77011562647017 Training Accuracy 88.5\n",
      "Epoch 885 Loss 115.74275453117018 Training Accuracy 88.5\n",
      "Epoch 886 Loss 115.71546136363884 Training Accuracy 88.5\n",
      "Epoch 887 Loss 115.68823586521226 Training Accuracy 88.5\n",
      "Epoch 888 Loss 115.66107777854495 Training Accuracy 88.5\n",
      "Epoch 889 Loss 115.6339868476013 Training Accuracy 88.5\n",
      "Epoch 890 Loss 115.60696281764716 Training Accuracy 88.5\n",
      "Epoch 891 Loss 115.58000543524167 Training Accuracy 88.5\n",
      "Epoch 892 Loss 115.55311444822892 Training Accuracy 88.5\n",
      "Epoch 893 Loss 115.5262896057298 Training Accuracy 88.5\n",
      "Epoch 894 Loss 115.49953065813388 Training Accuracy 88.5\n",
      "Epoch 895 Loss 115.47283735709127 Training Accuracy 88.5\n",
      "Epoch 896 Loss 115.44620945550486 Training Accuracy 88.5\n",
      "Epoch 897 Loss 115.41964670752209 Training Accuracy 88.5\n",
      "Epoch 898 Loss 115.39314886852735 Training Accuracy 88.5\n",
      "Epoch 899 Loss 115.36671569513393 Training Accuracy 88.5\n",
      "Epoch 900 Loss 115.34034694517645 Training Accuracy 88.5\n",
      "Epoch 901 Loss 115.31404237770309 Training Accuracy 88.5\n",
      "Epoch 902 Loss 115.28780175296791 Training Accuracy 88.5\n",
      "Epoch 903 Loss 115.26162483242334 Training Accuracy 88.5\n",
      "Epoch 904 Loss 115.2355113787127 Training Accuracy 88.75\n",
      "Epoch 905 Loss 115.20946115566257 Training Accuracy 88.75\n",
      "Epoch 906 Loss 115.18347392827555 Training Accuracy 88.75\n",
      "Epoch 907 Loss 115.15754946272286 Training Accuracy 88.75\n",
      "Epoch 908 Loss 115.13168752633698 Training Accuracy 88.75\n",
      "Epoch 909 Loss 115.10588788760448 Training Accuracy 88.75\n",
      "Epoch 910 Loss 115.0801503161588 Training Accuracy 88.75\n",
      "Epoch 911 Loss 115.05447458277311 Training Accuracy 88.75\n",
      "Epoch 912 Loss 115.0288604593533 Training Accuracy 88.75\n",
      "Epoch 913 Loss 115.00330771893084 Training Accuracy 88.75\n",
      "Epoch 914 Loss 114.97781613565584 Training Accuracy 88.75\n",
      "Epoch 915 Loss 114.95238548479023 Training Accuracy 88.75\n",
      "Epoch 916 Loss 114.92701554270076 Training Accuracy 88.75\n",
      "Epoch 917 Loss 114.90170608685219 Training Accuracy 88.75\n",
      "Epoch 918 Loss 114.8764568958006 Training Accuracy 88.75\n",
      "Epoch 919 Loss 114.85126774918656 Training Accuracy 88.75\n",
      "Epoch 920 Loss 114.82613842772865 Training Accuracy 88.75\n",
      "Epoch 921 Loss 114.80106871321661 Training Accuracy 88.75\n",
      "Epoch 922 Loss 114.77605838850486 Training Accuracy 88.75\n",
      "Epoch 923 Loss 114.75110723750608 Training Accuracy 88.75\n",
      "Epoch 924 Loss 114.72621504518456 Training Accuracy 88.75\n",
      "Epoch 925 Loss 114.70138159754987 Training Accuracy 88.75\n",
      "Epoch 926 Loss 114.6766066816505 Training Accuracy 88.75\n",
      "Epoch 927 Loss 114.65189008556744 Training Accuracy 88.75\n",
      "Epoch 928 Loss 114.62723159840802 Training Accuracy 88.75\n",
      "Epoch 929 Loss 114.6026310102995 Training Accuracy 88.75\n",
      "Epoch 930 Loss 114.57808811238304 Training Accuracy 88.75\n",
      "Epoch 931 Loss 114.55360269680742 Training Accuracy 88.75\n",
      "Epoch 932 Loss 114.52917455672305 Training Accuracy 88.75\n",
      "Epoch 933 Loss 114.50480348627579 Training Accuracy 88.75\n",
      "Epoch 934 Loss 114.48048928060105 Training Accuracy 88.75\n",
      "Epoch 935 Loss 114.45623173581771 Training Accuracy 88.75\n",
      "Epoch 936 Loss 114.4320306490223 Training Accuracy 88.75\n",
      "Epoch 937 Loss 114.40788581828303 Training Accuracy 88.75\n",
      "Epoch 938 Loss 114.38379704263392 Training Accuracy 88.75\n",
      "Epoch 939 Loss 114.35976412206912 Training Accuracy 88.75\n",
      "Epoch 940 Loss 114.33578685753704 Training Accuracy 88.75\n",
      "Epoch 941 Loss 114.3118650509347 Training Accuracy 88.75\n",
      "Epoch 942 Loss 114.28799850510202 Training Accuracy 88.75\n",
      "Epoch 943 Loss 114.26418702381613 Training Accuracy 88.75\n",
      "Epoch 944 Loss 114.24043041178592 Training Accuracy 88.75\n",
      "Epoch 945 Loss 114.21672847464635 Training Accuracy 88.75\n",
      "Epoch 946 Loss 114.19308101895297 Training Accuracy 88.75\n",
      "Epoch 947 Loss 114.16948785217649 Training Accuracy 88.75\n",
      "Epoch 948 Loss 114.14594878269727 Training Accuracy 88.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 949 Loss 114.12246361980002 Training Accuracy 88.75\n",
      "Epoch 950 Loss 114.09903217366829 Training Accuracy 88.75\n",
      "Epoch 951 Loss 114.07565425537936 Training Accuracy 88.75\n",
      "Epoch 952 Loss 114.05232967689871 Training Accuracy 88.75\n",
      "Epoch 953 Loss 114.02905825107499 Training Accuracy 88.75\n",
      "Epoch 954 Loss 114.00583979163468 Training Accuracy 88.75\n",
      "Epoch 955 Loss 113.982674113177 Training Accuracy 88.75\n",
      "Epoch 956 Loss 113.95956103116869 Training Accuracy 88.75\n",
      "Epoch 957 Loss 113.93650036193897 Training Accuracy 88.75\n",
      "Epoch 958 Loss 113.9134919226745 Training Accuracy 88.75\n",
      "Epoch 959 Loss 113.89053553141424 Training Accuracy 88.75\n",
      "Epoch 960 Loss 113.86763100704464 Training Accuracy 88.75\n",
      "Epoch 961 Loss 113.84477816929447 Training Accuracy 88.75\n",
      "Epoch 962 Loss 113.82197683873011 Training Accuracy 88.75\n",
      "Epoch 963 Loss 113.79922683675048 Training Accuracy 88.75\n",
      "Epoch 964 Loss 113.77652798558236 Training Accuracy 88.75\n",
      "Epoch 965 Loss 113.75388010827538 Training Accuracy 88.75\n",
      "Epoch 966 Loss 113.73128302869742 Training Accuracy 88.75\n",
      "Epoch 967 Loss 113.70873657152973 Training Accuracy 89.0\n",
      "Epoch 968 Loss 113.68624056226226 Training Accuracy 89.0\n",
      "Epoch 969 Loss 113.66379482718898 Training Accuracy 89.0\n",
      "Epoch 970 Loss 113.64139919340326 Training Accuracy 89.25\n",
      "Epoch 971 Loss 113.61905348879314 Training Accuracy 89.25\n",
      "Epoch 972 Loss 113.59675754203687 Training Accuracy 89.25\n",
      "Epoch 973 Loss 113.57451118259826 Training Accuracy 89.25\n",
      "Epoch 974 Loss 113.5523142407222 Training Accuracy 89.25\n",
      "Epoch 975 Loss 113.53016654743018 Training Accuracy 89.25\n",
      "Epoch 976 Loss 113.50806793451582 Training Accuracy 89.25\n",
      "Epoch 977 Loss 113.48601823454035 Training Accuracy 89.25\n",
      "Epoch 978 Loss 113.4640172808284 Training Accuracy 89.25\n",
      "Epoch 979 Loss 113.44206490746339 Training Accuracy 89.25\n",
      "Epoch 980 Loss 113.42016094928343 Training Accuracy 89.25\n",
      "Epoch 981 Loss 113.39830524187681 Training Accuracy 89.25\n",
      "Epoch 982 Loss 113.37649762157787 Training Accuracy 89.25\n",
      "Epoch 983 Loss 113.35473792546264 Training Accuracy 89.25\n",
      "Epoch 984 Loss 113.33302599134466 Training Accuracy 89.25\n",
      "Epoch 985 Loss 113.31136165777083 Training Accuracy 89.25\n",
      "Epoch 986 Loss 113.2897447640172 Training Accuracy 89.25\n",
      "Epoch 987 Loss 113.26817515008477 Training Accuracy 89.25\n",
      "Epoch 988 Loss 113.24665265669556 Training Accuracy 89.25\n",
      "Epoch 989 Loss 113.22517712528833 Training Accuracy 89.25\n",
      "Epoch 990 Loss 113.20374839801465 Training Accuracy 89.5\n",
      "Epoch 991 Loss 113.18236631773487 Training Accuracy 89.5\n",
      "Epoch 992 Loss 113.16103072801407 Training Accuracy 89.5\n",
      "Epoch 993 Loss 113.13974147311812 Training Accuracy 89.5\n",
      "Epoch 994 Loss 113.11849839800975 Training Accuracy 89.5\n",
      "Epoch 995 Loss 113.0973013483446 Training Accuracy 89.5\n",
      "Epoch 996 Loss 113.0761501704674 Training Accuracy 89.5\n",
      "Epoch 997 Loss 113.05504471140797 Training Accuracy 89.5\n",
      "Epoch 998 Loss 113.03398481887757 Training Accuracy 89.5\n",
      "Epoch 999 Loss 113.01297034126488 Training Accuracy 89.5\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.000004\n",
    "w = np.random.randn(1,3)\n",
    "loss_steps=np.array([])\n",
    "for i in range(1,1000):\n",
    "    z = np.dot(w,X_train)\n",
    "    y_pred = prediction(w, X_train)\n",
    "    val = -np.multiply(y_train,z)\n",
    "    J = np.sum(np.log(1+np.exp(val)))\n",
    "    num = -np.multiply(y_train,np.exp(val))\n",
    "    den = 1+np.exp(val)\n",
    "    f = num/den\n",
    "    gradJ = np.dot(X_train,f.T)\n",
    "    w = w - learning_rate*gradJ.T\n",
    "    #print(y_pred,y_train[0])\n",
    "    loss_steps=np.append(loss_steps,J)\n",
    "    print(\"Epoch\",i,\"Loss\",J,\"Training Accuracy\",accuracy_score(y_train[0], y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nO3deXRc9X338fd3ZrRYmyXZsixbwrKx8YLBxlFZSsgCKQGaYtpsEB7iJvTQtGlJm7QB0vM8OU2bNmnTECgpDQkkpM0DoQQKT0IgYGggLRBsFttgGwtjY3mVF3mTtc73+WN+MmNjW4tndEczn9c5c+be3/3NzPf6gj++93cXc3dEREROJBZ1ASIikvsUFiIiMiiFhYiIDEphISIig1JYiIjIoBJRF5ANEydO9Obm5qjLEBEZU5YvX77T3euOtSwvw6K5uZlly5ZFXYaIyJhiZhuPt0yHoUREZFAKCxERGZTCQkREBqWwEBGRQSksRERkUAoLEREZlMJCREQGpbBIs7ezl1ueWMeKto6oSxERySl5eVHeSMVicPMTr5OIG2c2VkddjohIztCeRZrK0iKmVo9j7bb9UZciIpJTFBZHmT25kte3KyxERNIpLI4ye3Ilb7QfoLc/GXUpIiI5Q2FxlNn1lfT2O+vbD0ZdiohIzlBYHGX25EoA1upQlIjIYQqLo5xaV0EiZqzdti/qUkREckbWwsLM7jKzHWa26hjLvmBmbmYTw7yZ2a1m1mpmK8xsUVrfJWa2LryWZKveAcWJGNMnlrN224Fs/5SIyJiRzT2LHwCXHN1oZk3AxcBbac2XArPC6zrg9tC3FvgycA5wNvBlM6vJYs1A6lDU2u3asxARGZC1sHD3p4Hdx1h0M/BFwNPaFgM/9JTngGozawA+CDzu7rvdfQ/wOMcIoEybXV/Jpt2HONDdl+2fEhEZE0Z1zMLMFgOb3f2VoxZNBTalzbeFtuO1Z9XAIPc6DXKLiACjGBZmVgZ8Cfg/Wfr+68xsmZkta29vP6nvmjO5CkBXcouIBKO5Z3EqMB14xcw2AI3Ai2Y2GdgMNKX1bQxtx2t/B3e/w91b3L2lrq7upAptrBlHWXGcNQoLERFgFMPC3Ve6+yR3b3b3ZlKHlBa5+zbgYeCT4ayoc4G97r4VeAy42MxqwsD2xaEtq2IxY1a9bvshIjIgm6fO3gM8C8w2szYzu/YE3R8B1gOtwHeBPwZw993A3wAvhNdXQlvWza6v0GEoEZEga7cod/erBlnenDbtwGeP0+8u4K6MFjcEsydXcd+yNnYe6GZiRclo/7yISE7RFdzHMWfgth/auxARUVgcz2n1qbDQILeIiMLiuOoqS5hQXszrCgsREYXFicyeXMkanRElIqKwOJHT6itZt30/yaQP3llEJI8pLE5gzuRKOnv6adtzKOpSREQipbA4gdMmDwxy6w60IlLYFBYnMHBGlK7kFpFCp7A4gYqSBE2143T6rIgUPIXFIGbXV+rCPBEpeAqLQcyeXMn6nQfp7uuPuhQRkcgoLAYxt6GK/qSzbrueyS0ihUthMYi5DakHIa3eqjOiRKRwKSwG0TyhnHFFcV5TWIhIAVNYDCIeM2ZPrtSehYgUNIXFEMxtqOK1LftIPXZDRKTwKCyGYN6UKvZ19bFlb1fUpYiIREJhMQTzGlJXcq/eokNRIlKYFBZDMHtyFWZokFtECpbCYggqShJMqy3TILeIFCyFxRDNbahSWIhIwcpaWJjZXWa2w8xWpbX9o5mtMbMVZvagmVWnLbvJzFrNbK2ZfTCt/ZLQ1mpmN2ar3sHMa6hiw65ODnT3RVWCiEhksrln8QPgkqPaHgfmu/uZwOvATQBmNg+4Ejg9fOZfzCxuZnHg28ClwDzgqtB31A1cyb1Wz7YQkQKUtbBw96eB3Ue1/cLdB/5p/hzQGKYXA/e6e7e7vwm0AmeHV6u7r3f3HuDe0HfUzZuSCovXdEaUiBSgKMcsPg38PExPBTalLWsLbcdrfwczu87MlpnZsvb29owX2zC+lPHjinhtq25XLiKFJ5KwMLO/AvqAH2XqO939DndvcfeWurq6TH3tYWbG3Abd9kNECtOoh4WZ/T7wIeBqf/v+GZuBprRujaHteO2RmNcwnjXb9tGf1G0/RKSwjGpYmNklwBeBy929M23Rw8CVZlZiZtOBWcCvgReAWWY23cyKSQ2CPzyaNaeb21BJV2+SDbsORlWCiEgksnnq7D3As8BsM2szs2uB24BK4HEze9nM/hXA3V8F7gNeAx4FPuvu/WEw/E+Ax4DVwH2hbyQ0yC0ihSqRrS9296uO0XznCfp/FfjqMdofAR7JYGkjNnNSBYmYsXrrPn5nwZSoyxERGTW6gnsYShJxZk6q0CC3iBQchcUwzWuo0g0FRaTgKCyGad6UKrbv66Z9f3fUpYiIjBqFxTDNnzoegFVb9kZciYjI6FFYDNPp4YyoVW0KCxEpHAqLYaosLWL6xHLtWYhIQVFYjMD8qeNZtVmD3CJSOBQWI3DG1Co2dxxi98GeqEsRERkVCosRmD8lDHJv1qEoESkMCosROD2cEbVSYSEiBUJhMQLjxxUxbUIZr2qQW0QKhMJihOZPGa89CxEpGAqLEZo/dTybdh+io1OD3CKS/xQWIzR/aurivFd1u3IRKQAKixEaOCNKh6JEpBAoLEaopryYxppxOn1WRAqCwuIkzJ8yXmEhIgVBYXESzmgcz4Zdnezr6o26FBGRrFJYnITDd6DV3oWI5DmFxUk4s7EagBW6XbmI5LmshYWZ3WVmO8xsVVpbrZk9bmbrwntNaDczu9XMWs1shZktSvvMktB/nZktyVa9I1FbXswptWW8sqkj6lJERLIqm3sWPwAuOartRmCpu88CloZ5gEuBWeF1HXA7pMIF+DJwDnA28OWBgMkVC5uqeVlhISJ5Lmth4e5PA7uPal4M3B2m7wauSGv/oac8B1SbWQPwQeBxd9/t7nuAx3lnAEVqQVM1W/d2sX1fV9SliIhkzWiPWdS7+9YwvQ2oD9NTgU1p/dpC2/Ha38HMrjOzZWa2rL29PbNVn8DCptS4hfYuRCSfRTbA7e4OeAa/7w53b3H3lrq6ukx97aBOn1JFImYatxCRvDbaYbE9HF4ivO8I7ZuBprR+jaHteO05o7QoztyGKu1ZiEheG+2weBgYOKNpCfBQWvsnw1lR5wJ7w+Gqx4CLzawmDGxfHNpyyoKm8axo20symbEdJRGRnJLNU2fvAZ4FZptZm5ldC3wN+C0zWwd8IMwDPAKsB1qB7wJ/DODuu4G/AV4Ir6+EtpyysKmGA919vNF+IOpSRESyIpGtL3b3q46z6KJj9HXgs8f5nruAuzJYWsYtbErdgfblTR3Mqq+MuBoRkczTFdwZMGNiBZUlCY1biEjeUlhkQCxmnNk0nlfaFBYikp8UFhmysKmaNVv309XbH3UpIiIZp7DIkAWN1fQlnVe36KaCIpJ/FBYZsvCU1JXcL27UoSgRyT8KiwyZVFnKKbVlLNuYc2f2ioicNIVFBrVMq2H5xj2kzgQWEckfCosMeldzDTsP9LBxV2fUpYiIZJTCIoNaptUCsGzjnogrERHJLIVFBs2aVEFVaYLlGrcQkTyjsMigWMxYNK2GZRu0ZyEi+UVhkWEt02pYt+MAHZ09UZciIpIxCosMe1cYt3jxLe1diEj+UFhk2MKmahIx06EoEckrCosMG1cc5/QpVTojSkTyisIiC941rZZXNnXQ05eMuhQRkYwYUliYWbmZxcL0aWZ2uZkVZbe0sevs6TV09yVZuVn3iRKR/DDUPYungVIzmwr8ArgG+EG2ihrrzp4+AYDn1ut6CxHJD0MNC3P3TuD3gH9x948Cp2evrLGttryYOZMrefaNXVGXIiKSEUMOCzM7D7ga+Floi2enpPxw7owJLNu4W+MWIpIXhhoWfwbcBDzo7q+a2QzgqZH+qJn9uZm9amarzOweMys1s+lm9ryZtZrZj82sOPQtCfOtYXnzSH93NJ07YwJdvUlW6FGrIpIHhhQW7v5Ld7/c3b8eBrp3uvv1I/nBMO5xPdDi7vNJ7aFcCXwduNndZwJ7gGvDR64F9oT2m0O/nHfO9FrM0KEoEckLQz0b6v+aWZWZlQOrgNfM7C9P4ncTwDgzSwBlwFbgQuD+sPxu4IowvTjME5ZfZGZ2Er89KmrKi5kzuYrn3lRYiMjYN9TDUPPcfR+pv8B/DkwndUbUsLn7ZuAbwFukQmIvsBzocPe+0K0NmBqmpwKbwmf7Qv8JR3+vmV1nZsvMbFl7e/tISsu4c2fUsnzjHrr7+qMuRUTkpAw1LIrCdRVXAA+7ey8wosfBmVkNqb2F6cAUoBy4ZCTflc7d73D3FndvqaurO9mvy4iBcYtXNu2NuhQRkZMy1LD4DrCB1F/sT5vZNGDfCH/zA8Cb7t4eQucB4HygOhyWAmgENofpzUATQFg+HhgTx3YGxi2eWz8myhUROa6hDnDf6u5T3f0yT9kIvH+Ev/kWcK6ZlYWxh4uA10idXfWR0GcJ8FCYfjjME5Y/6WPkIdfVZcXMnVylQW4RGfOGOsA93sy+OTAmYGb/RGovY9jc/XlSA9UvAitDDXcANwCfN7NWUmMSd4aP3AlMCO2fB24cye9G5TdPncDyt/ZwqEfjFiIydg31MNRdwH7gY+G1D/j+SH/U3b/s7nPcfb67X+Pu3e6+3t3PdveZ7v5Rd+8OfbvC/MywfP1IfzcKF5xWR09fkud1VpSIjGFDDYtTw1/w68Prr4EZ2SwsX5zdXEtxIsbTr++MuhQRkREbalgcMrN3D8yY2fnAoeyUlF/GFcc5Z3otz6zLjdN5RURGYqhh8Rng22a2wcw2ALcBf5i1qvLMBbMmsm7HAbbuVb6KyNg01LOhXnH3BcCZwJnufhapK65lCN5zWuq6j2d0KEpExqhhPSnP3feFK7khdWaSDMHs+komVZbwtA5FicgYdTKPVc35+zPlCjPjgll1/Kp1J/3JMXGJiIjIEU4mLPS33jC857SJdHT2snKzbv0hImNP4kQLzWw/xw4FA8ZlpaI8dcGsOmIGT67ZwcKm6qjLEREZlhPuWbh7pbtXHeNV6e4nDBo5Um15Me+aVsPS1dujLkVEZNhO5jCUDNNFc+t5dcs+tnToFFoRGVsUFqPoA3MnAbB0zY6IKxERGR6FxSg6ta6CaRPKdChKRMYchcUoMjM+MLee/3ljFwe7+wb/gIhIjlBYjLKL5k6ipy/JM+t0NbeIjB0Ki1H2G821VJYmeEKHokRkDFFYjLKieIyL5kziidXb6e1PRl2OiMiQKCwicNkZDXR09upxqyIyZigsIvCe0+qoKEnwyMqtUZciIjIkCosIlBbFuWjuJB59dZsORYnImKCwiIgORYnIWBJJWJhZtZndb2ZrzGy1mZ1nZrVm9riZrQvvNaGvmdmtZtZqZivMbFEUNWfae0+ro7w4rkNRIjImRLVncQvwqLvPARYAq4EbgaXuPgtYGuYBLgVmhdd1wO2jX27mpQ5F1fOYDkWJyBgw6mFhZuOB9wB3Arh7j7t3AIuBu0O3u4ErwvRi4Iee8hxQbWYNo1x2Vvz2mQ3s6ezlV626QE9EclsUexbTgXbg+2b2kpl9z8zKgXp3Hzgmsw2oD9NTgU1pn28LbUcws+vMbJmZLWtvHxuPL33f7Dqqy4p48MXNUZciInJCUYRFAlgE3O7uZwEHefuQEwDu7gzzSXzufoe7t7h7S11dXcaKzaaSRJzfOXMKj726jf1dvVGXIyJyXFGERRvQ5u7Ph/n7SYXH9oHDS+F94D7em4GmtM83hra88HuLptLdl+TnK7dFXYqIyHGNeli4+zZgk5nNDk0XAa8BDwNLQtsS4KEw/TDwyXBW1LnA3rTDVWPewqZqZkws5ycvtkVdiojIcUX1aNQ/BX5kZsXAeuBTpILrPjO7FtgIfCz0fQS4DGgFOkPfvGFm/N6iqXzjF6+zaXcnTbVlUZckIvIOkYSFu78MtBxj0UXH6OvAZ7NeVISuOCsVFg++tJnrL5oVdTkiIu+gK7hzQGNNGefPnMCPX9hEf3JY4/oiIqNCYZEjrj5nGps7DvH062PjtF8RKSwKixzxW/Pqqass4UfPb4y6FBGRd1BY5IiieIyPtzTx5JodbO44FHU5IiJHUFjkkCvPbsKBH//6rahLERE5gsIihzTWlPH+2ZO494VN9PTp5oIikjsUFjnmmvOmsWN/Nz9buSXqUkREDlNY5Jj3zqpj5qQKvvv0m6QuMRERiZ7CIsfEYsYfvHs6r23dp6foiUjOUFjkoCvOmsrEimK++8z6qEsREQEUFjmptCjONec289TadtZt3x91OSIiCotcdc150ygtinH7L9+IuhQREYVFrqotL+bqc6bx0Mtb2LDzYNTliEiBU1jksD987wwSMeO2p1qjLkVECpzCIodNqizl6nOm8eBLm9m4S3sXIhIdhUWO+8zA3sWT2rsQkegoLHLcpKpSPnHOKTzw0mZad+jMKBGJhsJiDPiT98+krCjO136+NupSRKRAKSzGgAkVJXzmfafyxOrtPL9eV3WLyOhTWIwR1757Og3jS/m7R1aT1KNXRWSURRYWZhY3s5fM7KdhfrqZPW9mrWb2YzMrDu0lYb41LG+OquYolRbF+YuLZ/NK214eemVz1OWISIGJcs/ic8DqtPmvAze7+0xgD3BtaL8W2BPabw79CtLvnjWVBU3VfPVna9jX1Rt1OSJSQCIJCzNrBH4b+F6YN+BC4P7Q5W7gijC9OMwTll8U+hecWMz428Xz2X2wm396TIPdIjJ6otqz+BbwRWDgcXATgA537wvzbcDUMD0V2AQQlu8N/Y9gZteZ2TIzW9be3p7N2iN1RuN4rjl3Gv/23EZWtu2NuhwRKRCjHhZm9iFgh7svz+T3uvsd7t7i7i11dXWZ/Oqc8/mLZ1NbXsKXHlxJX78evyoi2RfFnsX5wOVmtgG4l9Thp1uAajNLhD6NwMAo7magCSAsHw8U9Pmj48cV8ZXFp7Ny815u/y/dlVZEsm/Uw8Ldb3L3RndvBq4EnnT3q4GngI+EbkuAh8L0w2GesPxJ1/NGueyMBi5fMIVblq7j1S06HCUi2ZVL11ncAHzezFpJjUncGdrvBCaE9s8DN0ZUX875yuLTqSkv5gv3vUJ3X3/U5YhIHos0LNz9v9z9Q2F6vbuf7e4z3f2j7t4d2rvC/MywXM8aDarLivn6h89gzbb9/P0ja6IuR0TyWC7tWcgIXDinnk+fP50f/M8GHlm5NepyRCRPKSzywI2XzmFhUzU33L9CT9UTkaxQWOSB4kSM2z5xFrGY8Yf/tpz9urpbRDJMYZEnGmvK+PYnFtHafoDr73lJ11+ISEYpLPLIu2dN5CuLT+epte387c9WD/4BEZEhSgzeRcaSq8+Zxvr2g9z5qzeZVFXCH79vZtQliUgeUFjkoS9dNpedB7r5h0fXUlmS4JrzmqMuSUTGOIVFHorHjG98dAEHu/v53w+9SklRnI+1NEVdloiMYRqzyFNF8dQZUhfMmsgX71/B9//7zahLEpExTGGRx0qL4nxvSQsfPL2ev/5/r3HLE+vQbbVEZCQUFnmuJBHn259YxIcXNXLzE69z0wMr6enTabUiMjwasygAiXiMf/zImTSML+W2p1pZ336Q2//XIiZUlERdmoiMEdqzKBCxmPEXH5zNrVedxSttHVx+23+zfOPuqMsSkTFCYVFgLl8whf/4zHnEYvCx7zzHrUvX0Z/UOIaInJjCogCd2VjNz66/gA+d2cA3H3+dj3/nWVp37I+6LBHJYQqLAlVVWsS3Pr6Qmz++gHU7DnDZLb/i5sdf10OUROSYFBYFzMz43bMaWfqF93LpGZO5Zek6LvnWMzy6aqtOsRWRIygshIkVJdxy5Vnc/emziceMz/z7i3z0X5/VALiIHKawkMPee1odj37uAv7ud89gw65OPnz7s1x1x3P8d+tO7WmIFDjLx78EWlpafNmyZVGXMaYd7O7jnl+/xR1Pr2fH/m4WNFXz6fObuWT+ZEoS8ajLE5EsMLPl7t5yzGWjHRZm1gT8EKgHHLjD3W8xs1rgx0AzsAH4mLvvMTMDbgEuAzqB33f3F0/0GwqLzOnq7ecnL7bx3afXs2FXJxPKi/nYbzTxibNPoam2LOryRCSDci0sGoAGd3/RzCqB5cAVwO8Du939a2Z2I1Dj7jeY2WXAn5IKi3OAW9z9nBP9hsIi85JJ51etO/n35zbyxOrtJB1aptWweOEULjujQVeDi+SBnAqLdxRg9hBwW3i9z923hkD5L3efbWbfCdP3hP5rB/od7zsVFtm1de8hHnxpMw+9tIW12/cTjxm/eeoEPjC3ngvnTNIeh8gYlbNhYWbNwNPAfOAtd68O7QbscfdqM/sp8DV3/1VYthS4wd2XHfVd1wHXAZxyyinv2rhx46itRyFbs20fD728hcdWbWP9zoMAnFZfwYVz6nnPrIksmlZDaZHGOETGgpwMCzOrAH4JfNXdHzCzjoGwCMv3uHvNUMMinfYsorG+/QBPrtnB0tU7eGHDbvqSTnE8xsKmas49dQLnzqhlQWM15SW6f6VILjpRWETyf62ZFQE/AX7k7g+E5u1m1pB2GGpHaN8MpD/mrTG0SY6ZUVfBjLoK/uCCGezv6mXZhj08t34Xz67fxW1PruPWpRAzmDWpkgVN4zmzsZqFTdXMnlxJUVxncYvkslEPi3CI6U5gtbt/M23Rw8AS4Gvh/aG09j8xs3tJDXDvPdF4heSGytIi3j9nEu+fMwmAfV29LN+wh5c3dbCirYMnVu/gvmVtABTHY5w6qYLZ9RWcNrmS2fWVzJ5cydTqcaT+cxGRqEVxNtS7gWeAlcDAU3i+BDwP3AecAmwkders7hAutwGXkDp19lMnOgQFOgw1Frg7bXsO8UpbByvb9rJ2+35e37afLXu7DvepKEkwfWI50yaUhfdymieU0TyxnAnlxQoSkQzLyTGLbFJYjF37unp5fdv+w+GxfudBNu7qpG1PJ+l3Uq8oSdBUW8bU6lIaxo9jSvU4phyeLqW+qlSHtkSGKefGLESOp6q0iJbmWlqaa49o7+lL0rank427OtmwKxUgb+3upG3PIV7YsIe9h3qP6B8zqKssob6qlIkVJdRVlDCxspi6ihLqKkuZWFFMXWUJEytLqCxJaC9FZBAKCxkTihOxwwPox3Kwu4+tew+xuaOLrR2H2LK3iy0dh2jf3832fV2s2ryXXQd7jvmgp5JEjNryYqrLiqkeV0RNeRHVZcXUlBVRPa6Y6rIiasqKD7dXjyuialyR9lykoCgsJC+UlySYOamSmZMqj9snmXT2dPbQfqCbnft7aD/QRfv+bnYe6GH3wR46Onvo6Oxl7bb9dHT20nGo94RPESxJxKgsLaKyNHH4VVGSoLK0iIqSBFWlYTptWUVJgnHFccqLE5QVxxlXHKesOEE8pj0byW0KCykYsZgxoaIkdWuSyYP3d3f2d/fRcbCXPZ09dBzqpaOzhz0He9jf1ceB7j72dfWxv6uXA9197O/qo33/QQ50paYP9PQx1CHBkkSMshAcqfe3g2RgfmB6XFGc0qI4pUUxShJxSsJ76bHei+KUJt5+T2hvSEZIYSFyHGZGVWkRVaVFnDJh+LcwSSadgz19h4MlFSr9HOrpo7OnP7xS04d6+jmYNj2wbPu+rsPzB3v6ONTTT99JPDM9HrMjwqOkKE5J2ntxPEZR3ChOxCiKp+YHpovCdHHcDk8XxWMUJWKUxGMUJezwZ4oSR3/WKEn7nqJ4jETMSITviseMRMw0dpTDFBYiWRKLWThMVZTR7+3pS9Ld109X77Hfu/uSdPem3rvS33uTdPUd/Z7q29WXpKevn0O9/ew9lKS3P0lPf5KevtR0b7/T0/d2W7YMhEYqSN4OlEQsFt6Pmg5BUzTQ56j+8ZhRFIsRjxtFR31nPBajKGbEYnb4d2OWmo6F+bgNLId4LEbcUtMxS33HQP/4wHvaZ2Npbel93rHcjHh84Lc43C/XglNhITLGFCdS/2KvLI3m992d/qTT05+kty/1nppOBUt3WsD0hnDp6U8enk4FkdPfn6Qv6alX6N+fdHqTSfr7Q3sySV+/h2VJepMelqX170/S3ZvkQLKf/sP9k2FZqs/R/fvD7+aymKXCM2YhXNJCJzbwbhxuj4X5eVPG889XnZXxehQWIjIsFv5VnYjHoDjqakbOPRUY/UknGQLw8MudZBL6kkmSSej3t/v19af1T/tcMsz3DUwP9D/iN1KHJ/sO/8aRvznwPX3Jo36jP61/2vckPb0dku6cUjsuK39eCgsRKUhmqcNXuiny0OjUCBERGZTCQkREBqWwEBGRQSksRERkUAoLEREZlMJCREQGpbAQEZFBKSxERGRQefmkPDNrJ/Vo1pGaCOzMUDljhda5MGidC8NI13mau9cda0FehsXJMrNlx3u0YL7SOhcGrXNhyMY66zCUiIgMSmEhIiKDUlgc2x1RFxABrXNh0DoXhoyvs8YsRERkUNqzEBGRQSksRERkUAqLNGZ2iZmtNbNWM7sx6noyxcyazOwpM3vNzF41s8+F9loze9zM1oX3mtBuZnZr+HNYYWaLol2DkTOzuJm9ZGY/DfPTzez5sG4/NrPi0F4S5lvD8uYo6x4pM6s2s/vNbI2ZrTaz8/J9O5vZn4f/rleZ2T1mVppv29nM7jKzHWa2Kq1t2NvVzJaE/uvMbMlwalBYBGYWB74NXArMA64ys3nRVpUxfcAX3H0ecC7w2bBuNwJL3X0WsDTMQ+rPYFZ4XQfcPvolZ8zngNVp818Hbnb3mcAe4NrQfi2wJ7TfHPqNRbcAj7r7HGABqXXP2+1sZlOB64EWd58PxIEryb/t/APgkqPahrVdzawW+DJwDnA28OWBgBkSd9crNch/HvBY2vxNwE1R15WldX0I+C1gLdAQ2hqAtWH6O8BVaf0P9xtLL6Ax/E90IfBTwEhd1Zo4epsDjwHnhelE6GdRr8Mw13c88ObRdefzdgamApuA2rDdfgp8MB+3M9AMrBrpdgWuAr6T1n5Ev8Fe2rN428B/dAPaQlteCbvdZ30NHcoAAAQJSURBVAHPA/XuvjUs2gbUh+l8+bP4FvBFIBnmJwAd7t4X5tPX6/A6h+V7Q/+xZDrQDnw/HHr7npmVk8fb2d03A98A3gK2ktpuy8nv7TxguNv1pLa3wqKAmFkF8BPgz9x9X/oyT/1TI2/OozazDwE73H151LWMogSwCLjd3c8CDvL2oQkgL7dzDbCYVFBOAcp55+GavDca21Vh8bbNQFPafGNoywtmVkQqKH7k7g+E5u1m1hCWNwA7Qns+/FmcD1xuZhuAe0kdiroFqDazROiTvl6H1zksHw/sGs2CM6ANaHP358P8/aTCI5+38weAN9293d17gQdIbft83s4DhrtdT2p7Kyze9gIwK5xFUUxqkOzhiGvKCDMz4E5gtbt/M23Rw8DAGRFLSI1lDLR/MpxVcS6wN213d0xw95vcvdHdm0ltyyfd/WrgKeAjodvR6zzwZ/GR0H9M/Qvc3bcBm8xsdmi6CHiNPN7OpA4/nWtmZeG/84F1ztvtnGa42/Ux4GIzqwl7ZBeHtqGJetAml17AZcDrwBvAX0VdTwbX692kdlFXAC+H12WkjtUuBdYBTwC1ob+ROjPsDWAlqTNNIl+Pk1j/9wE/DdMzgF8DrcB/ACWhvTTMt4blM6Kue4TruhBYFrb1fwI1+b6dgb8G1gCrgH8DSvJtOwP3kBqT6SW1B3ntSLYr8Omw7q3Ap4ZTg273ISIig9JhKBERGZTCQkREBqWwEBGRQSksRERkUAoLEREZlMJCZBjMrN/MXk57ZezuxGbWnH5XUZFckhi8i4ikOeTuC6MuQmS0ac9CJAPMbIOZ/YOZrTSzX5vZzNDebGZPhucKLDWzU0J7vZk9aGavhNdvhq+Km9l3w/MZfmFm40L/6y31PJIVZnZvRKspBUxhITI84446DPXxtGV73f0M4DZSd7wF+Gfgbnc/E/gRcGtovxX4pbsvIHX/pldD+yzg2+5+OtABfDi03wicFb7nM9laOZHj0RXcIsNgZgfcveIY7RuAC919fbhp4zZ3n2BmO0k9c6A3tG9194lm1g40unt32nc0A4976mE2mNkNQJG7/62ZPQocIHULj/909wNZXlWRI2jPQiRz/DjTw9GdNt3P2+OKv03qfj+LgBfS7qgqMioUFiKZ8/G092fD9P+QuustwNXAM2F6KfBHcPg54eOP96VmFgOa3P0p4AZSt9V+x96NSDbpXyciwzPOzF5Om3/U3QdOn60xsxWk9g6uCm1/SurJdX9J6il2nwrtnwPuMLNrSe1B/BGpu4oeSxz49xAoBtzq7h0ZWyORIdCYhUgGhDGLFnffGXUtItmgw1AiIjIo7VmIiMigtGchIiKDUliIiMigFBYiIjIohYWIiAxKYSEiIoP6/35m1piTV+tNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.plot(loss_steps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.5\n"
     ]
    }
   ],
   "source": [
    "y_test_pred=prediction(w,X_test)\n",
    "print(accuracy_score(y_test[0],y_test_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm= (X_train-X_train.mean())/ np.std(X_train)\n",
    "X_test_norm= (X_test-X_test.mean())/ np.std(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.75139951, -0.04357539, -3.00984799, ...,  0.75854098,\n",
       "         1.06822657,  1.81549064],\n",
       "       [ 1.5495483 ,  0.5203809 ,  1.71795196, ..., -1.255422  ,\n",
       "        -0.81365117, -2.46241956],\n",
       "       [ 0.09893739,  0.09893739,  0.09893739, ...,  0.09893739,\n",
       "         0.09893739,  0.09893739]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 246.43588694029984 Training Accuracy 66.25\n",
      "Epoch 2 Loss 243.8371610245042 Training Accuracy 67.25\n",
      "Epoch 3 Loss 241.30891616362823 Training Accuracy 68.0\n",
      "Epoch 4 Loss 238.84868033938392 Training Accuracy 68.5\n",
      "Epoch 5 Loss 236.45407408281713 Training Accuracy 68.75\n",
      "Epoch 6 Loss 234.1228076420722 Training Accuracy 69.25\n",
      "Epoch 7 Loss 231.8526781326673 Training Accuracy 70.0\n",
      "Epoch 8 Loss 229.64156668623917 Training Accuracy 71.25\n",
      "Epoch 9 Loss 227.48743561164565 Training Accuracy 71.5\n",
      "Epoch 10 Loss 225.38832558042478 Training Accuracy 71.75\n",
      "Epoch 11 Loss 223.34235284688697 Training Accuracy 72.25\n",
      "Epoch 12 Loss 221.34770651155895 Training Accuracy 72.75\n",
      "Epoch 13 Loss 219.40264583529174 Training Accuracy 73.5\n",
      "Epoch 14 Loss 217.50549761008426 Training Accuracy 74.75\n",
      "Epoch 15 Loss 215.65465359154547 Training Accuracy 75.5\n",
      "Epoch 16 Loss 213.84856799691468 Training Accuracy 75.5\n",
      "Epoch 17 Loss 212.08575507167103 Training Accuracy 75.5\n",
      "Epoch 18 Loss 210.3647867269787 Training Accuracy 75.75\n",
      "Epoch 19 Loss 208.68429024952616 Training Accuracy 76.25\n",
      "Epoch 20 Loss 207.04294608471736 Training Accuracy 76.5\n",
      "Epoch 21 Loss 205.43948569365074 Training Accuracy 76.75\n",
      "Epoch 22 Loss 203.8726894838721 Training Accuracy 77.0\n",
      "Epoch 23 Loss 202.3413848135 Training Accuracy 77.0\n",
      "Epoch 24 Loss 200.8444440679949 Training Accuracy 77.75\n",
      "Epoch 25 Loss 199.38078280856516 Training Accuracy 77.75\n",
      "Epoch 26 Loss 197.94935799097195 Training Accuracy 78.0\n",
      "Epoch 27 Loss 196.5491662533038 Training Accuracy 78.0\n",
      "Epoch 28 Loss 195.1792422711363 Training Accuracy 78.25\n",
      "Epoch 29 Loss 193.83865717836895 Training Accuracy 79.25\n",
      "Epoch 30 Loss 192.52651705193568 Training Accuracy 79.5\n",
      "Epoch 31 Loss 191.24196145851352 Training Accuracy 80.0\n",
      "Epoch 32 Loss 189.98416206130335 Training Accuracy 80.25\n",
      "Epoch 33 Loss 188.75232128492434 Training Accuracy 80.5\n",
      "Epoch 34 Loss 187.54567103644814 Training Accuracy 80.75\n",
      "Epoch 35 Loss 186.36347148059406 Training Accuracy 81.0\n",
      "Epoch 36 Loss 185.20500986711684 Training Accuracy 81.0\n",
      "Epoch 37 Loss 184.06959940843572 Training Accuracy 81.25\n",
      "Epoch 38 Loss 182.95657820558017 Training Accuracy 81.25\n",
      "Epoch 39 Loss 181.86530822056096 Training Accuracy 81.5\n",
      "Epoch 40 Loss 180.79517429331423 Training Accuracy 81.5\n",
      "Epoch 41 Loss 179.74558320140866 Training Accuracy 82.0\n",
      "Epoch 42 Loss 178.71596276075343 Training Accuracy 82.0\n",
      "Epoch 43 Loss 177.70576096559384 Training Accuracy 82.0\n",
      "Epoch 44 Loss 176.71444516613298 Training Accuracy 82.25\n",
      "Epoch 45 Loss 175.74150128217093 Training Accuracy 82.25\n",
      "Epoch 46 Loss 174.78643305120713 Training Accuracy 82.25\n",
      "Epoch 47 Loss 173.84876130950448 Training Accuracy 82.25\n",
      "Epoch 48 Loss 172.92802330467097 Training Accuracy 82.25\n",
      "Epoch 49 Loss 172.0237720383648 Training Accuracy 82.25\n",
      "Epoch 50 Loss 171.13557563778647 Training Accuracy 82.25\n",
      "Epoch 51 Loss 170.26301675467127 Training Accuracy 82.5\n",
      "Epoch 52 Loss 169.40569199054818 Training Accuracy 82.5\n",
      "Epoch 53 Loss 168.5632113470831 Training Accuracy 82.5\n",
      "Epoch 54 Loss 167.73519770037183 Training Accuracy 82.75\n",
      "Epoch 55 Loss 166.92128629809767 Training Accuracy 83.0\n",
      "Epoch 56 Loss 166.1211242785152 Training Accuracy 83.0\n",
      "Epoch 57 Loss 165.33437021026572 Training Accuracy 83.0\n",
      "Epoch 58 Loss 164.56069365207594 Training Accuracy 83.0\n",
      "Epoch 59 Loss 163.79977473143146 Training Accuracy 83.0\n",
      "Epoch 60 Loss 163.05130374135825 Training Accuracy 83.0\n",
      "Epoch 61 Loss 162.3149807544853 Training Accuracy 83.0\n",
      "Epoch 62 Loss 161.5905152535978 Training Accuracy 83.0\n",
      "Epoch 63 Loss 160.87762577792734 Training Accuracy 83.25\n",
      "Epoch 64 Loss 160.17603958446045 Training Accuracy 83.5\n",
      "Epoch 65 Loss 159.4854923235792 Training Accuracy 83.5\n",
      "Epoch 66 Loss 158.80572772838053 Training Accuracy 83.75\n",
      "Epoch 67 Loss 158.13649731705092 Training Accuracy 83.5\n",
      "Epoch 68 Loss 157.4775601077016 Training Accuracy 83.5\n",
      "Epoch 69 Loss 156.82868234509903 Training Accuracy 83.75\n",
      "Epoch 70 Loss 156.18963723875004 Training Accuracy 84.0\n",
      "Epoch 71 Loss 155.56020471182825 Training Accuracy 84.0\n",
      "Epoch 72 Loss 154.94017116045066 Training Accuracy 84.0\n",
      "Epoch 73 Loss 154.32932922283896 Training Accuracy 84.25\n",
      "Epoch 74 Loss 153.72747755791977 Training Accuracy 84.25\n",
      "Epoch 75 Loss 153.13442063294008 Training Accuracy 84.5\n",
      "Epoch 76 Loss 152.54996851969526 Training Accuracy 84.75\n",
      "Epoch 77 Loss 151.97393669898355 Training Accuracy 84.75\n",
      "Epoch 78 Loss 151.40614587292194 Training Accuracy 84.75\n",
      "Epoch 79 Loss 150.84642178477344 Training Accuracy 84.75\n",
      "Epoch 80 Loss 150.29459504595354 Training Accuracy 84.75\n",
      "Epoch 81 Loss 149.75050096989932 Training Accuracy 84.75\n",
      "Epoch 82 Loss 149.21397941249825 Training Accuracy 85.0\n",
      "Epoch 83 Loss 148.68487461879 Training Accuracy 85.25\n",
      "Epoch 84 Loss 148.16303507566647 Training Accuracy 85.25\n",
      "Epoch 85 Loss 147.64831337030884 Training Accuracy 85.25\n",
      "Epoch 86 Loss 147.1405660541125 Training Accuracy 85.5\n",
      "Epoch 87 Loss 146.63965351186238 Training Accuracy 85.5\n",
      "Epoch 88 Loss 146.1454398359328 Training Accuracy 85.5\n",
      "Epoch 89 Loss 145.65779270529507 Training Accuracy 85.5\n",
      "Epoch 90 Loss 145.17658326912806 Training Accuracy 85.5\n",
      "Epoch 91 Loss 144.70168603483495 Training Accuracy 85.25\n",
      "Epoch 92 Loss 144.23297876027902 Training Accuracy 85.25\n",
      "Epoch 93 Loss 143.77034235006033 Training Accuracy 85.25\n",
      "Epoch 94 Loss 143.3136607556624 Training Accuracy 85.5\n",
      "Epoch 95 Loss 142.86282087930695 Training Accuracy 85.5\n",
      "Epoch 96 Loss 142.41771248136152 Training Accuracy 85.5\n",
      "Epoch 97 Loss 141.97822809115158 Training Accuracy 85.5\n",
      "Epoch 98 Loss 141.54426292103665 Training Accuracy 85.75\n",
      "Epoch 99 Loss 141.11571478361475 Training Accuracy 85.75\n",
      "Epoch 100 Loss 140.69248401192738 Training Accuracy 85.75\n",
      "Epoch 101 Loss 140.27447338254115 Training Accuracy 85.75\n",
      "Epoch 102 Loss 139.86158804138944 Training Accuracy 86.0\n",
      "Epoch 103 Loss 139.45373543226157 Training Accuracy 86.0\n",
      "Epoch 104 Loss 139.05082522783235 Training Accuracy 86.0\n",
      "Epoch 105 Loss 138.6527692631298 Training Accuracy 86.0\n",
      "Epoch 106 Loss 138.25948147134326 Training Accuracy 86.0\n",
      "Epoch 107 Loss 137.87087782187814 Training Accuracy 86.0\n",
      "Epoch 108 Loss 137.48687626056807 Training Accuracy 86.25\n",
      "Epoch 109 Loss 137.10739665195933 Training Accuracy 86.25\n",
      "Epoch 110 Loss 136.7323607235852 Training Accuracy 86.25\n",
      "Epoch 111 Loss 136.36169201215247 Training Accuracy 86.25\n",
      "Epoch 112 Loss 135.99531581156575 Training Accuracy 86.25\n",
      "Epoch 113 Loss 135.63315912271693 Training Accuracy 86.5\n",
      "Epoch 114 Loss 135.2751506049728 Training Accuracy 86.75\n",
      "Epoch 115 Loss 134.921220529294 Training Accuracy 86.75\n",
      "Epoch 116 Loss 134.5713007329236 Training Accuracy 86.75\n",
      "Epoch 117 Loss 134.22532457558466 Training Accuracy 86.75\n",
      "Epoch 118 Loss 133.8832268971299 Training Accuracy 86.75\n",
      "Epoch 119 Loss 133.54494397658755 Training Accuracy 86.75\n",
      "Epoch 120 Loss 133.21041349255188 Training Accuracy 86.75\n",
      "Epoch 121 Loss 132.87957448486668 Training Accuracy 86.75\n",
      "Epoch 122 Loss 132.55236731755423 Training Accuracy 87.0\n",
      "Epoch 123 Loss 132.2287336429428 Training Accuracy 87.25\n",
      "Epoch 124 Loss 131.9086163669488 Training Accuracy 87.5\n",
      "Epoch 125 Loss 131.59195961547027 Training Accuracy 87.75\n",
      "Epoch 126 Loss 131.27870870185154 Training Accuracy 87.75\n",
      "Epoch 127 Loss 130.96881009537935 Training Accuracy 87.75\n",
      "Epoch 128 Loss 130.6622113907733 Training Accuracy 87.75\n",
      "Epoch 129 Loss 130.35886127863432 Training Accuracy 87.75\n",
      "Epoch 130 Loss 130.05870951681646 Training Accuracy 88.0\n",
      "Epoch 131 Loss 129.7617069026893 Training Accuracy 88.0\n",
      "Epoch 132 Loss 129.4678052462586 Training Accuracy 88.0\n",
      "Epoch 133 Loss 129.17695734411492 Training Accuracy 88.0\n",
      "Epoch 134 Loss 128.88911695418085 Training Accuracy 88.0\n",
      "Epoch 135 Loss 128.60423877122838 Training Accuracy 88.0\n",
      "Epoch 136 Loss 128.3222784031397 Training Accuracy 88.0\n",
      "Epoch 137 Loss 128.0431923478855 Training Accuracy 88.0\n",
      "Epoch 138 Loss 127.76693797119502 Training Accuracy 88.0\n",
      "Epoch 139 Loss 127.49347348489482 Training Accuracy 88.0\n",
      "Epoch 140 Loss 127.22275792589284 Training Accuracy 88.25\n",
      "Epoch 141 Loss 126.95475113578493 Training Accuracy 88.25\n",
      "Epoch 142 Loss 126.68941374106392 Training Accuracy 88.5\n",
      "Epoch 143 Loss 126.42670713390925 Training Accuracy 88.5\n",
      "Epoch 144 Loss 126.16659345353838 Training Accuracy 88.5\n",
      "Epoch 145 Loss 125.90903556810089 Training Accuracy 88.5\n",
      "Epoch 146 Loss 125.65399705709666 Training Accuracy 88.5\n",
      "Epoch 147 Loss 125.40144219430101 Training Accuracy 88.5\n",
      "Epoch 148 Loss 125.15133593117956 Training Accuracy 88.5\n",
      "Epoch 149 Loss 124.90364388077683 Training Accuracy 88.5\n",
      "Epoch 150 Loss 124.6583323020629 Training Accuracy 88.5\n",
      "Epoch 151 Loss 124.41536808472289 Training Accuracy 88.5\n",
      "Epoch 152 Loss 124.17471873437482 Training Accuracy 88.5\n",
      "Epoch 153 Loss 123.9363523582021 Training Accuracy 88.5\n",
      "Epoch 154 Loss 123.70023765098694 Training Accuracy 88.5\n",
      "Epoch 155 Loss 123.46634388153177 Training Accuracy 88.5\n",
      "Epoch 156 Loss 123.2346408794563 Training Accuracy 88.5\n",
      "Epoch 157 Loss 123.00509902235801 Training Accuracy 88.5\n",
      "Epoch 158 Loss 122.77768922332481 Training Accuracy 88.5\n",
      "Epoch 159 Loss 122.55238291878817 Training Accuracy 88.75\n",
      "Epoch 160 Loss 122.32915205670632 Training Accuracy 88.75\n",
      "Epoch 161 Loss 122.10796908506718 Training Accuracy 88.75\n",
      "Epoch 162 Loss 121.88880694070069 Training Accuracy 88.75\n",
      "Epoch 163 Loss 121.67163903839116 Training Accuracy 88.75\n",
      "Epoch 164 Loss 121.45643926028006 Training Accuracy 88.75\n",
      "Epoch 165 Loss 121.24318194555056 Training Accuracy 88.75\n",
      "Epoch 166 Loss 121.03184188038486 Training Accuracy 88.75\n",
      "Epoch 167 Loss 120.82239428818605 Training Accuracy 88.75\n",
      "Epoch 168 Loss 120.61481482005651 Training Accuracy 88.75\n",
      "Epoch 169 Loss 120.40907954552475 Training Accuracy 89.0\n",
      "Epoch 170 Loss 120.20516494351361 Training Accuracy 89.25\n",
      "Epoch 171 Loss 120.003047893542 Training Accuracy 89.5\n",
      "Epoch 172 Loss 119.80270566715372 Training Accuracy 89.25\n",
      "Epoch 173 Loss 119.60411591956598 Training Accuracy 89.25\n",
      "Epoch 174 Loss 119.40725668153162 Training Accuracy 89.0\n",
      "Epoch 175 Loss 119.21210635140827 Training Accuracy 89.0\n",
      "Epoch 176 Loss 119.0186436874286 Training Accuracy 89.0\n",
      "Epoch 177 Loss 118.82684780016555 Training Accuracy 89.0\n",
      "Epoch 178 Loss 118.63669814518704 Training Accuracy 89.0\n",
      "Epoch 179 Loss 118.44817451589432 Training Accuracy 89.0\n",
      "Epoch 180 Loss 118.26125703653888 Training Accuracy 89.0\n",
      "Epoch 181 Loss 118.0759261554127 Training Accuracy 89.0\n",
      "Epoch 182 Loss 117.89216263820676 Training Accuracy 89.0\n",
      "Epoch 183 Loss 117.70994756153301 Training Accuracy 89.0\n",
      "Epoch 184 Loss 117.52926230660515 Training Accuracy 89.0\n",
      "Epoch 185 Loss 117.35008855307362 Training Accuracy 89.0\n",
      "Epoch 186 Loss 117.17240827301055 Training Accuracy 89.0\n",
      "Epoch 187 Loss 116.99620372504027 Training Accuracy 89.0\n",
      "Epoch 188 Loss 116.82145744861154 Training Accuracy 89.0\n",
      "Epoch 189 Loss 116.64815225840707 Training Accuracy 89.0\n",
      "Epoch 190 Loss 116.47627123888697 Training Accuracy 89.0\n",
      "Epoch 191 Loss 116.30579773896221 Training Accuracy 89.0\n",
      "Epoch 192 Loss 116.1367153667944 Training Accuracy 89.0\n",
      "Epoch 193 Loss 115.96900798471854 Training Accuracy 89.0\n",
      "Epoch 194 Loss 115.80265970428525 Training Accuracy 89.0\n",
      "Epoch 195 Loss 115.63765488141925 Training Accuracy 89.25\n",
      "Epoch 196 Loss 115.4739781116911 Training Accuracy 89.25\n",
      "Epoch 197 Loss 115.31161422569868 Training Accuracy 89.25\n",
      "Epoch 198 Loss 115.15054828455588 Training Accuracy 89.25\n",
      "Epoch 199 Loss 114.9907655754854 Training Accuracy 89.5\n",
      "Epoch 200 Loss 114.83225160751289 Training Accuracy 89.5\n",
      "Epoch 201 Loss 114.67499210725967 Training Accuracy 89.5\n",
      "Epoch 202 Loss 114.51897301483153 Training Accuracy 89.5\n",
      "Epoch 203 Loss 114.36418047980092 Training Accuracy 89.5\n",
      "Epoch 204 Loss 114.21060085728018 Training Accuracy 89.5\n",
      "Epoch 205 Loss 114.05822070408328 Training Accuracy 89.5\n",
      "Epoch 206 Loss 113.90702677497382 Training Accuracy 89.5\n",
      "Epoch 207 Loss 113.75700601899709 Training Accuracy 89.5\n",
      "Epoch 208 Loss 113.60814557589383 Training Accuracy 89.5\n",
      "Epoch 209 Loss 113.4604327725938 Training Accuracy 89.5\n",
      "Epoch 210 Loss 113.31385511978678 Training Accuracy 89.5\n",
      "Epoch 211 Loss 113.1684003085694 Training Accuracy 89.5\n",
      "Epoch 212 Loss 113.02405620716542 Training Accuracy 89.5\n",
      "Epoch 213 Loss 112.88081085771806 Training Accuracy 89.5\n",
      "Epoch 214 Loss 112.73865247315193 Training Accuracy 89.5\n",
      "Epoch 215 Loss 112.59756943410352 Training Accuracy 89.5\n",
      "Epoch 216 Loss 112.45755028591786 Training Accuracy 89.5\n",
      "Epoch 217 Loss 112.3185837357101 Training Accuracy 89.5\n",
      "Epoch 218 Loss 112.18065864949016 Training Accuracy 89.5\n",
      "Epoch 219 Loss 112.04376404934892 Training Accuracy 89.5\n",
      "Epoch 220 Loss 111.90788911070464 Training Accuracy 89.5\n",
      "Epoch 221 Loss 111.77302315960763 Training Accuracy 89.5\n",
      "Epoch 222 Loss 111.63915567010213 Training Accuracy 89.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223 Loss 111.50627626164399 Training Accuracy 89.5\n",
      "Epoch 224 Loss 111.37437469657243 Training Accuracy 89.5\n",
      "Epoch 225 Loss 111.2434408776349 Training Accuracy 89.5\n",
      "Epoch 226 Loss 111.11346484556366 Training Accuracy 89.5\n",
      "Epoch 227 Loss 110.98443677670262 Training Accuracy 89.75\n",
      "Epoch 228 Loss 110.85634698068353 Training Accuracy 89.75\n",
      "Epoch 229 Loss 110.72918589815018 Training Accuracy 89.75\n",
      "Epoch 230 Loss 110.60294409852924 Training Accuracy 89.75\n",
      "Epoch 231 Loss 110.4776122778471 Training Accuracy 89.5\n",
      "Epoch 232 Loss 110.3531812565912 Training Accuracy 89.5\n",
      "Epoch 233 Loss 110.22964197761497 Training Accuracy 89.5\n",
      "Epoch 234 Loss 110.10698550408517 Training Accuracy 89.5\n",
      "Epoch 235 Loss 109.98520301747101 Training Accuracy 89.5\n",
      "Epoch 236 Loss 109.86428581557348 Training Accuracy 89.5\n",
      "Epoch 237 Loss 109.74422531059467 Training Accuracy 89.5\n",
      "Epoch 238 Loss 109.62501302724546 Training Accuracy 89.5\n",
      "Epoch 239 Loss 109.50664060089092 Training Accuracy 89.5\n",
      "Epoch 240 Loss 109.38909977573289 Training Accuracy 89.5\n",
      "Epoch 241 Loss 109.27238240302833 Training Accuracy 89.5\n",
      "Epoch 242 Loss 109.15648043934293 Training Accuracy 89.5\n",
      "Epoch 243 Loss 109.04138594483919 Training Accuracy 89.5\n",
      "Epoch 244 Loss 108.92709108159785 Training Accuracy 89.5\n",
      "Epoch 245 Loss 108.81358811197248 Training Accuracy 89.5\n",
      "Epoch 246 Loss 108.7008693969758 Training Accuracy 89.5\n",
      "Epoch 247 Loss 108.5889273946976 Training Accuracy 89.5\n",
      "Epoch 248 Loss 108.47775465875313 Training Accuracy 89.5\n",
      "Epoch 249 Loss 108.36734383676144 Training Accuracy 89.5\n",
      "Epoch 250 Loss 108.25768766885315 Training Accuracy 89.5\n",
      "Epoch 251 Loss 108.14877898620662 Training Accuracy 89.5\n",
      "Epoch 252 Loss 108.04061070961208 Training Accuracy 89.5\n",
      "Epoch 253 Loss 107.93317584806337 Training Accuracy 89.5\n",
      "Epoch 254 Loss 107.82646749737604 Training Accuracy 89.5\n",
      "Epoch 255 Loss 107.7204788388319 Training Accuracy 89.5\n",
      "Epoch 256 Loss 107.61520313784894 Training Accuracy 89.5\n",
      "Epoch 257 Loss 107.5106337426763 Training Accuracy 89.5\n",
      "Epoch 258 Loss 107.40676408311379 Training Accuracy 89.5\n",
      "Epoch 259 Loss 107.30358766925517 Training Accuracy 89.5\n",
      "Epoch 260 Loss 107.20109809025487 Training Accuracy 89.5\n",
      "Epoch 261 Loss 107.09928901311764 Training Accuracy 89.5\n",
      "Epoch 262 Loss 106.99815418151061 Training Accuracy 89.25\n",
      "Epoch 263 Loss 106.8976874145971 Training Accuracy 89.5\n",
      "Epoch 264 Loss 106.79788260589203 Training Accuracy 89.5\n",
      "Epoch 265 Loss 106.69873372213824 Training Accuracy 89.5\n",
      "Epoch 266 Loss 106.60023480220327 Training Accuracy 89.5\n",
      "Epoch 267 Loss 106.50237995599637 Training Accuracy 89.5\n",
      "Epoch 268 Loss 106.40516336340492 Training Accuracy 89.5\n",
      "Epoch 269 Loss 106.30857927325042 Training Accuracy 89.5\n",
      "Epoch 270 Loss 106.21262200226299 Training Accuracy 89.5\n",
      "Epoch 271 Loss 106.11728593407453 Training Accuracy 89.5\n",
      "Epoch 272 Loss 106.02256551822978 Training Accuracy 89.5\n",
      "Epoch 273 Loss 105.92845526921522 Training Accuracy 89.5\n",
      "Epoch 274 Loss 105.834949765505 Training Accuracy 89.5\n",
      "Epoch 275 Loss 105.74204364862405 Training Accuracy 89.5\n",
      "Epoch 276 Loss 105.64973162222779 Training Accuracy 89.5\n",
      "Epoch 277 Loss 105.5580084511978 Training Accuracy 89.5\n",
      "Epoch 278 Loss 105.46686896075369 Training Accuracy 89.5\n",
      "Epoch 279 Loss 105.37630803558045 Training Accuracy 89.5\n",
      "Epoch 280 Loss 105.286320618971 Training Accuracy 89.5\n",
      "Epoch 281 Loss 105.1969017119838 Training Accuracy 89.5\n",
      "Epoch 282 Loss 105.10804637261501 Training Accuracy 89.5\n",
      "Epoch 283 Loss 105.01974971498501 Training Accuracy 89.5\n",
      "Epoch 284 Loss 104.932006908539 Training Accuracy 89.5\n",
      "Epoch 285 Loss 104.84481317726144 Training Accuracy 89.5\n",
      "Epoch 286 Loss 104.75816379890372 Training Accuracy 89.5\n",
      "Epoch 287 Loss 104.6720541042255 Training Accuracy 89.75\n",
      "Epoch 288 Loss 104.58647947624843 Training Accuracy 89.75\n",
      "Epoch 289 Loss 104.50143534952312 Training Accuracy 89.75\n",
      "Epoch 290 Loss 104.41691720940818 Training Accuracy 89.75\n",
      "Epoch 291 Loss 104.33292059136166 Training Accuracy 89.75\n",
      "Epoch 292 Loss 104.24944108024422 Training Accuracy 89.75\n",
      "Epoch 293 Loss 104.16647430963438 Training Accuracy 89.75\n",
      "Epoch 294 Loss 104.08401596115485 Training Accuracy 89.75\n",
      "Epoch 295 Loss 104.0020617638104 Training Accuracy 90.0\n",
      "Epoch 296 Loss 103.92060749333673 Training Accuracy 90.0\n",
      "Epoch 297 Loss 103.83964897155994 Training Accuracy 90.0\n",
      "Epoch 298 Loss 103.75918206576704 Training Accuracy 90.0\n",
      "Epoch 299 Loss 103.67920268808635 Training Accuracy 90.0\n",
      "Epoch 300 Loss 103.59970679487866 Training Accuracy 90.0\n",
      "Epoch 301 Loss 103.5206903861378 Training Accuracy 90.0\n",
      "Epoch 302 Loss 103.44214950490164 Training Accuracy 90.0\n",
      "Epoch 303 Loss 103.36408023667218 Training Accuracy 90.0\n",
      "Epoch 304 Loss 103.2864787088455 Training Accuracy 90.0\n",
      "Epoch 305 Loss 103.20934109015062 Training Accuracy 90.0\n",
      "Epoch 306 Loss 103.13266359009782 Training Accuracy 90.0\n",
      "Epoch 307 Loss 103.05644245843546 Training Accuracy 90.0\n",
      "Epoch 308 Loss 102.98067398461592 Training Accuracy 90.0\n",
      "Epoch 309 Loss 102.90535449726987 Training Accuracy 90.25\n",
      "Epoch 310 Loss 102.83048036368909 Training Accuracy 90.25\n",
      "Epoch 311 Loss 102.75604798931747 Training Accuracy 90.25\n",
      "Epoch 312 Loss 102.68205381725014 Training Accuracy 90.25\n",
      "Epoch 313 Loss 102.60849432774064 Training Accuracy 90.25\n",
      "Epoch 314 Loss 102.53536603771576 Training Accuracy 90.25\n",
      "Epoch 315 Loss 102.46266550029807 Training Accuracy 90.25\n",
      "Epoch 316 Loss 102.39038930433607 Training Accuracy 90.25\n",
      "Epoch 317 Loss 102.31853407394156 Training Accuracy 90.25\n",
      "Epoch 318 Loss 102.24709646803444 Training Accuracy 90.25\n",
      "Epoch 319 Loss 102.17607317989447 Training Accuracy 90.25\n",
      "Epoch 320 Loss 102.10546093672005 Training Accuracy 90.25\n",
      "Epoch 321 Loss 102.03525649919385 Training Accuracy 90.25\n",
      "Epoch 322 Loss 101.96545666105521 Training Accuracy 90.25\n",
      "Epoch 323 Loss 101.8960582486792 Training Accuracy 90.25\n",
      "Epoch 324 Loss 101.82705812066189 Training Accuracy 90.25\n",
      "Epoch 325 Loss 101.75845316741231 Training Accuracy 90.25\n",
      "Epoch 326 Loss 101.69024031075057 Training Accuracy 90.25\n",
      "Epoch 327 Loss 101.62241650351189 Training Accuracy 90.25\n",
      "Epoch 328 Loss 101.55497872915709 Training Accuracy 90.25\n",
      "Epoch 329 Loss 101.48792400138865 Training Accuracy 90.25\n",
      "Epoch 330 Loss 101.42124936377263 Training Accuracy 90.25\n",
      "Epoch 331 Loss 101.35495188936656 Training Accuracy 90.25\n",
      "Epoch 332 Loss 101.28902868035259 Training Accuracy 90.25\n",
      "Epoch 333 Loss 101.22347686767641 Training Accuracy 90.25\n",
      "Epoch 334 Loss 101.15829361069144 Training Accuracy 90.25\n",
      "Epoch 335 Loss 101.0934760968085 Training Accuracy 90.25\n",
      "Epoch 336 Loss 101.02902154115046 Training Accuracy 90.25\n",
      "Epoch 337 Loss 100.96492718621226 Training Accuracy 90.25\n",
      "Epoch 338 Loss 100.90119030152587 Training Accuracy 90.25\n",
      "Epoch 339 Loss 100.83780818333017 Training Accuracy 90.25\n",
      "Epoch 340 Loss 100.77477815424582 Training Accuracy 90.25\n",
      "Epoch 341 Loss 100.71209756295485 Training Accuracy 90.5\n",
      "Epoch 342 Loss 100.64976378388482 Training Accuracy 90.5\n",
      "Epoch 343 Loss 100.58777421689787 Training Accuracy 90.5\n",
      "Epoch 344 Loss 100.52612628698412 Training Accuracy 90.5\n",
      "Epoch 345 Loss 100.46481744395963 Training Accuracy 90.5\n",
      "Epoch 346 Loss 100.40384516216864 Training Accuracy 90.5\n",
      "Epoch 347 Loss 100.34320694019021 Training Accuracy 90.5\n",
      "Epoch 348 Loss 100.28290030054916 Training Accuracy 90.5\n",
      "Epoch 349 Loss 100.22292278943098 Training Accuracy 90.5\n",
      "Epoch 350 Loss 100.16327197640103 Training Accuracy 90.5\n",
      "Epoch 351 Loss 100.10394545412768 Training Accuracy 90.5\n",
      "Epoch 352 Loss 100.04494083810945 Training Accuracy 90.75\n",
      "Epoch 353 Loss 99.98625576640596 Training Accuracy 90.75\n",
      "Epoch 354 Loss 99.92788789937279 Training Accuracy 90.75\n",
      "Epoch 355 Loss 99.86983491940015 Training Accuracy 90.75\n",
      "Epoch 356 Loss 99.81209453065505 Training Accuracy 90.75\n",
      "Epoch 357 Loss 99.75466445882736 Training Accuracy 90.75\n",
      "Epoch 358 Loss 99.69754245087924 Training Accuracy 90.75\n",
      "Epoch 359 Loss 99.6407262747983 Training Accuracy 90.75\n",
      "Epoch 360 Loss 99.58421371935395 Training Accuracy 90.75\n",
      "Epoch 361 Loss 99.52800259385747 Training Accuracy 90.75\n",
      "Epoch 362 Loss 99.47209072792506 Training Accuracy 90.75\n",
      "Epoch 363 Loss 99.41647597124465 Training Accuracy 90.75\n",
      "Epoch 364 Loss 99.36115619334547 Training Accuracy 90.5\n",
      "Epoch 365 Loss 99.30612928337109 Training Accuracy 90.5\n",
      "Epoch 366 Loss 99.25139314985563 Training Accuracy 90.5\n",
      "Epoch 367 Loss 99.19694572050278 Training Accuracy 90.5\n",
      "Epoch 368 Loss 99.14278494196822 Training Accuracy 90.5\n",
      "Epoch 369 Loss 99.08890877964465 Training Accuracy 90.5\n",
      "Epoch 370 Loss 99.03531521745012 Training Accuracy 90.5\n",
      "Epoch 371 Loss 98.98200225761889 Training Accuracy 90.5\n",
      "Epoch 372 Loss 98.92896792049547 Training Accuracy 90.5\n",
      "Epoch 373 Loss 98.87621024433122 Training Accuracy 90.5\n",
      "Epoch 374 Loss 98.82372728508383 Training Accuracy 90.5\n",
      "Epoch 375 Loss 98.7715171162194 Training Accuracy 90.5\n",
      "Epoch 376 Loss 98.71957782851732 Training Accuracy 90.5\n",
      "Epoch 377 Loss 98.66790752987765 Training Accuracy 90.5\n",
      "Epoch 378 Loss 98.61650434513122 Training Accuracy 90.5\n",
      "Epoch 379 Loss 98.56536641585208 Training Accuracy 90.5\n",
      "Epoch 380 Loss 98.51449190017263 Training Accuracy 90.5\n",
      "Epoch 381 Loss 98.46387897260115 Training Accuracy 90.5\n",
      "Epoch 382 Loss 98.41352582384167 Training Accuracy 90.5\n",
      "Epoch 383 Loss 98.36343066061644 Training Accuracy 90.5\n",
      "Epoch 384 Loss 98.31359170549048 Training Accuracy 90.5\n",
      "Epoch 385 Loss 98.26400719669863 Training Accuracy 90.5\n",
      "Epoch 386 Loss 98.21467538797481 Training Accuracy 90.5\n",
      "Epoch 387 Loss 98.16559454838352 Training Accuracy 90.5\n",
      "Epoch 388 Loss 98.11676296215367 Training Accuracy 90.5\n",
      "Epoch 389 Loss 98.06817892851424 Training Accuracy 90.5\n",
      "Epoch 390 Loss 98.01984076153252 Training Accuracy 90.5\n",
      "Epoch 391 Loss 97.97174678995418 Training Accuracy 90.5\n",
      "Epoch 392 Loss 97.92389535704541 Training Accuracy 90.5\n",
      "Epoch 393 Loss 97.87628482043729 Training Accuracy 90.5\n",
      "Epoch 394 Loss 97.82891355197197 Training Accuracy 90.5\n",
      "Epoch 395 Loss 97.78177993755091 Training Accuracy 90.5\n",
      "Epoch 396 Loss 97.73488237698517 Training Accuracy 90.5\n",
      "Epoch 397 Loss 97.6882192838474 Training Accuracy 90.5\n",
      "Epoch 398 Loss 97.64178908532591 Training Accuracy 90.5\n",
      "Epoch 399 Loss 97.5955902220806 Training Accuracy 90.5\n",
      "Epoch 400 Loss 97.54962114810053 Training Accuracy 90.5\n",
      "Epoch 401 Loss 97.50388033056353 Training Accuracy 90.5\n",
      "Epoch 402 Loss 97.45836624969752 Training Accuracy 90.5\n",
      "Epoch 403 Loss 97.41307739864348 Training Accuracy 90.5\n",
      "Epoch 404 Loss 97.36801228332027 Training Accuracy 90.5\n",
      "Epoch 405 Loss 97.3231694222911 Training Accuracy 90.5\n",
      "Epoch 406 Loss 97.27854734663177 Training Accuracy 90.5\n",
      "Epoch 407 Loss 97.23414459980033 Training Accuracy 90.5\n",
      "Epoch 408 Loss 97.18995973750867 Training Accuracy 90.5\n",
      "Epoch 409 Loss 97.14599132759544 Training Accuracy 90.5\n",
      "Epoch 410 Loss 97.1022379499008 Training Accuracy 90.5\n",
      "Epoch 411 Loss 97.0586981961425 Training Accuracy 90.5\n",
      "Epoch 412 Loss 97.01537066979367 Training Accuracy 90.5\n",
      "Epoch 413 Loss 96.972253985962 Training Accuracy 90.5\n",
      "Epoch 414 Loss 96.92934677127053 Training Accuracy 90.5\n",
      "Epoch 415 Loss 96.88664766373975 Training Accuracy 90.5\n",
      "Epoch 416 Loss 96.84415531267129 Training Accuracy 90.5\n",
      "Epoch 417 Loss 96.80186837853303 Training Accuracy 90.5\n",
      "Epoch 418 Loss 96.75978553284548 Training Accuracy 90.75\n",
      "Epoch 419 Loss 96.71790545806974 Training Accuracy 90.75\n",
      "Epoch 420 Loss 96.67622684749658 Training Accuracy 90.75\n",
      "Epoch 421 Loss 96.63474840513717 Training Accuracy 90.75\n",
      "Epoch 422 Loss 96.5934688456149 Training Accuracy 90.75\n",
      "Epoch 423 Loss 96.55238689405856 Training Accuracy 90.75\n",
      "Epoch 424 Loss 96.5115012859969 Training Accuracy 90.75\n",
      "Epoch 425 Loss 96.47081076725433 Training Accuracy 90.75\n",
      "Epoch 426 Loss 96.43031409384804 Training Accuracy 90.75\n",
      "Epoch 427 Loss 96.39001003188609 Training Accuracy 90.75\n",
      "Epoch 428 Loss 96.34989735746711 Training Accuracy 90.75\n",
      "Epoch 429 Loss 96.30997485658075 Training Accuracy 90.75\n",
      "Epoch 430 Loss 96.27024132500969 Training Accuracy 90.75\n",
      "Epoch 431 Loss 96.23069556823263 Training Accuracy 90.75\n",
      "Epoch 432 Loss 96.1913364013284 Training Accuracy 90.75\n",
      "Epoch 433 Loss 96.15216264888139 Training Accuracy 90.75\n",
      "Epoch 434 Loss 96.1131731448879 Training Accuracy 90.75\n",
      "Epoch 435 Loss 96.07436673266368 Training Accuracy 90.75\n",
      "Epoch 436 Loss 96.0357422647526 Training Accuracy 90.75\n",
      "Epoch 437 Loss 95.99729860283634 Training Accuracy 90.75\n",
      "Epoch 438 Loss 95.9590346176451 Training Accuracy 90.75\n",
      "Epoch 439 Loss 95.92094918886943 Training Accuracy 90.75\n",
      "Epoch 440 Loss 95.88304120507308 Training Accuracy 90.75\n",
      "Epoch 441 Loss 95.84530956360686 Training Accuracy 90.75\n",
      "Epoch 442 Loss 95.80775317052341 Training Accuracy 90.75\n",
      "Epoch 443 Loss 95.77037094049311 Training Accuracy 90.75\n",
      "Epoch 444 Loss 95.73316179672085 Training Accuracy 90.75\n",
      "Epoch 445 Loss 95.69612467086387 Training Accuracy 90.75\n",
      "Epoch 446 Loss 95.65925850295034 Training Accuracy 90.75\n",
      "Epoch 447 Loss 95.6225622412992 Training Accuracy 90.75\n",
      "Epoch 448 Loss 95.58603484244061 Training Accuracy 90.75\n",
      "Epoch 449 Loss 95.54967527103747 Training Accuracy 90.75\n",
      "Epoch 450 Loss 95.5134824998078 Training Accuracy 90.75\n",
      "Epoch 451 Loss 95.47745550944802 Training Accuracy 90.75\n",
      "Epoch 452 Loss 95.44159328855717 Training Accuracy 91.0\n",
      "Epoch 453 Loss 95.40589483356175 Training Accuracy 91.0\n",
      "Epoch 454 Loss 95.37035914864177 Training Accuracy 91.0\n",
      "Epoch 455 Loss 95.33498524565732 Training Accuracy 91.0\n",
      "Epoch 456 Loss 95.2997721440761 Training Accuracy 91.0\n",
      "Epoch 457 Loss 95.2647188709019 Training Accuracy 91.0\n",
      "Epoch 458 Loss 95.22982446060362 Training Accuracy 91.0\n",
      "Epoch 459 Loss 95.19508795504521 Training Accuracy 91.0\n",
      "Epoch 460 Loss 95.16050840341654 Training Accuracy 91.0\n",
      "Epoch 461 Loss 95.1260848621647 Training Accuracy 91.0\n",
      "Epoch 462 Loss 95.09181639492655 Training Accuracy 91.25\n",
      "Epoch 463 Loss 95.05770207246147 Training Accuracy 91.25\n",
      "Epoch 464 Loss 95.0237409725853 Training Accuracy 91.25\n",
      "Epoch 465 Loss 94.9899321801048 Training Accuracy 91.25\n",
      "Epoch 466 Loss 94.95627478675294 Training Accuracy 91.25\n",
      "Epoch 467 Loss 94.92276789112472 Training Accuracy 91.25\n",
      "Epoch 468 Loss 94.889410598614 Training Accuracy 91.25\n",
      "Epoch 469 Loss 94.8562020213507 Training Accuracy 91.25\n",
      "Epoch 470 Loss 94.82314127813892 Training Accuracy 91.25\n",
      "Epoch 471 Loss 94.79022749439571 Training Accuracy 91.25\n",
      "Epoch 472 Loss 94.75745980209038 Training Accuracy 91.25\n",
      "Epoch 473 Loss 94.72483733968461 Training Accuracy 91.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 474 Loss 94.69235925207312 Training Accuracy 91.25\n",
      "Epoch 475 Loss 94.66002469052508 Training Accuracy 91.25\n",
      "Epoch 476 Loss 94.62783281262607 Training Accuracy 91.25\n",
      "Epoch 477 Loss 94.59578278222071 Training Accuracy 91.25\n",
      "Epoch 478 Loss 94.56387376935596 Training Accuracy 91.25\n",
      "Epoch 479 Loss 94.5321049502249 Training Accuracy 91.25\n",
      "Epoch 480 Loss 94.50047550711123 Training Accuracy 91.25\n",
      "Epoch 481 Loss 94.46898462833434 Training Accuracy 91.25\n",
      "Epoch 482 Loss 94.43763150819497 Training Accuracy 91.25\n",
      "Epoch 483 Loss 94.40641534692143 Training Accuracy 91.25\n",
      "Epoch 484 Loss 94.37533535061644 Training Accuracy 91.25\n",
      "Epoch 485 Loss 94.34439073120446 Training Accuracy 91.25\n",
      "Epoch 486 Loss 94.3135807063797 Training Accuracy 91.25\n",
      "Epoch 487 Loss 94.28290449955458 Training Accuracy 91.25\n",
      "Epoch 488 Loss 94.25236133980877 Training Accuracy 91.25\n",
      "Epoch 489 Loss 94.22195046183882 Training Accuracy 91.25\n",
      "Epoch 490 Loss 94.19167110590818 Training Accuracy 91.25\n",
      "Epoch 491 Loss 94.161522517798 Training Accuracy 91.25\n",
      "Epoch 492 Loss 94.13150394875814 Training Accuracy 91.25\n",
      "Epoch 493 Loss 94.10161465545897 Training Accuracy 91.25\n",
      "Epoch 494 Loss 94.07185389994353 Training Accuracy 91.25\n",
      "Epoch 495 Loss 94.04222094958014 Training Accuracy 91.25\n",
      "Epoch 496 Loss 94.01271507701571 Training Accuracy 91.25\n",
      "Epoch 497 Loss 93.98333556012938 Training Accuracy 91.25\n",
      "Epoch 498 Loss 93.95408168198662 Training Accuracy 91.25\n",
      "Epoch 499 Loss 93.92495273079392 Training Accuracy 91.25\n",
      "Epoch 500 Loss 93.89594799985392 Training Accuracy 91.25\n",
      "Epoch 501 Loss 93.86706678752097 Training Accuracy 91.25\n",
      "Epoch 502 Loss 93.83830839715709 Training Accuracy 91.25\n",
      "Epoch 503 Loss 93.80967213708858 Training Accuracy 91.25\n",
      "Epoch 504 Loss 93.78115732056295 Training Accuracy 91.25\n",
      "Epoch 505 Loss 93.7527632657062 Training Accuracy 91.25\n",
      "Epoch 506 Loss 93.72448929548071 Training Accuracy 91.25\n",
      "Epoch 507 Loss 93.69633473764355 Training Accuracy 91.25\n",
      "Epoch 508 Loss 93.66829892470506 Training Accuracy 91.25\n",
      "Epoch 509 Loss 93.64038119388803 Training Accuracy 91.25\n",
      "Epoch 510 Loss 93.61258088708723 Training Accuracy 91.25\n",
      "Epoch 511 Loss 93.58489735082927 Training Accuracy 91.25\n",
      "Epoch 512 Loss 93.55732993623303 Training Accuracy 91.25\n",
      "Epoch 513 Loss 93.52987799897039 Training Accuracy 91.25\n",
      "Epoch 514 Loss 93.50254089922737 Training Accuracy 91.25\n",
      "Epoch 515 Loss 93.47531800166563 Training Accuracy 91.25\n",
      "Epoch 516 Loss 93.4482086753845 Training Accuracy 91.25\n",
      "Epoch 517 Loss 93.42121229388322 Training Accuracy 91.25\n",
      "Epoch 518 Loss 93.39432823502361 Training Accuracy 91.25\n",
      "Epoch 519 Loss 93.36755588099322 Training Accuracy 91.25\n",
      "Epoch 520 Loss 93.34089461826873 Training Accuracy 91.25\n",
      "Epoch 521 Loss 93.31434383757976 Training Accuracy 91.25\n",
      "Epoch 522 Loss 93.28790293387303 Training Accuracy 91.25\n",
      "Epoch 523 Loss 93.2615713062769 Training Accuracy 91.25\n",
      "Epoch 524 Loss 93.23534835806629 Training Accuracy 91.25\n",
      "Epoch 525 Loss 93.20923349662783 Training Accuracy 91.25\n",
      "Epoch 526 Loss 93.18322613342549 Training Accuracy 91.5\n",
      "Epoch 527 Loss 93.1573256839665 Training Accuracy 91.5\n",
      "Epoch 528 Loss 93.13153156776758 Training Accuracy 91.5\n",
      "Epoch 529 Loss 93.10584320832157 Training Accuracy 91.5\n",
      "Epoch 530 Loss 93.08026003306435 Training Accuracy 91.5\n",
      "Epoch 531 Loss 93.05478147334199 Training Accuracy 91.5\n",
      "Epoch 532 Loss 93.02940696437848 Training Accuracy 91.5\n",
      "Epoch 533 Loss 93.00413594524349 Training Accuracy 91.5\n",
      "Epoch 534 Loss 92.97896785882062 Training Accuracy 91.5\n",
      "Epoch 535 Loss 92.95390215177594 Training Accuracy 91.5\n",
      "Epoch 536 Loss 92.92893827452676 Training Accuracy 91.5\n",
      "Epoch 537 Loss 92.90407568121077 Training Accuracy 91.5\n",
      "Epoch 538 Loss 92.87931382965553 Training Accuracy 91.5\n",
      "Epoch 539 Loss 92.85465218134807 Training Accuracy 91.5\n",
      "Epoch 540 Loss 92.83009020140508 Training Accuracy 91.5\n",
      "Epoch 541 Loss 92.80562735854308 Training Accuracy 91.5\n",
      "Epoch 542 Loss 92.78126312504904 Training Accuracy 91.5\n",
      "Epoch 543 Loss 92.75699697675131 Training Accuracy 91.5\n",
      "Epoch 544 Loss 92.73282839299078 Training Accuracy 91.5\n",
      "Epoch 545 Loss 92.70875685659229 Training Accuracy 91.5\n",
      "Epoch 546 Loss 92.68478185383631 Training Accuracy 91.5\n",
      "Epoch 547 Loss 92.66090287443109 Training Accuracy 91.5\n",
      "Epoch 548 Loss 92.63711941148468 Training Accuracy 91.5\n",
      "Epoch 549 Loss 92.61343096147765 Training Accuracy 91.5\n",
      "Epoch 550 Loss 92.58983702423578 Training Accuracy 91.5\n",
      "Epoch 551 Loss 92.5663371029031 Training Accuracy 91.5\n",
      "Epoch 552 Loss 92.54293070391529 Training Accuracy 91.5\n",
      "Epoch 553 Loss 92.51961733697306 Training Accuracy 91.5\n",
      "Epoch 554 Loss 92.49639651501616 Training Accuracy 91.5\n",
      "Epoch 555 Loss 92.47326775419728 Training Accuracy 91.5\n",
      "Epoch 556 Loss 92.4502305738564 Training Accuracy 91.5\n",
      "Epoch 557 Loss 92.42728449649539 Training Accuracy 91.5\n",
      "Epoch 558 Loss 92.40442904775267 Training Accuracy 91.5\n",
      "Epoch 559 Loss 92.3816637563784 Training Accuracy 91.5\n",
      "Epoch 560 Loss 92.35898815420958 Training Accuracy 91.5\n",
      "Epoch 561 Loss 92.33640177614565 Training Accuracy 91.5\n",
      "Epoch 562 Loss 92.31390416012408 Training Accuracy 91.5\n",
      "Epoch 563 Loss 92.29149484709652 Training Accuracy 91.5\n",
      "Epoch 564 Loss 92.2691733810048 Training Accuracy 91.5\n",
      "Epoch 565 Loss 92.24693930875733 Training Accuracy 91.5\n",
      "Epoch 566 Loss 92.22479218020587 Training Accuracy 91.5\n",
      "Epoch 567 Loss 92.20273154812216 Training Accuracy 91.5\n",
      "Epoch 568 Loss 92.18075696817516 Training Accuracy 91.5\n",
      "Epoch 569 Loss 92.15886799890814 Training Accuracy 91.5\n",
      "Epoch 570 Loss 92.13706420171626 Training Accuracy 91.5\n",
      "Epoch 571 Loss 92.11534514082423 Training Accuracy 91.5\n",
      "Epoch 572 Loss 92.09371038326418 Training Accuracy 91.5\n",
      "Epoch 573 Loss 92.07215949885375 Training Accuracy 91.5\n",
      "Epoch 574 Loss 92.05069206017441 Training Accuracy 91.5\n",
      "Epoch 575 Loss 92.0293076425499 Training Accuracy 91.5\n",
      "Epoch 576 Loss 92.00800582402499 Training Accuracy 91.5\n",
      "Epoch 577 Loss 91.98678618534436 Training Accuracy 91.5\n",
      "Epoch 578 Loss 91.96564830993154 Training Accuracy 91.5\n",
      "Epoch 579 Loss 91.9445917838684 Training Accuracy 91.5\n",
      "Epoch 580 Loss 91.92361619587443 Training Accuracy 91.5\n",
      "Epoch 581 Loss 91.90272113728646 Training Accuracy 91.5\n",
      "Epoch 582 Loss 91.88190620203855 Training Accuracy 91.5\n",
      "Epoch 583 Loss 91.86117098664187 Training Accuracy 91.5\n",
      "Epoch 584 Loss 91.84051509016497 Training Accuracy 91.5\n",
      "Epoch 585 Loss 91.81993811421421 Training Accuracy 91.5\n",
      "Epoch 586 Loss 91.79943966291422 Training Accuracy 91.5\n",
      "Epoch 587 Loss 91.77901934288869 Training Accuracy 91.5\n",
      "Epoch 588 Loss 91.7586767632412 Training Accuracy 91.5\n",
      "Epoch 589 Loss 91.73841153553644 Training Accuracy 91.5\n",
      "Epoch 590 Loss 91.71822327378126 Training Accuracy 91.5\n",
      "Epoch 591 Loss 91.69811159440626 Training Accuracy 91.5\n",
      "Epoch 592 Loss 91.6780761162473 Training Accuracy 91.5\n",
      "Epoch 593 Loss 91.65811646052714 Training Accuracy 91.5\n",
      "Epoch 594 Loss 91.63823225083763 Training Accuracy 91.5\n",
      "Epoch 595 Loss 91.61842311312145 Training Accuracy 91.5\n",
      "Epoch 596 Loss 91.5986886756546 Training Accuracy 91.5\n",
      "Epoch 597 Loss 91.57902856902862 Training Accuracy 91.5\n",
      "Epoch 598 Loss 91.55944242613322 Training Accuracy 91.5\n",
      "Epoch 599 Loss 91.53992988213895 Training Accuracy 91.5\n",
      "Epoch 600 Loss 91.52049057448014 Training Accuracy 91.5\n",
      "Epoch 601 Loss 91.50112414283771 Training Accuracy 91.5\n",
      "Epoch 602 Loss 91.48183022912256 Training Accuracy 91.5\n",
      "Epoch 603 Loss 91.4626084774587 Training Accuracy 91.5\n",
      "Epoch 604 Loss 91.44345853416681 Training Accuracy 91.5\n",
      "Epoch 605 Loss 91.42438004774775 Training Accuracy 91.5\n",
      "Epoch 606 Loss 91.40537266886636 Training Accuracy 91.5\n",
      "Epoch 607 Loss 91.38643605033539 Training Accuracy 91.5\n",
      "Epoch 608 Loss 91.36756984709938 Training Accuracy 91.5\n",
      "Epoch 609 Loss 91.34877371621896 Training Accuracy 91.5\n",
      "Epoch 610 Loss 91.33004731685514 Training Accuracy 91.5\n",
      "Epoch 611 Loss 91.31139031025364 Training Accuracy 91.5\n",
      "Epoch 612 Loss 91.29280235972968 Training Accuracy 91.5\n",
      "Epoch 613 Loss 91.27428313065242 Training Accuracy 91.5\n",
      "Epoch 614 Loss 91.25583229043004 Training Accuracy 91.5\n",
      "Epoch 615 Loss 91.23744950849458 Training Accuracy 91.5\n",
      "Epoch 616 Loss 91.21913445628712 Training Accuracy 91.5\n",
      "Epoch 617 Loss 91.200886807243 Training Accuracy 91.5\n",
      "Epoch 618 Loss 91.18270623677708 Training Accuracy 91.5\n",
      "Epoch 619 Loss 91.16459242226944 Training Accuracy 91.5\n",
      "Epoch 620 Loss 91.14654504305079 Training Accuracy 91.5\n",
      "Epoch 621 Loss 91.12856378038833 Training Accuracy 91.5\n",
      "Epoch 622 Loss 91.1106483174716 Training Accuracy 91.5\n",
      "Epoch 623 Loss 91.09279833939836 Training Accuracy 91.5\n",
      "Epoch 624 Loss 91.07501353316084 Training Accuracy 91.5\n",
      "Epoch 625 Loss 91.0572935876319 Training Accuracy 91.5\n",
      "Epoch 626 Loss 91.03963819355131 Training Accuracy 91.5\n",
      "Epoch 627 Loss 91.02204704351229 Training Accuracy 91.5\n",
      "Epoch 628 Loss 91.00451983194809 Training Accuracy 91.5\n",
      "Epoch 629 Loss 90.9870562551186 Training Accuracy 91.5\n",
      "Epoch 630 Loss 90.96965601109717 Training Accuracy 91.5\n",
      "Epoch 631 Loss 90.9523187997576 Training Accuracy 91.5\n",
      "Epoch 632 Loss 90.9350443227611 Training Accuracy 91.5\n",
      "Epoch 633 Loss 90.91783228354339 Training Accuracy 91.5\n",
      "Epoch 634 Loss 90.90068238730203 Training Accuracy 91.5\n",
      "Epoch 635 Loss 90.88359434098368 Training Accuracy 91.5\n",
      "Epoch 636 Loss 90.86656785327163 Training Accuracy 91.5\n",
      "Epoch 637 Loss 90.84960263457336 Training Accuracy 91.5\n",
      "Epoch 638 Loss 90.83269839700809 Training Accuracy 91.5\n",
      "Epoch 639 Loss 90.81585485439476 Training Accuracy 91.5\n",
      "Epoch 640 Loss 90.79907172223972 Training Accuracy 91.5\n",
      "Epoch 641 Loss 90.78234871772476 Training Accuracy 91.5\n",
      "Epoch 642 Loss 90.76568555969521 Training Accuracy 91.5\n",
      "Epoch 643 Loss 90.74908196864814 Training Accuracy 91.5\n",
      "Epoch 644 Loss 90.73253766672057 Training Accuracy 91.5\n",
      "Epoch 645 Loss 90.71605237767787 Training Accuracy 91.5\n",
      "Epoch 646 Loss 90.69962582690223 Training Accuracy 91.5\n",
      "Epoch 647 Loss 90.68325774138125 Training Accuracy 91.5\n",
      "Epoch 648 Loss 90.66694784969653 Training Accuracy 91.5\n",
      "Epoch 649 Loss 90.65069588201251 Training Accuracy 91.5\n",
      "Epoch 650 Loss 90.63450157006531 Training Accuracy 91.5\n",
      "Epoch 651 Loss 90.61836464715154 Training Accuracy 91.5\n",
      "Epoch 652 Loss 90.6022848481175 Training Accuracy 91.5\n",
      "Epoch 653 Loss 90.58626190934828 Training Accuracy 91.5\n",
      "Epoch 654 Loss 90.5702955687568 Training Accuracy 91.5\n",
      "Epoch 655 Loss 90.55438556577334 Training Accuracy 91.5\n",
      "Epoch 656 Loss 90.53853164133477 Training Accuracy 91.5\n",
      "Epoch 657 Loss 90.52273353787413 Training Accuracy 91.5\n",
      "Epoch 658 Loss 90.50699099931005 Training Accuracy 91.5\n",
      "Epoch 659 Loss 90.49130377103661 Training Accuracy 91.5\n",
      "Epoch 660 Loss 90.47567159991286 Training Accuracy 91.5\n",
      "Epoch 661 Loss 90.46009423425275 Training Accuracy 91.5\n",
      "Epoch 662 Loss 90.44457142381503 Training Accuracy 91.5\n",
      "Epoch 663 Loss 90.42910291979321 Training Accuracy 91.5\n",
      "Epoch 664 Loss 90.4136884748056 Training Accuracy 91.5\n",
      "Epoch 665 Loss 90.39832784288546 Training Accuracy 91.5\n",
      "Epoch 666 Loss 90.38302077947131 Training Accuracy 91.5\n",
      "Epoch 667 Loss 90.36776704139709 Training Accuracy 91.5\n",
      "Epoch 668 Loss 90.35256638688264 Training Accuracy 91.5\n",
      "Epoch 669 Loss 90.33741857552417 Training Accuracy 91.5\n",
      "Epoch 670 Loss 90.32232336828474 Training Accuracy 91.5\n",
      "Epoch 671 Loss 90.3072805274849 Training Accuracy 91.5\n",
      "Epoch 672 Loss 90.29228981679336 Training Accuracy 91.5\n",
      "Epoch 673 Loss 90.27735100121775 Training Accuracy 91.5\n",
      "Epoch 674 Loss 90.26246384709555 Training Accuracy 91.5\n",
      "Epoch 675 Loss 90.24762812208488 Training Accuracy 91.5\n",
      "Epoch 676 Loss 90.23284359515553 Training Accuracy 91.5\n",
      "Epoch 677 Loss 90.21811003658001 Training Accuracy 91.5\n",
      "Epoch 678 Loss 90.2034272179248 Training Accuracy 91.5\n",
      "Epoch 679 Loss 90.18879491204126 Training Accuracy 91.5\n",
      "Epoch 680 Loss 90.17421289305723 Training Accuracy 91.5\n",
      "Epoch 681 Loss 90.15968093636812 Training Accuracy 91.5\n",
      "Epoch 682 Loss 90.1451988186285 Training Accuracy 91.5\n",
      "Epoch 683 Loss 90.13076631774341 Training Accuracy 91.5\n",
      "Epoch 684 Loss 90.11638321286006 Training Accuracy 91.5\n",
      "Epoch 685 Loss 90.1020492843593 Training Accuracy 91.5\n",
      "Epoch 686 Loss 90.0877643138474 Training Accuracy 91.5\n",
      "Epoch 687 Loss 90.07352808414782 Training Accuracy 91.5\n",
      "Epoch 688 Loss 90.05934037929285 Training Accuracy 91.5\n",
      "Epoch 689 Loss 90.04520098451563 Training Accuracy 91.5\n",
      "Epoch 690 Loss 90.0311096862421 Training Accuracy 91.5\n",
      "Epoch 691 Loss 90.0170662720829 Training Accuracy 91.5\n",
      "Epoch 692 Loss 90.00307053082548 Training Accuracy 91.5\n",
      "Epoch 693 Loss 89.98912225242628 Training Accuracy 91.5\n",
      "Epoch 694 Loss 89.97522122800281 Training Accuracy 91.5\n",
      "Epoch 695 Loss 89.96136724982603 Training Accuracy 91.5\n",
      "Epoch 696 Loss 89.9475601113125 Training Accuracy 91.5\n",
      "Epoch 697 Loss 89.93379960701692 Training Accuracy 91.5\n",
      "Epoch 698 Loss 89.92008553262444 Training Accuracy 91.5\n",
      "Epoch 699 Loss 89.9064176849432 Training Accuracy 91.5\n",
      "Epoch 700 Loss 89.89279586189687 Training Accuracy 91.5\n",
      "Epoch 701 Loss 89.87921986251723 Training Accuracy 91.5\n",
      "Epoch 702 Loss 89.86568948693687 Training Accuracy 91.5\n",
      "Epoch 703 Loss 89.8522045363819 Training Accuracy 91.5\n",
      "Epoch 704 Loss 89.83876481316473 Training Accuracy 91.5\n",
      "Epoch 705 Loss 89.82537012067687 Training Accuracy 91.5\n",
      "Epoch 706 Loss 89.81202026338184 Training Accuracy 91.5\n",
      "Epoch 707 Loss 89.79871504680811 Training Accuracy 91.5\n",
      "Epoch 708 Loss 89.78545427754214 Training Accuracy 91.5\n",
      "Epoch 709 Loss 89.77223776322137 Training Accuracy 91.5\n",
      "Epoch 710 Loss 89.7590653125273 Training Accuracy 91.5\n",
      "Epoch 711 Loss 89.74593673517877 Training Accuracy 91.5\n",
      "Epoch 712 Loss 89.73285184192505 Training Accuracy 91.5\n",
      "Epoch 713 Loss 89.71981044453918 Training Accuracy 91.5\n",
      "Epoch 714 Loss 89.70681235581122 Training Accuracy 91.5\n",
      "Epoch 715 Loss 89.69385738954165 Training Accuracy 91.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 716 Loss 89.68094536053479 Training Accuracy 91.5\n",
      "Epoch 717 Loss 89.66807608459227 Training Accuracy 91.5\n",
      "Epoch 718 Loss 89.65524937850653 Training Accuracy 91.5\n",
      "Epoch 719 Loss 89.64246506005436 Training Accuracy 91.5\n",
      "Epoch 720 Loss 89.6297229479906 Training Accuracy 91.5\n",
      "Epoch 721 Loss 89.61702286204171 Training Accuracy 91.5\n",
      "Epoch 722 Loss 89.60436462289952 Training Accuracy 91.5\n",
      "Epoch 723 Loss 89.59174805221501 Training Accuracy 91.5\n",
      "Epoch 724 Loss 89.57917297259209 Training Accuracy 91.5\n",
      "Epoch 725 Loss 89.56663920758143 Training Accuracy 91.5\n",
      "Epoch 726 Loss 89.55414658167442 Training Accuracy 91.5\n",
      "Epoch 727 Loss 89.54169492029709 Training Accuracy 91.5\n",
      "Epoch 728 Loss 89.52928404980406 Training Accuracy 91.5\n",
      "Epoch 729 Loss 89.51691379747264 Training Accuracy 91.5\n",
      "Epoch 730 Loss 89.50458399149684 Training Accuracy 91.5\n",
      "Epoch 731 Loss 89.49229446098163 Training Accuracy 91.5\n",
      "Epoch 732 Loss 89.4800450359369 Training Accuracy 91.75\n",
      "Epoch 733 Loss 89.46783554727187 Training Accuracy 91.75\n",
      "Epoch 734 Loss 89.45566582678927 Training Accuracy 91.75\n",
      "Epoch 735 Loss 89.44353570717962 Training Accuracy 91.75\n",
      "Epoch 736 Loss 89.4314450220156 Training Accuracy 91.75\n",
      "Epoch 737 Loss 89.4193936057464 Training Accuracy 91.75\n",
      "Epoch 738 Loss 89.4073812936922 Training Accuracy 91.75\n",
      "Epoch 739 Loss 89.39540792203863 Training Accuracy 91.75\n",
      "Epoch 740 Loss 89.38347332783124 Training Accuracy 91.75\n",
      "Epoch 741 Loss 89.37157734897006 Training Accuracy 91.75\n",
      "Epoch 742 Loss 89.35971982420423 Training Accuracy 91.75\n",
      "Epoch 743 Loss 89.34790059312664 Training Accuracy 91.75\n",
      "Epoch 744 Loss 89.33611949616846 Training Accuracy 91.75\n",
      "Epoch 745 Loss 89.32437637459404 Training Accuracy 91.75\n",
      "Epoch 746 Loss 89.31267107049555 Training Accuracy 91.75\n",
      "Epoch 747 Loss 89.30100342678782 Training Accuracy 91.75\n",
      "Epoch 748 Loss 89.28937328720312 Training Accuracy 91.75\n",
      "Epoch 749 Loss 89.27778049628598 Training Accuracy 91.75\n",
      "Epoch 750 Loss 89.26622489938828 Training Accuracy 91.75\n",
      "Epoch 751 Loss 89.25470634266398 Training Accuracy 91.75\n",
      "Epoch 752 Loss 89.24322467306422 Training Accuracy 91.75\n",
      "Epoch 753 Loss 89.23177973833229 Training Accuracy 91.75\n",
      "Epoch 754 Loss 89.22037138699868 Training Accuracy 91.75\n",
      "Epoch 755 Loss 89.20899946837616 Training Accuracy 91.75\n",
      "Epoch 756 Loss 89.19766383255498 Training Accuracy 91.75\n",
      "Epoch 757 Loss 89.18636433039791 Training Accuracy 91.75\n",
      "Epoch 758 Loss 89.17510081353548 Training Accuracy 91.75\n",
      "Epoch 759 Loss 89.16387313436127 Training Accuracy 91.75\n",
      "Epoch 760 Loss 89.15268114602708 Training Accuracy 91.75\n",
      "Epoch 761 Loss 89.1415247024383 Training Accuracy 91.75\n",
      "Epoch 762 Loss 89.13040365824921 Training Accuracy 91.75\n",
      "Epoch 763 Loss 89.11931786885827 Training Accuracy 91.75\n",
      "Epoch 764 Loss 89.10826719040372 Training Accuracy 91.75\n",
      "Epoch 765 Loss 89.0972514797588 Training Accuracy 91.75\n",
      "Epoch 766 Loss 89.08627059452735 Training Accuracy 91.75\n",
      "Epoch 767 Loss 89.07532439303922 Training Accuracy 91.75\n",
      "Epoch 768 Loss 89.0644127343459 Training Accuracy 91.75\n",
      "Epoch 769 Loss 89.05353547821599 Training Accuracy 91.75\n",
      "Epoch 770 Loss 89.04269248513083 Training Accuracy 91.75\n",
      "Epoch 771 Loss 89.03188361628017 Training Accuracy 91.75\n",
      "Epoch 772 Loss 89.02110873355775 Training Accuracy 91.75\n",
      "Epoch 773 Loss 89.01036769955702 Training Accuracy 91.75\n",
      "Epoch 774 Loss 88.99966037756693 Training Accuracy 91.75\n",
      "Epoch 775 Loss 88.98898663156751 Training Accuracy 91.75\n",
      "Epoch 776 Loss 88.97834632622589 Training Accuracy 91.75\n",
      "Epoch 777 Loss 88.96773932689192 Training Accuracy 91.75\n",
      "Epoch 778 Loss 88.957165499594 Training Accuracy 91.75\n",
      "Epoch 779 Loss 88.94662471103518 Training Accuracy 91.75\n",
      "Epoch 780 Loss 88.93611682858878 Training Accuracy 91.75\n",
      "Epoch 781 Loss 88.92564172029452 Training Accuracy 91.75\n",
      "Epoch 782 Loss 88.91519925485437 Training Accuracy 91.75\n",
      "Epoch 783 Loss 88.90478930162854 Training Accuracy 91.75\n",
      "Epoch 784 Loss 88.89441173063162 Training Accuracy 91.75\n",
      "Epoch 785 Loss 88.88406641252841 Training Accuracy 91.75\n",
      "Epoch 786 Loss 88.87375321863024 Training Accuracy 91.75\n",
      "Epoch 787 Loss 88.86347202089087 Training Accuracy 91.75\n",
      "Epoch 788 Loss 88.85322269190269 Training Accuracy 91.75\n",
      "Epoch 789 Loss 88.8430051048929 Training Accuracy 91.75\n",
      "Epoch 790 Loss 88.83281913371964 Training Accuracy 91.75\n",
      "Epoch 791 Loss 88.82266465286828 Training Accuracy 91.75\n",
      "Epoch 792 Loss 88.81254153744757 Training Accuracy 91.75\n",
      "Epoch 793 Loss 88.80244966318593 Training Accuracy 91.75\n",
      "Epoch 794 Loss 88.79238890642772 Training Accuracy 91.75\n",
      "Epoch 795 Loss 88.78235914412966 Training Accuracy 91.75\n",
      "Epoch 796 Loss 88.77236025385699 Training Accuracy 92.0\n",
      "Epoch 797 Loss 88.76239211377998 Training Accuracy 92.0\n",
      "Epoch 798 Loss 88.75245460267031 Training Accuracy 92.0\n",
      "Epoch 799 Loss 88.74254759989739 Training Accuracy 92.0\n",
      "Epoch 800 Loss 88.73267098542487 Training Accuracy 92.0\n",
      "Epoch 801 Loss 88.72282463980714 Training Accuracy 92.0\n",
      "Epoch 802 Loss 88.71300844418576 Training Accuracy 92.0\n",
      "Epoch 803 Loss 88.70322228028596 Training Accuracy 92.0\n",
      "Epoch 804 Loss 88.69346603041325 Training Accuracy 92.0\n",
      "Epoch 805 Loss 88.68373957744996 Training Accuracy 92.0\n",
      "Epoch 806 Loss 88.67404280485177 Training Accuracy 92.0\n",
      "Epoch 807 Loss 88.66437559664439 Training Accuracy 92.0\n",
      "Epoch 808 Loss 88.65473783742019 Training Accuracy 92.0\n",
      "Epoch 809 Loss 88.64512941233481 Training Accuracy 92.0\n",
      "Epoch 810 Loss 88.6355502071039 Training Accuracy 92.0\n",
      "Epoch 811 Loss 88.62600010799974 Training Accuracy 92.0\n",
      "Epoch 812 Loss 88.61647900184805 Training Accuracy 92.0\n",
      "Epoch 813 Loss 88.60698677602471 Training Accuracy 92.0\n",
      "Epoch 814 Loss 88.59752331845256 Training Accuracy 92.0\n",
      "Epoch 815 Loss 88.5880885175981 Training Accuracy 92.0\n",
      "Epoch 816 Loss 88.57868226246839 Training Accuracy 92.0\n",
      "Epoch 817 Loss 88.56930444260789 Training Accuracy 92.0\n",
      "Epoch 818 Loss 88.55995494809527 Training Accuracy 92.0\n",
      "Epoch 819 Loss 88.55063366954033 Training Accuracy 92.0\n",
      "Epoch 820 Loss 88.54134049808087 Training Accuracy 92.0\n",
      "Epoch 821 Loss 88.53207532537965 Training Accuracy 92.0\n",
      "Epoch 822 Loss 88.5228380436213 Training Accuracy 92.0\n",
      "Epoch 823 Loss 88.5136285455093 Training Accuracy 92.0\n",
      "Epoch 824 Loss 88.50444672426298 Training Accuracy 92.0\n",
      "Epoch 825 Loss 88.4952924736145 Training Accuracy 92.0\n",
      "Epoch 826 Loss 88.48616568780588 Training Accuracy 92.0\n",
      "Epoch 827 Loss 88.47706626158605 Training Accuracy 92.0\n",
      "Epoch 828 Loss 88.46799409020794 Training Accuracy 92.0\n",
      "Epoch 829 Loss 88.45894906942554 Training Accuracy 92.0\n",
      "Epoch 830 Loss 88.44993109549102 Training Accuracy 92.0\n",
      "Epoch 831 Loss 88.4409400651518 Training Accuracy 92.0\n",
      "Epoch 832 Loss 88.43197587564785 Training Accuracy 92.0\n",
      "Epoch 833 Loss 88.42303842470864 Training Accuracy 92.0\n",
      "Epoch 834 Loss 88.41412761055045 Training Accuracy 92.0\n",
      "Epoch 835 Loss 88.40524333187362 Training Accuracy 92.0\n",
      "Epoch 836 Loss 88.39638548785966 Training Accuracy 92.0\n",
      "Epoch 837 Loss 88.38755397816854 Training Accuracy 92.0\n",
      "Epoch 838 Loss 88.37874870293595 Training Accuracy 92.0\n",
      "Epoch 839 Loss 88.36996956277058 Training Accuracy 92.0\n",
      "Epoch 840 Loss 88.36121645875137 Training Accuracy 92.0\n",
      "Epoch 841 Loss 88.35248929242493 Training Accuracy 92.0\n",
      "Epoch 842 Loss 88.34378796580272 Training Accuracy 92.0\n",
      "Epoch 843 Loss 88.33511238135853 Training Accuracy 92.0\n",
      "Epoch 844 Loss 88.32646244202581 Training Accuracy 92.0\n",
      "Epoch 845 Loss 88.31783805119504 Training Accuracy 92.25\n",
      "Epoch 846 Loss 88.30923911271108 Training Accuracy 92.25\n",
      "Epoch 847 Loss 88.3006655308707 Training Accuracy 92.25\n",
      "Epoch 848 Loss 88.29211721041995 Training Accuracy 92.25\n",
      "Epoch 849 Loss 88.2835940565516 Training Accuracy 92.25\n",
      "Epoch 850 Loss 88.27509597490268 Training Accuracy 92.25\n",
      "Epoch 851 Loss 88.26662287155187 Training Accuracy 92.25\n",
      "Epoch 852 Loss 88.2581746530171 Training Accuracy 92.25\n",
      "Epoch 853 Loss 88.24975122625298 Training Accuracy 92.25\n",
      "Epoch 854 Loss 88.24135249864842 Training Accuracy 92.25\n",
      "Epoch 855 Loss 88.23297837802414 Training Accuracy 92.25\n",
      "Epoch 856 Loss 88.22462877263021 Training Accuracy 92.25\n",
      "Epoch 857 Loss 88.21630359114374 Training Accuracy 92.25\n",
      "Epoch 858 Loss 88.20800274266632 Training Accuracy 92.25\n",
      "Epoch 859 Loss 88.19972613672181 Training Accuracy 92.25\n",
      "Epoch 860 Loss 88.1914736832538 Training Accuracy 92.25\n",
      "Epoch 861 Loss 88.18324529262347 Training Accuracy 92.25\n",
      "Epoch 862 Loss 88.17504087560701 Training Accuracy 92.25\n",
      "Epoch 863 Loss 88.16686034339348 Training Accuracy 92.25\n",
      "Epoch 864 Loss 88.15870360758245 Training Accuracy 92.25\n",
      "Epoch 865 Loss 88.15057058018166 Training Accuracy 92.25\n",
      "Epoch 866 Loss 88.14246117360484 Training Accuracy 92.25\n",
      "Epoch 867 Loss 88.13437530066938 Training Accuracy 92.25\n",
      "Epoch 868 Loss 88.12631287459405 Training Accuracy 92.25\n",
      "Epoch 869 Loss 88.11827380899692 Training Accuracy 92.25\n",
      "Epoch 870 Loss 88.1102580178929 Training Accuracy 92.25\n",
      "Epoch 871 Loss 88.10226541569183 Training Accuracy 92.25\n",
      "Epoch 872 Loss 88.09429591719604 Training Accuracy 92.25\n",
      "Epoch 873 Loss 88.08634943759827 Training Accuracy 92.25\n",
      "Epoch 874 Loss 88.0784258924796 Training Accuracy 92.25\n",
      "Epoch 875 Loss 88.07052519780714 Training Accuracy 92.25\n",
      "Epoch 876 Loss 88.06264726993201 Training Accuracy 92.25\n",
      "Epoch 877 Loss 88.05479202558719 Training Accuracy 92.25\n",
      "Epoch 878 Loss 88.04695938188539 Training Accuracy 92.25\n",
      "Epoch 879 Loss 88.03914925631702 Training Accuracy 92.25\n",
      "Epoch 880 Loss 88.03136156674803 Training Accuracy 92.25\n",
      "Epoch 881 Loss 88.02359623141794 Training Accuracy 92.25\n",
      "Epoch 882 Loss 88.0158531689377 Training Accuracy 92.25\n",
      "Epoch 883 Loss 88.0081322982877 Training Accuracy 92.25\n",
      "Epoch 884 Loss 88.00043353881577 Training Accuracy 92.25\n",
      "Epoch 885 Loss 87.99275681023511 Training Accuracy 92.25\n",
      "Epoch 886 Loss 87.98510203262228 Training Accuracy 92.25\n",
      "Epoch 887 Loss 87.97746912641533 Training Accuracy 92.25\n",
      "Epoch 888 Loss 87.96985801241169 Training Accuracy 92.25\n",
      "Epoch 889 Loss 87.96226861176629 Training Accuracy 92.25\n",
      "Epoch 890 Loss 87.95470084598958 Training Accuracy 92.25\n",
      "Epoch 891 Loss 87.9471546369456 Training Accuracy 92.25\n",
      "Epoch 892 Loss 87.93962990685006 Training Accuracy 92.25\n",
      "Epoch 893 Loss 87.93212657826845 Training Accuracy 92.25\n",
      "Epoch 894 Loss 87.92464457411411 Training Accuracy 92.25\n",
      "Epoch 895 Loss 87.91718381764633 Training Accuracy 92.25\n",
      "Epoch 896 Loss 87.90974423246854 Training Accuracy 92.25\n",
      "Epoch 897 Loss 87.90232574252636 Training Accuracy 92.25\n",
      "Epoch 898 Loss 87.89492827210583 Training Accuracy 92.25\n",
      "Epoch 899 Loss 87.8875517458315 Training Accuracy 92.25\n",
      "Epoch 900 Loss 87.8801960886647 Training Accuracy 92.25\n",
      "Epoch 901 Loss 87.8728612259016 Training Accuracy 92.25\n",
      "Epoch 902 Loss 87.86554708317146 Training Accuracy 92.25\n",
      "Epoch 903 Loss 87.85825358643493 Training Accuracy 92.25\n",
      "Epoch 904 Loss 87.85098066198205 Training Accuracy 92.25\n",
      "Epoch 905 Loss 87.84372823643074 Training Accuracy 92.25\n",
      "Epoch 906 Loss 87.8364962367248 Training Accuracy 92.25\n",
      "Epoch 907 Loss 87.82928459013237 Training Accuracy 92.25\n",
      "Epoch 908 Loss 87.82209322424401 Training Accuracy 92.0\n",
      "Epoch 909 Loss 87.81492206697115 Training Accuracy 92.0\n",
      "Epoch 910 Loss 87.80777104654419 Training Accuracy 92.0\n",
      "Epoch 911 Loss 87.8006400915109 Training Accuracy 92.0\n",
      "Epoch 912 Loss 87.79352913073478 Training Accuracy 92.0\n",
      "Epoch 913 Loss 87.78643809339327 Training Accuracy 92.0\n",
      "Epoch 914 Loss 87.77936690897609 Training Accuracy 92.0\n",
      "Epoch 915 Loss 87.77231550728364 Training Accuracy 92.0\n",
      "Epoch 916 Loss 87.76528381842529 Training Accuracy 92.0\n",
      "Epoch 917 Loss 87.75827177281774 Training Accuracy 92.0\n",
      "Epoch 918 Loss 87.75127930118347 Training Accuracy 92.0\n",
      "Epoch 919 Loss 87.74430633454901 Training Accuracy 92.0\n",
      "Epoch 920 Loss 87.73735280424336 Training Accuracy 92.0\n",
      "Epoch 921 Loss 87.73041864189643 Training Accuracy 92.0\n",
      "Epoch 922 Loss 87.72350377943741 Training Accuracy 92.0\n",
      "Epoch 923 Loss 87.71660814909319 Training Accuracy 92.0\n",
      "Epoch 924 Loss 87.70973168338679 Training Accuracy 92.0\n",
      "Epoch 925 Loss 87.70287431513583 Training Accuracy 92.0\n",
      "Epoch 926 Loss 87.69603597745093 Training Accuracy 92.0\n",
      "Epoch 927 Loss 87.6892166037342 Training Accuracy 92.0\n",
      "Epoch 928 Loss 87.68241612767768 Training Accuracy 92.0\n",
      "Epoch 929 Loss 87.67563448326183 Training Accuracy 92.0\n",
      "Epoch 930 Loss 87.66887160475406 Training Accuracy 92.0\n",
      "Epoch 931 Loss 87.66212742670707 Training Accuracy 92.0\n",
      "Epoch 932 Loss 87.65540188395758 Training Accuracy 92.0\n",
      "Epoch 933 Loss 87.64869491162467 Training Accuracy 92.0\n",
      "Epoch 934 Loss 87.6420064451084 Training Accuracy 92.0\n",
      "Epoch 935 Loss 87.63533642008818 Training Accuracy 92.0\n",
      "Epoch 936 Loss 87.62868477252164 Training Accuracy 92.0\n",
      "Epoch 937 Loss 87.62205143864276 Training Accuracy 92.0\n",
      "Epoch 938 Loss 87.61543635496074 Training Accuracy 92.0\n",
      "Epoch 939 Loss 87.60883945825847 Training Accuracy 92.0\n",
      "Epoch 940 Loss 87.60226068559106 Training Accuracy 92.0\n",
      "Epoch 941 Loss 87.59569997428451 Training Accuracy 92.0\n",
      "Epoch 942 Loss 87.5891572619342 Training Accuracy 92.0\n",
      "Epoch 943 Loss 87.5826324864036 Training Accuracy 92.0\n",
      "Epoch 944 Loss 87.57612558582281 Training Accuracy 92.0\n",
      "Epoch 945 Loss 87.56963649858717 Training Accuracy 92.0\n",
      "Epoch 946 Loss 87.56316516335596 Training Accuracy 92.0\n",
      "Epoch 947 Loss 87.55671151905095 Training Accuracy 92.0\n",
      "Epoch 948 Loss 87.5502755048551 Training Accuracy 92.0\n",
      "Epoch 949 Loss 87.5438570602112 Training Accuracy 92.0\n",
      "Epoch 950 Loss 87.53745612482051 Training Accuracy 92.0\n",
      "Epoch 951 Loss 87.53107263864146 Training Accuracy 92.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 952 Loss 87.52470654188829 Training Accuracy 92.0\n",
      "Epoch 953 Loss 87.51835777502974 Training Accuracy 92.0\n",
      "Epoch 954 Loss 87.5120262787878 Training Accuracy 92.0\n",
      "Epoch 955 Loss 87.5057119941363 Training Accuracy 92.0\n",
      "Epoch 956 Loss 87.49941486229976 Training Accuracy 92.0\n",
      "Epoch 957 Loss 87.49313482475196 Training Accuracy 92.0\n",
      "Epoch 958 Loss 87.48687182321473 Training Accuracy 92.0\n",
      "Epoch 959 Loss 87.48062579965672 Training Accuracy 92.0\n",
      "Epoch 960 Loss 87.47439669629206 Training Accuracy 92.0\n",
      "Epoch 961 Loss 87.46818445557919 Training Accuracy 92.0\n",
      "Epoch 962 Loss 87.46198902021953 Training Accuracy 92.0\n",
      "Epoch 963 Loss 87.45581033315631 Training Accuracy 92.0\n",
      "Epoch 964 Loss 87.44964833757324 Training Accuracy 92.0\n",
      "Epoch 965 Loss 87.44350297689343 Training Accuracy 92.0\n",
      "Epoch 966 Loss 87.43737419477806 Training Accuracy 92.0\n",
      "Epoch 967 Loss 87.43126193512519 Training Accuracy 92.0\n",
      "Epoch 968 Loss 87.42516614206863 Training Accuracy 92.0\n",
      "Epoch 969 Loss 87.41908675997664 Training Accuracy 92.0\n",
      "Epoch 970 Loss 87.41302373345079 Training Accuracy 92.0\n",
      "Epoch 971 Loss 87.40697700732477 Training Accuracy 92.0\n",
      "Epoch 972 Loss 87.4009465266633 Training Accuracy 92.0\n",
      "Epoch 973 Loss 87.39493223676078 Training Accuracy 92.0\n",
      "Epoch 974 Loss 87.3889340831403 Training Accuracy 92.0\n",
      "Epoch 975 Loss 87.38295201155242 Training Accuracy 92.0\n",
      "Epoch 976 Loss 87.376985967974 Training Accuracy 92.0\n",
      "Epoch 977 Loss 87.37103589860709 Training Accuracy 92.0\n",
      "Epoch 978 Loss 87.36510174987785 Training Accuracy 92.0\n",
      "Epoch 979 Loss 87.35918346843528 Training Accuracy 92.0\n",
      "Epoch 980 Loss 87.35328100115024 Training Accuracy 92.0\n",
      "Epoch 981 Loss 87.34739429511427 Training Accuracy 92.0\n",
      "Epoch 982 Loss 87.3415232976385 Training Accuracy 92.0\n",
      "Epoch 983 Loss 87.33566795625255 Training Accuracy 92.0\n",
      "Epoch 984 Loss 87.32982821870343 Training Accuracy 92.0\n",
      "Epoch 985 Loss 87.32400403295442 Training Accuracy 92.0\n",
      "Epoch 986 Loss 87.31819534718409 Training Accuracy 92.0\n",
      "Epoch 987 Loss 87.31240210978513 Training Accuracy 92.0\n",
      "Epoch 988 Loss 87.30662426936331 Training Accuracy 92.0\n",
      "Epoch 989 Loss 87.3008617747364 Training Accuracy 92.0\n",
      "Epoch 990 Loss 87.29511457493317 Training Accuracy 92.0\n",
      "Epoch 991 Loss 87.28938261919228 Training Accuracy 92.0\n",
      "Epoch 992 Loss 87.28366585696128 Training Accuracy 92.0\n",
      "Epoch 993 Loss 87.27796423789552 Training Accuracy 92.0\n",
      "Epoch 994 Loss 87.2722777118572 Training Accuracy 92.0\n",
      "Epoch 995 Loss 87.26660622891426 Training Accuracy 92.0\n",
      "Epoch 996 Loss 87.2609497393394 Training Accuracy 92.0\n",
      "Epoch 997 Loss 87.2553081936091 Training Accuracy 92.0\n",
      "Epoch 998 Loss 87.2496815424025 Training Accuracy 92.0\n",
      "Epoch 999 Loss 87.24406973660055 Training Accuracy 92.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00009\n",
    "w1 = np.random.randn(1,3)\n",
    "loss_steps1=np.array([])\n",
    "for i in range(1,1000):\n",
    "    z1 = np.dot(w1,X_train_norm)\n",
    "    y_pred = prediction(w1, X_train_norm)\n",
    "    val = -np.multiply(y_train,z1)\n",
    "    J1 = np.sum(np.log(1+np.exp(val)))\n",
    "    num = -np.multiply(y_train,np.exp(val))\n",
    "    den = 1+np.exp(val)\n",
    "    f = num/den\n",
    "    gradJ1 = np.dot(X_train_norm,f.T)\n",
    "    w1 = w1 - learning_rate*gradJ1.T\n",
    "    #print(y_pred,y_train[0])\n",
    "    loss_steps1=np.append(loss_steps1,J1)\n",
    "    print(\"Epoch\",i,\"Loss\",J1,\"Training Accuracy\",accuracy_score(y_train[0], y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.plot(loss_steps1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
